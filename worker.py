# -*- coding: utf-8 -*-
"""
å·¥ä½œçº¿ç¨‹æ¨¡å—
åŒ…å«è®­ç»ƒã€æµ‹è¯•å’Œé¢„æµ‹çš„å·¥ä½œçº¿ç¨‹ç±»
"""

# PyQt5 ç›¸å…³å¯¼å…¥
from PyQt5.QtCore import QThread, pyqtSignal, QObject, Qt, QMutex
# æ‰¾åˆ°ç±»ä¼¼è¿™ä¸€è¡Œï¼ŒåŠ ä¸Š QApplication
from PyQt5.QtWidgets import QApplication, QMessageBox

# PyTorch ç›¸å…³å¯¼å…¥
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
from torch.amp import autocast, GradScaler
from torch.nn.utils import clip_grad_norm_
from torch import optim
from torch.optim.swa_utils import AveragedModel, SWALR, update_bn

# æ•°æ®å¤„ç†ç›¸å…³å¯¼å…¥
import numpy as np
import cv2
import albumentations as A
from albumentations.pytorch import ToTensorV2
from albumentations import Compose

# å…¶ä»–æ ‡å‡†åº“å¯¼å…¥
import os
import sys
import time
import tempfile
import json
import random
import copy
import shutil
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
import pandas as pd

# ç§‘å­¦è®¡ç®—åº“
from scipy.ndimage import binary_erosion, distance_transform_edt, gaussian_filter
from scipy.stats import wasserstein_distance
from scipy.io import loadmat, savemat

# å›¾åƒå¤„ç†
try:
    from skimage.exposure import match_histograms
    SKIMAGE_AVAILABLE = True
except ImportError:
    SKIMAGE_AVAILABLE = False
    print("[è­¦å‘Š] skimageæœªå®‰è£…ï¼Œç›´æ–¹å›¾åŒ¹é…åŠŸèƒ½å°†ä¸å¯ç”¨")

# å¯è§†åŒ–
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯ï¼Œé¿å…å­çº¿ç¨‹å¯åŠ¨GUIè­¦å‘Š
import matplotlib.pyplot as plt
from matplotlib import font_manager
from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas
from matplotlib.figure import Figure

# NIfTI æ”¯æŒ
try:
    import nibabel as nib
    NIBABEL_AVAILABLE = True
except ImportError:
    NIBABEL_AVAILABLE = False
    print("[è­¦å‘Š] nibabel æœªå®‰è£…ï¼ŒNIfTI å¯è§†åŒ–å°†ä¸å¯ç”¨")

# è®¾ç½®matplotlibæ”¯æŒä¸­æ–‡æ˜¾ç¤º
try:
    chinese_fonts = ['SimHei', 'Microsoft YaHei', 'KaiTi', 'FangSong', 'STSong']
    available_fonts = [f.name for f in font_manager.fontManager.ttflist]
    chinese_font = None
    for font in chinese_fonts:
        if font in available_fonts:
            chinese_font = font
            break
    
    if chinese_font:
        matplotlib.rcParams['font.sans-serif'] = [chinese_font] + matplotlib.rcParams['font.sans-serif']
    else:
        matplotlib.rcParams['font.sans-serif'] = ['DejaVu Sans']
except Exception:
    matplotlib.rcParams['font.sans-serif'] = ['DejaVu Sans']

matplotlib.rcParams['axes.unicode_minus'] = False

# å¯¼å…¥æ¨¡å‹å’Œå·¥å…·å‡½æ•°
from models import *
from utils import *

class ModelTestThread(QThread):
    """æ¨¡å‹æµ‹è¯•çº¿ç¨‹"""
    update_progress = pyqtSignal(int, str)  # (è¿›åº¦ç™¾åˆ†æ¯”, çŠ¶æ€æ¶ˆæ¯)
    test_finished = pyqtSignal(dict, str, list)  # (æ€§èƒ½æŒ‡æ ‡, æ³¨æ„åŠ›çƒ­å›¾è·¯å¾„, ä½Diceæ¡ˆä¾‹åˆ—è¡¨)
    # é˜ˆå€¼æ‰«æç»“æœï¼ˆå®Œæ•´è¡¨æ ¼ + æ¨èé˜ˆå€¼ä¿¡æ¯ï¼‰ï¼Œé€šè¿‡objectä¼ é€’ï¼Œé¿å…PyQtç±»å‹é™åˆ¶
    threshold_sweep_ready = pyqtSignal(object)
    
    def __init__(self, model_paths, data_dir, model_type, use_tta=True):
        super().__init__()
        # æ”¯æŒå•æ¨¡å‹ï¼ˆé›†æˆåŠŸèƒ½å·²åˆ é™¤ï¼‰
        if isinstance(model_paths, str):
            self.model_paths = [model_paths]
        else:
            self.model_paths = model_paths
        # åªä½¿ç”¨ç¬¬ä¸€ä¸ªæ¨¡å‹
        if len(self.model_paths) > 1:
            print(f"[è­¦å‘Š] æ£€æµ‹åˆ°å¤šä¸ªæ¨¡å‹æ–‡ä»¶ï¼Œä»…ä½¿ç”¨ç¬¬ä¸€ä¸ª: {self.model_paths[0]}")
        self.model_path = self.model_paths[0]
        self.data_dir = data_dir
        self.model_type = model_type
        self.use_tta = use_tta
        self.stop_requested = False
        self.temp_dir = tempfile.mkdtemp(prefix="model_test_")
        
    def run(self):
        try:
            import torch
            from torch.utils.data import DataLoader
            import platform
            
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            self.update_progress.emit(5, f"ä½¿ç”¨è®¾å¤‡: {device}")
            
            # åŠ è½½æ¨¡å‹ï¼ˆä»…æ”¯æŒå•æ¨¡å‹ï¼Œé›†æˆåŠŸèƒ½å·²åˆ é™¤ï¼‰
            self.update_progress.emit(10, f"æ­£åœ¨åŠ è½½æ¨¡å‹: {os.path.basename(self.model_path)}")
            model = self._load_model(device, self.model_path)
            model.eval()
            
            # åŠ è½½æµ‹è¯•æ•°æ®
            self.update_progress.emit(20, "æ­£åœ¨åŠ è½½æµ‹è¯•æ•°æ®...")
            
            # åˆ›å»ºä¸´æ—¶TrainThreadå®ä¾‹æ¥ä½¿ç”¨å…¶æ•°æ®åŠ è½½æ–¹æ³•
            temp_train_thread = TrainThread(
                data_dir=self.data_dir,
                epochs=1,
                batch_size=4,
                model_path=None,
                save_best=False
            )
            temp_train_thread.model_type = self.model_type  # è®¾ç½®æ¨¡å‹ç±»å‹
            
            # è·å–patient_idsï¼ˆå­æ–‡ä»¶å¤¹ï¼‰
            patient_ids = [pid for pid in os.listdir(self.data_dir) 
                          if os.path.isdir(os.path.join(self.data_dir, pid))]
            
            if not patient_ids:
                raise ValueError("æµ‹è¯•æ•°æ®ç›®å½•ä¸ºç©ºï¼Œæœªæ‰¾åˆ°å­æ–‡ä»¶å¤¹")
            
            # ä½¿ç”¨TrainThreadçš„_collect_image_mask_pathsæ–¹æ³•è·å–å›¾åƒè·¯å¾„
            # è¿™ä¸ªæ–¹æ³•ä¼šæ­£ç¡®å¤„ç†æ–‡ä»¶ç»“æ„ï¼šdata_dir/images/patient_id/*.png å’Œ data_dir/masks/patient_id/*.png
            image_paths, mask_paths = temp_train_thread._collect_image_mask_paths(patient_ids)
            
            if not image_paths:
                raise ValueError(f"æœªæ‰¾åˆ°æµ‹è¯•å›¾åƒæ–‡ä»¶ã€‚è¯·æ£€æŸ¥æ•°æ®ç›®å½•ç»“æ„ï¼š\n{self.data_dir}\n\n"
                               f"æœŸæœ›ç»“æ„ï¼š\n"
                               f"  {self.data_dir}/\n"
                               f"    images/\n"
                               f"      patient_id1/\n"
                               f"        *.png\n"
                               f"      patient_id2/\n"
                               f"        *.png\n"
                               f"    masks/\n"
                               f"      patient_id1/\n"
                               f"        *.png\n"
                               f"      patient_id2/\n"
                               f"        *.png")
            
            # ä½¿ç”¨å…¨éƒ¨æ•°æ®ä½œä¸ºæµ‹è¯•é›†
            val_transform = A.Compose([
                A.Resize(256, 256),
                A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
                ToTensorV2()
            ])
            
            # ä½¿ç”¨å…¨éƒ¨patient_idsä½œä¸ºæµ‹è¯•é›†
            test_dataset = temp_train_thread.load_dataset(
                patient_ids, val_transform, split_name="test", 
                return_classification=False, use_weighted_sampling=False
            )
            test_loader = DataLoader(
                test_dataset, batch_size=4, shuffle=False, num_workers=0
            )
            
            # è¯„ä¼°æ¨¡å‹ï¼ˆé›†æˆåŠŸèƒ½å·²åˆ é™¤ï¼Œä»…æ”¯æŒå•æ¨¡å‹ï¼‰
            self.update_progress.emit(30, "æ­£åœ¨è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
            detailed_metrics, low_dice_cases = self._evaluate_model(model, test_loader, device, image_paths)
            
            # ç”Ÿæˆæ³¨æ„åŠ›çƒ­å›¾
            self.update_progress.emit(80, "æ­£åœ¨ç”Ÿæˆæ³¨æ„åŠ›çƒ­å›¾...")
            attention_path = self._generate_attention_maps(model, test_loader, device)
            
            self.update_progress.emit(100, "æµ‹è¯•å®Œæˆï¼")
            self.test_finished.emit(detailed_metrics, attention_path, low_dice_cases)
            
        except Exception as e:
            import traceback
            error_msg = f"æµ‹è¯•å¤±è´¥: {str(e)}\n{traceback.format_exc()}"
            self.update_progress.emit(0, error_msg)
            self.test_finished.emit({}, "", [])
    
    def _load_model(self, device, model_path=None):
        """åŠ è½½æ¨¡å‹ - ä¼˜å…ˆä»checkpointæ¨æ–­æ¨¡å‹ç±»å‹"""
        # ä½¿ç”¨ä¼ å…¥çš„model_pathï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨self.model_pathï¼ˆé›†æˆåŠŸèƒ½å·²åˆ é™¤ï¼‰
        if model_path is None:
            model_path = self.model_path
        
        # é¦–å…ˆå°è¯•ä»checkpointä¸­è¯»å–æ¨¡å‹ç±»å‹å’Œé…ç½®
        swin_params = None
        dstrans_params = None
        mamba_params = None
        resnet_params = None
        inferred_model_type = None
        
        if model_path and os.path.exists(model_path):
            try:
                checkpoint = torch.load(model_path, map_location=device)
                
                # å°è¯•ä»checkpointä¸­è¯»å–æ¨¡å‹ç±»å‹
                if isinstance(checkpoint, dict):
                    if 'model_type' in checkpoint:
                        inferred_model_type = checkpoint['model_type']
                    elif 'config' in checkpoint and isinstance(checkpoint['config'], dict):
                        if 'model_type' in checkpoint['config']:
                            inferred_model_type = checkpoint['config']['model_type']
                    
                    # è¯»å–æ¨¡å‹å‚æ•°é…ç½®ï¼ˆcheckpoint é¡¶å±‚ï¼‰
                    if 'swin_params' in checkpoint:
                        swin_params = checkpoint['swin_params']
                    if 'dstrans_params' in checkpoint:
                        dstrans_params = checkpoint['dstrans_params']
                    if 'mamba_params' in checkpoint:
                        mamba_params = checkpoint['mamba_params']
                    if 'resnet_params' in checkpoint:
                        resnet_params = checkpoint['resnet_params']

                    # ä» config ä¸­ä¼˜å…ˆè¯»å–ç»“æ„å‚æ•°ï¼ˆé…ç½®ä¼˜å…ˆåŠ è½½ï¼‰
                    if 'config' in checkpoint and isinstance(checkpoint['config'], dict):
                        cfg = checkpoint['config']
                        # ResNet ç›¸å…³å‚æ•°
                        if 'resnet_params' in cfg:
                            resnet_params = cfg['resnet_params']

                        # DS-TransUNet ç›¸å…³å‚æ•°ï¼ˆä¼˜å…ˆäºé¡¶å±‚ dstrans_paramsï¼‰
                        cfg_dstrans = cfg.get('dstrans_params') or cfg.get('dstransunet_args') or cfg.get('model_kwargs')
                        if isinstance(cfg_dstrans, dict):
                            if dstrans_params is None:
                                dstrans_params = {}
                            dstrans_params.update(cfg_dstrans)
                            print(f"[æ¨¡å‹åŠ è½½] ä»checkpoint.configè¯»å–DS-TransUNetå‚æ•°: {list(dstrans_params.keys())}")

                        # å…œåº•ï¼šè‹¥æ²¡æœ‰ dstrans_paramsï¼Œä½†å­˜åœ¨å…³é”®è¶…å‚ï¼Œåˆ™ç»„è£…ä¸€ä¸ªæœ€å°é…ç½®
                        if dstrans_params is None:
                            possible_keys = ('embed_dim', 'num_heads', 'num_layers', 'mlp_ratio', 'img_size', 'num_classes',
                                             'in_channels', 'out_channels', 'dropout')
                            has_dstrans_like = any(k in cfg for k in possible_keys)
                            if has_dstrans_like:
                                dstrans_params = {}
                                for k in possible_keys:
                                    if k in cfg:
                                        dstrans_params[k] = cfg[k]
                                print(f"[æ¨¡å‹åŠ è½½] ä»checkpoint.configæ¨æ–­DS-TransUNetæœ€å°å‚æ•°é›†: {dstrans_params}")
                    
                    # ä»state_dictæ¨æ–­æ¨¡å‹ç±»å‹ï¼ˆå¦‚æœæ— æ³•ä»checkpointè¯»å–ï¼‰
                    # ä½¿ç”¨ä¸read_checkpoint_configç›¸åŒçš„æ£€æµ‹é€»è¾‘å’Œé¡ºåº
                    if not inferred_model_type:
                        state_dict = checkpoint.get('state_dict', checkpoint)
                        # å¤„ç†DataParallelåŒ…è£…
                        if state_dict and all(k.startswith('module.') for k in state_dict.keys()):
                            state_dict = {k[7:]: v for k, v in state_dict.items()}
                        
                        # æ£€æµ‹é¡ºåºä¸read_checkpoint_configä¿æŒä¸€è‡´
                        # 1. æ£€æµ‹DS-TransUNet (patch_embed3) - ä¼˜å…ˆçº§æœ€é«˜
                        # æ£€æŸ¥å¤šç§å¯èƒ½çš„é”®åå˜ä½“ï¼ˆè€ƒè™‘DataParallelåŒ…è£…ç­‰ï¼‰
                        has_dstrans = False
                        for key in state_dict.keys():
                            if 'patch_embed3.weight' in key or key.endswith('patch_embed3.weight'):
                                has_dstrans = True
                                break
                        
                        if has_dstrans:
                            inferred_model_type = 'ds_trans_unet'
                            # ä»state_dictæ¨æ–­å‚æ•°ï¼ˆä¼˜å…ˆä½¿ç”¨ï¼Œå› ä¸ºå®ƒæ˜¯ä»å®é™…æƒé‡å½¢çŠ¶æ¨æ–­çš„ï¼Œæœ€å‡†ç¡®ï¼‰
                            inferred_dstrans_params = infer_dstrans_params_from_state_dict(state_dict)
                            if inferred_dstrans_params:
                                if dstrans_params is None:
                                    dstrans_params = {}
                                # ä¼˜å…ˆä½¿ç”¨æ¨æ–­çš„å‚æ•°ï¼ˆä»state_dictè¯»å–ï¼Œæœ€å‡†ç¡®ï¼‰ï¼Œè¦†ç›–checkpoint configä¸­çš„å‚æ•°
                                # è¿™æ ·å¯ä»¥ç¡®ä¿æ¨¡å‹ç»“æ„ä¸checkpointä¸­çš„æƒé‡åŒ¹é…
                                dstrans_params.update(inferred_dstrans_params)
                                print(f"[æ¨¡å‹åŠ è½½] ä»checkpointæ¨æ–­DS-TransUNetå‚æ•°: embed_dim={dstrans_params.get('embed_dim')}, num_heads={dstrans_params.get('num_heads')}, num_layers={dstrans_params.get('num_layers')}, mlp_ratio={dstrans_params.get('mlp_ratio', 4.0):.2f}")
                            else:
                                print(f"[è­¦å‘Š] æ£€æµ‹åˆ°DS-TransUNetä½†å‚æ•°æ¨æ–­å¤±è´¥ï¼Œå°†ä½¿ç”¨checkpoint configæˆ–é»˜è®¤å‚æ•°")
                        
                        # 2. æ£€æµ‹SwinUNet (patch_embed.proj)
                        elif 'patch_embed.proj.weight' in state_dict:
                            inferred_model_type = 'swin_unet'
                        
                        # 3. æ£€æµ‹ResNetUNet (enc0æˆ–layer0)
                        elif 'enc0.0.weight' in state_dict or 'enc0.weight' in state_dict:
                            # æ£€æµ‹æ˜¯å¦æ˜¯æ—§ç‰ˆæœ¬checkpointï¼ˆä½¿ç”¨layer0/layer1ç­‰é”®åï¼‰
                            old_version_keys = ['layer0', 'layer1', 'layer2', 'layer3', 'layer4']
                            has_old_keys = any(any(k.startswith(old_key) for k in state_dict.keys()) for old_key in old_version_keys)
                            
                            inferred_model_type = 'resnet_unet'
                            # å°è¯•æ¨æ–­backboneç±»å‹
                            if 'enc1.0.conv1.weight' in state_dict or (has_old_keys and 'layer1.0.conv1.weight' in state_dict):
                                # æ£€æŸ¥æ˜¯å¦æ˜¯ResNet101 (layer1æœ‰3ä¸ªblock)
                                if 'enc1.2.conv1.weight' in state_dict or (has_old_keys and 'layer1.2.conv1.weight' in state_dict):
                                    resnet_params = {'backbone_name': 'resnet101'}
                                else:
                                    resnet_params = {'backbone_name': 'resnet50'}
                            
                            # æ£€æµ‹æ˜¯å¦æœ‰ASPPæ¨¡å—
                            has_aspp = any('aspp' in k.lower() for k in state_dict.keys())
                            # å¦‚æœæ˜¯æ—§ç‰ˆæœ¬checkpointä¸”æ²¡æœ‰ASPPï¼Œåˆ™ç¦ç”¨ASPP
                            if has_old_keys and not has_aspp:
                                if resnet_params is None:
                                    resnet_params = {}
                                resnet_params['use_aspp'] = False
                                print(f"[æ¨¡å‹åŠ è½½] æ£€æµ‹åˆ°æ—§ç‰ˆæœ¬checkpointï¼ˆæ— ASPPï¼‰ï¼Œå°†ä½¿ç”¨å…¼å®¹æ¨¡å¼")
                        
                        # 4. æ£€æµ‹TransUNet (encoder.0)
                        elif 'encoder.0.weight' in state_dict:
                            inferred_model_type = 'trans_unet'
                        
                        # 5. æ£€æµ‹å…¶ä»–ResNetUNetå˜ä½“ (backbone.layer1)
                        elif 'backbone.layer1.0.conv1.weight' in state_dict:
                            inferred_model_type = 'resnet_unet'
                        
                        # 6. æ£€æµ‹æ—§ç‰ˆæœ¬ResNetUNet (layer0/layer1ç­‰é”®å)
                        else:
                            old_version_keys = ['layer0', 'layer1', 'layer2', 'layer3', 'layer4']
                            has_old_keys = any(any(k.startswith(old_key) for k in state_dict.keys()) for old_key in old_version_keys)
                            if has_old_keys:
                                inferred_model_type = 'resnet_unet'
                                # å°è¯•æ¨æ–­backboneç±»å‹
                                if 'layer1.0.conv1.weight' in state_dict:
                                    if 'layer1.2.conv1.weight' in state_dict:
                                        resnet_params = {'backbone_name': 'resnet101'}
                                    else:
                                        resnet_params = {'backbone_name': 'resnet50'}
                                
                                # æ£€æµ‹æ˜¯å¦æœ‰ASPPæ¨¡å—
                                has_aspp = any('aspp' in k.lower() for k in state_dict.keys())
                                if not has_aspp:
                                    if resnet_params is None:
                                        resnet_params = {}
                                    resnet_params['use_aspp'] = False
                                    print(f"[æ¨¡å‹åŠ è½½] æ£€æµ‹åˆ°æ—§ç‰ˆæœ¬checkpointï¼ˆæ— ASPPï¼‰ï¼Œå°†ä½¿ç”¨å…¼å®¹æ¨¡å¼")
            except Exception as e:
                print(f"[è­¦å‘Š] è¯»å–checkpointé…ç½®å¤±è´¥: {e}")
        
        # ã€ä¿åº•é€»è¾‘ã€‘ä»æ–‡ä»¶åæ¨æ–­åˆ†è¾¨ç‡ï¼ˆå¦‚æœæ— æ³•ä»checkpointè¯»å–ï¼‰
        # æ£€æŸ¥æ–‡ä»¶åä¸­æ˜¯å¦åŒ…å«"512"å…³é”®è¯ï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦ä¸ºé«˜åˆ†è¾¨ç‡æ¨¡å‹
        is_highres = False
        if model_path:
            filename = os.path.basename(model_path).lower()
            if '512' in filename or 'highres' in filename or 'high_res' in filename:
                is_highres = True
                print(f"[æ¨¡å‹åŠ è½½] ä»æ–‡ä»¶åæ¨æ–­ï¼šæ£€æµ‹åˆ°é«˜åˆ†è¾¨ç‡æ¨¡å‹ï¼ˆ512ï¼‰")
        
        # ä½¿ç”¨æ¨æ–­çš„æ¨¡å‹ç±»å‹ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„
        model_type_to_use = inferred_model_type or self.model_type
        
        if model_type_to_use != self.model_type:
            print(f"[æç¤º] ä»checkpointæ¨æ–­æ¨¡å‹ç±»å‹: {model_type_to_use} (ç”¨æˆ·é€‰æ‹©: {self.model_type})")
        
        # ä½¿ç”¨instantiate_modelåˆ›å»ºæ¨¡å‹ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
        model = instantiate_model(
            model_type_to_use, 
            device, 
            swin_params=swin_params,
            dstrans_params=dstrans_params,
            mamba_params=mamba_params,
            resnet_params=resnet_params
        )
        
        # åŠ è½½æƒé‡ï¼ˆå¸¦æ™ºèƒ½è¯Šæ–­ä¸å…¼å®¹åŠ è½½ï¼‰
        if model_path and os.path.exists(model_path):
            success, msg = load_model_compatible(model, model_path, device, verbose=True)
            if not success:
                print(f"[è­¦å‘Š] load_model_compatible åŠ è½½å¤±è´¥ï¼Œå°†å¯åŠ¨è¯¦ç»†è¯Šæ–­å¹¶å°è¯•å…¼å®¹åŠ è½½ã€‚åŸå› : {msg}")
                try:
                    checkpoint = torch.load(model_path, map_location=device)
                except Exception as e:
                    raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥ä¸”æ— æ³•è¯»å–checkpoint: {e}")

                # æå– state_dict
                if isinstance(checkpoint, dict) and 'state_dict' in checkpoint:
                    state_dict = checkpoint['state_dict']
                else:
                    state_dict = checkpoint if isinstance(checkpoint, dict) else checkpoint

                # å¤„ç†DataParallelå‰ç¼€
                if state_dict and all(k.startswith('module.') for k in state_dict.keys()):
                    state_dict = {k[7:]: v for k, v in state_dict.items()}

                model_state = model.state_dict()
                missing_keys = []
                unexpected_keys = []
                shape_mismatch = []

                # æ£€æŸ¥ç¼ºå¤±é”® & å½¢çŠ¶ä¸åŒ¹é…
                for k, v in model_state.items():
                    if k not in state_dict:
                        missing_keys.append(k)
                    else:
                        if state_dict[k].shape != v.shape:
                            shape_mismatch.append((k, tuple(v.shape), tuple(state_dict[k].shape)))

                # æ£€æŸ¥å¤šä½™é”®
                for k in state_dict.keys():
                    if k not in model_state:
                        unexpected_keys.append(k)

                print("\n[æ¨¡å‹åŠ è½½è¯Šæ–­] state_dict ä¸åŒ¹é…è¯¦æƒ…ï¼š")
                if missing_keys:
                    print(f"  Missing keys ({len(missing_keys)}):")
                    for k in missing_keys[:50]:
                        print(f"    - {k}")
                    if len(missing_keys) > 50:
                        print(f"    ... ä»¥åŠå¦å¤– {len(missing_keys)-50} ä¸ªç¼ºå¤±é”®")
                else:
                    print("  Missing keys: æ— ")

                if unexpected_keys:
                    print(f"  Unexpected keys ({len(unexpected_keys)}):")
                    for k in unexpected_keys[:50]:
                        print(f"    - {k}")
                    if len(unexpected_keys) > 50:
                        print(f"    ... ä»¥åŠå¦å¤– {len(unexpected_keys)-50} ä¸ªå¤šä½™é”®")
                else:
                    print("  Unexpected keys: æ— ")

                if shape_mismatch:
                    print(f"  Shape mismatch ({len(shape_mismatch)}):")
                    for k, m_shape, c_shape in shape_mismatch[:50]:
                        print(f"    - Key: {k}, Model: {m_shape}, Checkpoint: {c_shape}")
                    if len(shape_mismatch) > 50:
                        print(f"    ... ä»¥åŠå¦å¤– {len(shape_mismatch)-50} ä¸ªå½¢çŠ¶ä¸åŒ¹é…å‚æ•°")
                else:
                    print("  Shape mismatch: æ— ")

                # ç‰¹åˆ«æç¤º Transformer / DS-TransUNet çš„å°ºå¯¸é—®é¢˜
                cfg = None
                if isinstance(checkpoint, dict) and isinstance(checkpoint.get('config', None), dict):
                    cfg = checkpoint['config']
                if cfg and self.model_type in ("ds_trans_unet", "swin_unet", "swin_unet_v2", "swinunet"):
                    img_size_cfg = cfg.get("img_size") or cfg.get("image_size")
                    num_classes_cfg = cfg.get("num_classes")
                    print("\n[æç¤º] Transformer/DS-TransUNet é…ç½®æ£€æŸ¥ï¼š")
                    print(f"  checkpoint.config.img_size   = {img_size_cfg}")
                    print(f"  checkpoint.config.num_classes= {num_classes_cfg}")
                    print("  è¯·ç¡®è®¤å½“å‰å®ä¾‹åŒ–çš„æ¨¡å‹ img_size / num_classes ä¸ä¸Šè¿°å€¼ä¸€è‡´ï¼Œå¦åˆ™ä½ç½®ç¼–ç æˆ–è¾“å‡ºå¤´ä¼šå½¢çŠ¶ä¸åŒ¹é…ã€‚")

                # å°è¯•éä¸¥æ ¼åŠ è½½ï¼ˆå¿½ç•¥å¤šä½™é”®å’Œå½¢çŠ¶ä¸åŒ¹é…çš„éƒ¨åˆ†ï¼‰
                try:
                    missing, unexpected = model.load_state_dict(state_dict, strict=False)
                    print("[è­¦å‘Š] æ¨¡å‹ä½¿ç”¨ strict=False å…¼å®¹åŠ è½½æˆåŠŸã€‚")
                    if missing:
                        print(f"  strict=False ä»å­˜åœ¨ missing keys ({len(missing)}):")
                        for k in missing[:50]:
                            print(f"    - {k}")
                    if unexpected:
                        print(f"  strict=False ä»å­˜åœ¨ unexpected keys ({len(unexpected)}):")
                        for k in unexpected[:50]:
                            print(f"    - {k}")
                except Exception as e2:
                    raise RuntimeError(f"æ¨¡å‹ä¸¥æ ¼åŠ è½½ä¸å…¼å®¹åŠ è½½å‡å¤±è´¥ï¼Œè¯·æ ¹æ®ä¸Šæ–¹è¯Šæ–­æ£€æŸ¥æ¨¡å‹ç»“æ„ä¸checkpointæ˜¯å¦åŒ¹é…ã€‚æœ€åé”™è¯¯: {e2}")
        
        return model.to(device)
    
    def _evaluate_model(self, model, dataloader, device, image_paths):
        """è¯„ä¼°æ¨¡å‹å¹¶æ‰¾å‡ºä½Diceæ¡ˆä¾‹ - ä¸è®­ç»ƒæ—¶çš„è¯„ä¼°é€»è¾‘ä¿æŒä¸€è‡´"""
        import torch.nn.functional as F
        import numpy as np
        from tqdm import tqdm
        
        metrics = {
            'dice': [], 'iou': [], 'precision': [], 'recall': [],
            'sensitivity': [], 'specificity': [], 'f1': [], 'hd95': []
        }
        low_dice_cases = []  # [(image_path, dice, iou, precision, recall), ...]
        accum_tp = accum_fp = accum_fn = accum_tn = 0.0
        
        # ç»Ÿè®¡ç©ºmaskæƒ…å†µ
        empty_target_count = 0  # çœŸå®maskä¸ºç©ºçš„æ ·æœ¬æ•°
        empty_pred_count = 0   # é¢„æµ‹maskä¸ºç©ºçš„æ ·æœ¬æ•°
        both_empty_count = 0    # ä¸¤è€…éƒ½ç©ºçš„æ ·æœ¬æ•°
        both_non_empty_count = 0  # ä¸¤è€…éƒ½ä¸ç©ºçš„æ ·æœ¬æ•°
        
        # æˆåˆ†åˆ†æï¼šæµ‹è¯•é›†æ ·æœ¬åˆ†å¸ƒ + åˆ†ç±»Diceï¼ˆPos/Negï¼‰
        test_total_samples = 0
        test_pos_samples = 0
        test_neg_samples = 0
        test_dice_pos_sum = 0.0
        test_dice_neg_sum = 0.0
        
        model.eval()
        # æ¨¡å¼æ£€æŸ¥ï¼šç¡®ä¿å·²è¿›å…¥ eval
        print(f"[æµ‹è¯•|æ¨¡å¼æ£€æŸ¥] model.training={getattr(model, 'training', None)} (æœŸæœ› False)")
        image_idx = 0
        
        # åˆ›å»ºä¸´æ—¶TrainThreadå®ä¾‹ä»¥ä½¿ç”¨å…¶æ–¹æ³•ï¼ˆä¸è®­ç»ƒè¿‡ç¨‹ä¸€è‡´ï¼‰
        temp_train_thread = TrainThread(
            data_dir=self.data_dir,
            epochs=1,
            batch_size=4,
            model_path=None,
            save_best=False
        )
        
        # ==============================
        # æµ‹è¯•æœŸè¶…å‚æœç´¢ï¼šTTA + é˜ˆå€¼æ‰«æ
        # ==============================
        # ã€ä¿®æ”¹ã€‘é˜ˆå€¼æœç´¢èŒƒå›´æ”¹ä¸º0.89-0.99ï¼Œæ­¥é•¿ 0.01ï¼Œå…±10ä¸ªé˜ˆå€¼ç‚¹
        thresholds = [round(0.89 + i * 0.01, 2) for i in range(10)]  # [0.89, 0.90, 0.91, ..., 0.98]
        # ã€ä¿®æ”¹ã€‘æ”¹ä¸ºæ ·æœ¬çº§æŒ‡æ ‡è®¡ç®—ï¼šä¸ºæ¯ä¸ªé˜ˆå€¼å­˜å‚¨æ ·æœ¬çº§æŒ‡æ ‡åˆ—è¡¨
        sweep_dice_scores = {t: [] for t in thresholds}  # å­˜å‚¨æ¯ä¸ªæ ·æœ¬çš„Diceå€¼
        sweep_iou_scores = {t: [] for t in thresholds}  # å­˜å‚¨æ¯ä¸ªæ ·æœ¬çš„IoUå€¼
        sweep_precision_scores = {t: [] for t in thresholds}  # å­˜å‚¨æ¯ä¸ªæ ·æœ¬çš„Precisionå€¼
        sweep_recall_scores = {t: [] for t in thresholds}  # å­˜å‚¨æ¯ä¸ªæ ·æœ¬çš„Recallå€¼
        sweep_specificity_scores = {t: [] for t in thresholds}   # å­˜å‚¨æ¯ä¸ªæ ·æœ¬çš„Specificityå€¼
        sweep_stats = {t: {"tp": 0.0, "fp": 0.0, "fn": 0.0, "tn": 0.0, "fp_pix": 0.0} for t in thresholds}

        def _forward_with_tta(images_tensor: torch.Tensor) -> torch.Tensor:
            """
            ç¡®ä¿TTAå¼€å¯ï¼šä¼˜å…ˆç”¨å†…ç½® _tta_inferenceï¼›è‹¥å…³é—­/ä¸å¯ç”¨åˆ™ç”¨ç®€æ˜“æ°´å¹³ç¿»è½¬TTAã€‚
            è¿”å› logits (B,1,H,W)
            """
            # å¼ºåˆ¶å¼€å¯TTAï¼šä¼˜å…ˆä½¿ç”¨ self.use_tta + _tta_inference
            try:
                logits = self._tta_inference(model, images_tensor)
                if isinstance(logits, tuple):
                    logits = logits[0]
                return logits
            except Exception:
                # ç®€æ˜“ TTAï¼šåŸå›¾ + æ°´å¹³ç¿»è½¬å¹³å‡
                logits1 = model(images_tensor)
                if isinstance(logits1, tuple):
                    logits1 = logits1[0]
                logits2 = model(torch.flip(images_tensor, dims=[3]))
                if isinstance(logits2, tuple):
                    logits2 = logits2[0]
                logits2 = torch.flip(logits2, dims=[3])
                return (logits1 + logits2) * 0.5

        print("\n[æµ‹è¯•] å¼€å§‹é˜ˆå€¼æ‰«æï¼ˆTTA + Threshold Sweepï¼‰")
        print("Threshold | Global Dice | Precision | Recall | FP Count")
        print("--- | --- | --- | --- | ---")

        with torch.no_grad():
            for batch_data in tqdm(dataloader, desc="é˜ˆå€¼æ‰«æä¸­"):
                if len(batch_data) == 3:
                    images, masks, _ = batch_data
                else:
                    images, masks = batch_data
                images, masks = images.to(device), masks.float().to(device)

                logits = _forward_with_tta(images)
                if logits.shape[2:] != masks.shape[2:]:
                    logits = F.interpolate(logits, size=masks.shape[2:], mode='bilinear', align_corners=False)
                probs = torch.sigmoid(logits)

                # ã€HD95ä¼˜åŒ–åå¤„ç†ã€‘å¯¹æ¯ä¸ªé˜ˆå€¼åˆ†åˆ«è®¡ç®—ï¼ˆé«˜æ–¯æ¨¡ç³Š + å½¢æ€å­¦é—­è¿ç®— + ä¸¥æ ¼è¿é€šåŸŸè¿‡æ»¤ï¼‰
                # ç¡®ä¿é˜ˆå€¼æ‰«ææ—¶çš„é€»è¾‘ä¸æœ€ç»ˆæŠ¥å‘Šå®Œå…¨ä¸€è‡´
                for thr in thresholds:
                    # å¯¹æ¯ä¸ªæ ·æœ¬åº”ç”¨ä¼˜åŒ–çš„åå¤„ç†
                    preds_bin_list = []
                    for i in range(probs.shape[0]):
                        prob_single = probs[i, 0]  # H x W
                        # åº”ç”¨ä¼˜åŒ–çš„åå¤„ç†æµæ°´çº¿ï¼ˆå¯ç”¨åŠ¨æ€é¢ç§¯é˜ˆå€¼ï¼‰
                        pred_single = temp_train_thread.post_process_refine_for_hd95(
                            prob_single, 
                            threshold=thr,
                            min_area_threshold=100,  # åŸºç¡€é¢ç§¯é˜ˆå€¼ï¼ˆä¼šåŠ¨æ€è°ƒæ•´ï¼‰
                            use_gaussian_blur=True,  # å¯ç”¨é«˜æ–¯æ¨¡ç³Šå¹³æ»‘è¾¹ç¼˜
                            use_morphology=True,      # å¯ç”¨å½¢æ€å­¦é—­è¿ç®—
                            dynamic_area_threshold=True  # å¯ç”¨åŠ¨æ€é¢ç§¯é˜ˆå€¼
                        )
                        if isinstance(pred_single, torch.Tensor):
                            preds_bin_list.append(pred_single.unsqueeze(0))
                        else:
                            preds_bin_list.append(torch.from_numpy(pred_single).unsqueeze(0).to(device))
                    # ã€å®‰å…¨æ£€æŸ¥ã€‘ç¡®ä¿åˆ—è¡¨ä¸ä¸ºç©º
                    if len(preds_bin_list) == 0:
                        raise ValueError(f"preds_bin_listä¸ºç©ºï¼Œprobs.shape={probs.shape}")
                    preds_bin = torch.cat(preds_bin_list, dim=0).unsqueeze(1).to(device)  # B x 1 x H x W
                    
                    # --- é—­è¿ç®—ä»£ç  (å·²æ³¨é‡Š) ---
                    # # åº”ç”¨é—­è¿ç®—ï¼ˆå¡«å……å°å­”æ´ï¼Œè¿æ¥æ¥è¿‘çš„ç‰©ä½“ï¼‰
                    # for i in range(preds_bin.shape[0]):
                    #     pred_mask_np = preds_bin[i, 0].cpu().numpy()
                    #     # è½¬æ¢ä¸º uint8 æ ¼å¼
                    #     if pred_mask_np.max() <= 1.0:
                    #         pred_mask_np = (pred_mask_np * 255).astype(np.uint8)
                    #     else:
                    #         pred_mask_np = pred_mask_np.astype(np.uint8)
                    #     
                    #     # é—­è¿ç®—ï¼šå…ˆè†¨èƒ€åè…èš€ï¼Œå¡«å……å°å­”æ´
                    #     kernel = np.ones((3, 3), np.uint8)
                    #     pred_mask_closed = cv2.morphologyEx(pred_mask_np, cv2.MORPH_CLOSE, kernel, iterations=1)
                    #     
                    #     # è½¬æ¢å› 0-1 èŒƒå›´å¹¶æ›´æ–°
                    #     pred_mask_closed = (pred_mask_closed > 127).astype(np.float32)
                    #     preds_bin[i, 0] = torch.from_numpy(pred_mask_closed).float().to(device)

                    # ã€ä¿®æ”¹ã€‘æ”¹ä¸ºæ ·æœ¬çº§æŒ‡æ ‡è®¡ç®—ï¼šå¯¹æ¯ä¸ªæ ·æœ¬è®¡ç®—æ‰€æœ‰æŒ‡æ ‡å¹¶å­˜å‚¨
                    # ä½¿ç”¨ä¸è®­ç»ƒè¿‡ç¨‹ç›¸åŒçš„calculate_batch_diceå‡½æ•°è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„Dice
                    batch_dice = temp_train_thread.calculate_batch_dice(preds_bin, masks)
                    batch_dice_np = batch_dice.cpu().numpy()
                    
                    # å¯¹æ¯ä¸ªæ ·æœ¬è®¡ç®—æ‰€æœ‰æŒ‡æ ‡ï¼ˆIoU, Precision, Recall, Specificityï¼‰
                    pred = preds_bin > 0.5
                    gt = masks > 0.5
                    
                    for i in range(preds_bin.shape[0]):
                        pred_i = pred[i, 0].cpu().numpy()
                        gt_i = gt[i, 0].cpu().numpy()
                        
                        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æ··æ·†çŸ©é˜µ
                        tp = np.sum((pred_i > 0.5) & (gt_i > 0.5))
                        fp = np.sum((pred_i > 0.5) & (gt_i <= 0.5))
                        fn = np.sum((pred_i <= 0.5) & (gt_i > 0.5))
                        tn = np.sum((pred_i <= 0.5) & (gt_i <= 0.5))
                        
                        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„Dice
                        dice_val = float(batch_dice_np[i])
                        sweep_dice_scores[thr].append(dice_val)
                        
                        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„IoU
                        iou_den = tp + fp + fn
                        iou_val = 1.0 if iou_den < 1e-8 else float(tp / (iou_den + 1e-8))
                        sweep_iou_scores[thr].append(iou_val)
                        
                        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„Precision
                        # ã€ä¿®å¤ã€‘å¦‚æœæ²¡æœ‰é¢„æµ‹å‡ºä»»ä½•æ­£æ ·æœ¬(tp+fp=0)ï¼Œåˆ™ç²¾ç¡®ç‡è§†ä¸º1.0(æ— è¯¯æ£€)
                        prec_den = tp + fp
                        precision_val = float(tp / (prec_den + 1e-8)) if prec_den > 0 else 1.0
                        sweep_precision_scores[thr].append(precision_val)
                        
                        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„Recall
                        # ã€ä¿®å¤ã€‘å¦‚æœGround Truthä¸ºç©º(æ— ç—…ç¶ï¼Œtp+fn=0)ï¼Œåˆ™å¬å›ç‡è§†ä¸º1.0(å®Œç¾è¡¨ç°)
                        rec_den = tp + fn
                        recall_val = float(tp / (rec_den + 1e-8)) if rec_den > 0 else 1.0
                        sweep_recall_scores[thr].append(recall_val)
                        
                        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„Specificity
                        spec_den = tn + fp
                        specificity_val = float(tn / (spec_den + 1e-8)) if spec_den > 0 else 0.0
                        sweep_specificity_scores[thr].append(specificity_val)
                    
                    # ç´¯è®¡åƒç´ çº§æ··æ·†çŸ©é˜µï¼ˆä»…ç”¨äºFPè®¡æ•°ç­‰ç»Ÿè®¡ä¿¡æ¯ï¼‰
                    tp_total = torch.sum(pred & gt).item()
                    fp_total = torch.sum(pred & (~gt)).item()
                    fn_total = torch.sum((~pred) & gt).item()
                    tn_total = torch.sum((~pred) & (~gt)).item()
                    sweep_stats[thr]["tp"] += tp_total
                    sweep_stats[thr]["fp"] += fp_total
                    sweep_stats[thr]["fn"] += fn_total
                    sweep_stats[thr]["tn"] += tn_total
                    sweep_stats[thr]["fp_pix"] += fp_total

        # æ‰“å°è¡¨æ ¼å¹¶é€‰æ‹©æœ€ä¼˜é˜ˆå€¼ï¼ˆä½¿ç”¨è‡ªå®šä¹‰ç»¼åˆè¯„åˆ†å‡½æ•°ï¼‰
        sweep_rows = []
        for thr in thresholds:
            tp = sweep_stats[thr]["tp"]
            fp = sweep_stats[thr]["fp"]
            fn = sweep_stats[thr]["fn"]
            tn = sweep_stats[thr]["tn"]
            
            # ã€ä¿®æ”¹ã€‘ä½¿ç”¨æ ·æœ¬çº§å®å¹³å‡è®¡ç®—æ‰€æœ‰æŒ‡æ ‡ï¼šå¯¹æ¯ä¸ªæ ·æœ¬çš„æŒ‡æ ‡å€¼æ±‚å¹³å‡
            # è€Œä¸æ˜¯åŸºäºæ€»TP/FP/FNçš„åƒç´ çº§å¾®å¹³å‡
            if sweep_dice_scores[thr]:
                dice_val = float(np.mean(sweep_dice_scores[thr]))
            else:
                dice_val = 0.0
            
            if sweep_iou_scores[thr]:
                iou_val = float(np.mean(sweep_iou_scores[thr]))
            else:
                iou_val = 0.0
            
            if sweep_precision_scores[thr]:
                precision = float(np.mean(sweep_precision_scores[thr]))
            else:
                precision = 0.0
            
            if sweep_recall_scores[thr]:
                recall = float(np.mean(sweep_recall_scores[thr]))
            else:
                recall = 0.0
            
            if sweep_specificity_scores[thr]:
                specificity = float(np.mean(sweep_specificity_scores[thr]))
            else:
                specificity = 0.0
            
            fp_count = int(sweep_stats[thr]["fp_pix"])

            # ã€æ›´æ–°è¯„åˆ†å…¬å¼ã€‘ç»¼åˆå¾—åˆ† = Dice * 0.6 + IoU * 0.1 + Sensitivity(Recall) * 0.1 + Specificity * 0.1
            # ç”¨äºé˜ˆå€¼é€‰æ‹©æ—¶çš„ç»¼åˆè¯„åˆ†
            total_score = (
                dice_val * 0.6 +
                iou_val * 0.1 +
                recall * 0.1 +  # Sensitivity = Recall
                specificity * 0.1
            )

            row = {
                "threshold": float(thr),
                "dice": float(dice_val),
                "precision": float(precision),
                "recall": float(recall),
                "iou": float(iou_val),
                "specificity": float(specificity),
                "score": float(total_score),
                "fp_count": int(fp_count),
            }
            sweep_rows.append(row)
            print(f"{thr:0.2f}      | {dice_val:0.4f}      | {precision:0.4f}    | {recall:0.4f} | {fp_count}")

        # ç›´æ¥ä»¥è‡ªå®šä¹‰ç»¼åˆè¯„åˆ† Score ä½œä¸ºä¼˜åŒ–ç›®æ ‡é€‰æ‹©æœ€ä½³é˜ˆå€¼
        fallback_used = False
        if sweep_rows:
            best_row = max(sweep_rows, key=lambda r: r.get("score", 0.0))
        else:
            fallback_used = True
            best_row = {"threshold": thresholds[0], "dice": 0.0, "precision": 0.0, "recall": 0.0, "fp_count": 0, "score": 0.0}

        optimal_threshold = float(best_row["threshold"])
        print(
            f"\nBest Threshold found: {optimal_threshold:.2f} "
            f"with TotalScore: {best_row.get('score', 0.0):.4f}, "
            f"Dice: {best_row.get('dice', 0.0):.4f}, "
            f"IoU: {best_row.get('iou', 0.0):.4f}, "
            f"Precision: {best_row.get('precision', 0.0):.4f}, "
            f"Recall: {best_row.get('recall', 0.0):.4f}, "
            f"Specificity: {best_row.get('specificity', 0.0):.4f}"
        )

        # é€šè¿‡ä¿¡å·æŠŠæ‰«æè¡¨ + æ¨èé˜ˆå€¼ä¿¡æ¯ä¼ ç»™GUI
        try:
            self.threshold_sweep_ready.emit({
                "rows": sweep_rows,
                "best": best_row,
                # ä¸ GUI ä¾§ on_threshold_sweep_ready ä¸­çš„é»˜è®¤å€¼ä¿æŒä¸€è‡´
                "recall_floor": 0.90,
                "fallback_used": fallback_used,
            })
        except Exception:
            pass
        
        # è°ƒè¯•ï¼šç»Ÿè®¡æ¨¡å‹è¾“å‡º
        output_stats = {'min': [], 'max': [], 'mean': [], 'std': []}
        pred_stats = {'min': [], 'max': [], 'mean': [], 'std': []}
        binary_stats = {'positive_pixels': []}
        
        # è¿›å…¥è¯¦ç»†è¯„ä¼°å‰ï¼Œç¡®ä¿DataLoaderå¯ä»¥é‡æ–°è¿­ä»£
        with torch.no_grad():
            for batch_idx, batch_data in enumerate(tqdm(dataloader, desc="è¯„ä¼°ä¸­")):
                if len(batch_data) == 3:
                    images, masks, _ = batch_data 
                else:
                    images, masks = batch_data
                images, masks = images.to(device), masks.to(device)
                
                # é¢„æµ‹
                # å¼ºåˆ¶å¼€å¯TTAï¼ˆä¸é˜ˆå€¼æ‰«æä¸€è‡´ï¼‰
                outputs = _forward_with_tta(images)
                if isinstance(outputs, tuple):
                    outputs = outputs[0]
                
                if outputs.shape[2:] != masks.shape[2:]:
                    outputs = F.interpolate(outputs, size=masks.shape[2:], mode='bilinear', align_corners=False)
                
                # è°ƒè¯•ï¼šè®°å½•è¾“å‡ºç»Ÿè®¡
                if batch_idx == 0:
                    output_stats['min'].append(outputs.min().item())
                    output_stats['max'].append(outputs.max().item())
                    output_stats['mean'].append(outputs.mean().item())
                    output_stats['std'].append(outputs.std().item())
                    print(f"[è°ƒè¯•] æ¨¡å‹åŸå§‹è¾“å‡ºç»Ÿè®¡: min={outputs.min().item():.4f}, max={outputs.max().item():.4f}, mean={outputs.mean().item():.4f}, std={outputs.std().item():.4f}")
                
                preds = torch.sigmoid(outputs)
                
                # è°ƒè¯•ï¼šè®°å½•sigmoidåç»Ÿè®¡
                if batch_idx == 0:
                    pred_stats['min'].append(preds.min().item())
                    pred_stats['max'].append(preds.max().item())
                    pred_stats['mean'].append(preds.mean().item())
                    pred_stats['std'].append(preds.std().item())
                    print(f"[è°ƒè¯•] Sigmoidåç»Ÿè®¡: min={preds.min().item():.4f}, max={preds.max().item():.4f}, mean={preds.mean().item():.4f}, std={preds.std().item():.4f}")
                
                # ã€HD95ä¼˜åŒ–åå¤„ç†ã€‘ä½¿ç”¨æœ€ä¼˜é˜ˆå€¼ + ä¼˜åŒ–çš„åå¤„ç†æµæ°´çº¿
                # ç¡®ä¿æœ€ç»ˆæŠ¥å‘ŠæŒ‡æ ‡ä¸æœ€ä½³é˜ˆå€¼æœç´¢ç»“æœå®Œå…¨ä¸€è‡´
                preds_binary_list = []
                for i in range(preds.shape[0]):
                    prob_single = preds[i, 0]  # H x W
                    # åº”ç”¨ä¼˜åŒ–çš„åå¤„ç†æµæ°´çº¿ï¼ˆå¯ç”¨åŠ¨æ€é¢ç§¯é˜ˆå€¼ï¼‰
                    pred_single = temp_train_thread.post_process_refine_for_hd95(
                        prob_single,
                        threshold=optimal_threshold,
                        min_area_threshold=100,  # åŸºç¡€é¢ç§¯é˜ˆå€¼ï¼ˆä¼šåŠ¨æ€è°ƒæ•´ï¼‰
                        use_gaussian_blur=True,  # å¯ç”¨é«˜æ–¯æ¨¡ç³Šå¹³æ»‘è¾¹ç¼˜
                        use_morphology=True,      # å¯ç”¨å½¢æ€å­¦é—­è¿ç®—
                        dynamic_area_threshold=True  # å¯ç”¨åŠ¨æ€é¢ç§¯é˜ˆå€¼
                    )
                    if isinstance(pred_single, torch.Tensor):
                        preds_binary_list.append(pred_single.unsqueeze(0))
                    else:
                        preds_binary_list.append(torch.from_numpy(pred_single).unsqueeze(0).to(device))
                # ã€å®‰å…¨æ£€æŸ¥ã€‘ç¡®ä¿åˆ—è¡¨ä¸ä¸ºç©º
                if len(preds_binary_list) == 0:
                    raise ValueError(f"preds_binary_listä¸ºç©ºï¼Œpreds.shape={preds.shape}")
                preds_binary = torch.cat(preds_binary_list, dim=0).unsqueeze(1).to(device)  # B x 1 x H x W
                
                # --- é—­è¿ç®—ä»£ç  (å·²æ³¨é‡Š) ---
                # # åº”ç”¨é—­è¿ç®—ï¼ˆå¡«å……å°å­”æ´ï¼Œè¿æ¥æ¥è¿‘çš„ç‰©ä½“ï¼‰- ä¸é˜ˆå€¼æ‰«ææ—¶ä¸€è‡´
                # for i in range(preds_binary.shape[0]):
                #     pred_mask_np = preds_binary[i, 0].cpu().numpy()
                #     # è½¬æ¢ä¸º uint8 æ ¼å¼
                #     if pred_mask_np.max() <= 1.0:
                #         pred_mask_np = (pred_mask_np * 255).astype(np.uint8)
                #     else:
                #         pred_mask_np = pred_mask_np.astype(np.uint8)
                #     
                #     # é—­è¿ç®—ï¼šå…ˆè†¨èƒ€åè…èš€ï¼Œå¡«å……å°å­”æ´
                #     kernel = np.ones((3, 3), np.uint8)
                #     pred_mask_closed = cv2.morphologyEx(pred_mask_np, cv2.MORPH_CLOSE, kernel, iterations=1)
                #     
                #     # è½¬æ¢å› 0-1 èŒƒå›´å¹¶æ›´æ–°
                #     pred_mask_closed = (pred_mask_closed > 127).astype(np.float32)
                #     preds_binary[i, 0] = torch.from_numpy(pred_mask_closed).float().to(preds_binary.device)
                
                # è°ƒè¯•ï¼šè®°å½•äºŒå€¼åŒ–åç»Ÿè®¡
                if batch_idx == 0:
                    positive_count = (preds_binary > 0.5).sum().item()
                    total_pixels = preds_binary.numel()
                    binary_stats['positive_pixels'].append(positive_count)
                    print(f"[è°ƒè¯•] äºŒå€¼åŒ–åæ­£æ ·æœ¬åƒç´ æ•°: {positive_count}/{total_pixels} ({100*positive_count/total_pixels:.2f}%)")
                    print(f"[è°ƒè¯•] çœŸå®maskæ­£æ ·æœ¬åƒç´ æ•°: {(masks > 0.5).sum().item()}/{masks.numel()} ({100*(masks > 0.5).sum().item()/masks.numel():.2f}%)")
                    print(f"ğŸ¯ HD95ä¼˜åŒ–åå¤„ç†å·²å¯ç”¨: é«˜æ–¯æ¨¡ç³Š + å½¢æ€å­¦é—­è¿ç®— + ä¸¥æ ¼è¿é€šåŸŸè¿‡æ»¤(ä¿ç•™å‰2ä¸ª, æœ€å°é¢ç§¯100) | é˜ˆå€¼: {optimal_threshold:.2f}")
                
                # ä½¿ç”¨ä¸è®­ç»ƒè¿‡ç¨‹ç›¸åŒçš„calculate_batch_diceå‡½æ•°è®¡ç®—Diceï¼ˆä½¿ç”¨çº¯ç²¹é˜ˆå€¼æˆªæ–­çš„ maskï¼‰
                batch_dice = temp_train_thread.calculate_batch_dice(preds_binary, masks)
                batch_dice_np = batch_dice.cpu().numpy()
                
                # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æŒ‡æ ‡
                for i in range(images.size(0)):
                    pred_mask = preds_binary[i, 0].cpu().numpy()
                    target_mask = masks[i, 0].cpu().numpy()
                    
                    # ä½¿ç”¨çº¯ç²¹é˜ˆå€¼æˆªæ–­çš„ Dice å€¼ï¼ˆä¸æ§åˆ¶å°æœç´¢æ—¶ä¸€è‡´ï¼‰
                    dice = float(batch_dice_np[i])
                    
                    # æˆåˆ†åˆ†æï¼šç»Ÿè®¡æ­£/è´Ÿæ ·æœ¬åˆ†å¸ƒ & åˆ†ç±»Dice
                    test_total_samples += 1
                    target_sum = float(np.sum(target_mask > 0.5))
                    if target_sum < 1e-7:
                        test_neg_samples += 1
                        test_dice_neg_sum += dice
                    else:
                        test_pos_samples += 1
                        test_dice_pos_sum += dice
                    
                    # æ£€æŸ¥ç©ºmaskæƒ…å†µï¼ˆç”¨äºç»Ÿè®¡ï¼‰
                    pred_sum = np.sum(pred_mask > 0.5)
                    # target_sum å·²åœ¨ä¸Šé¢è®¡ç®—ï¼ˆé¿å…é‡å¤ï¼‰
                    
                    # ç»Ÿè®¡ç©ºmaskæƒ…å†µ
                    if target_sum < 1e-7:
                        empty_target_count += 1
                    if pred_sum < 1e-7:
                        empty_pred_count += 1
                    if target_sum < 1e-7 and pred_sum < 1e-7:
                        both_empty_count += 1
                    if target_sum >= 1e-7 and pred_sum >= 1e-7:
                        both_non_empty_count += 1
                    
                    # è®¡ç®—æ··æ·†çŸ©é˜µï¼ˆç”¨äºå…¶ä»–æŒ‡æ ‡ï¼‰
                    tp = np.sum((pred_mask > 0.5) & (target_mask > 0.5))
                    fp = np.sum((pred_mask > 0.5) & (target_mask <= 0.5))
                    fn = np.sum((pred_mask <= 0.5) & (target_mask > 0.5))
                    tn = np.sum((pred_mask <= 0.5) & (target_mask <= 0.5))
                    
                    # è®¡ç®—å…¶ä»–æŒ‡æ ‡ï¼ˆIoU, Precision, Recallç­‰ï¼‰
                    iou_den = tp + fp + fn
                    iou = 1.0 if iou_den < 1e-8 else tp / (iou_den + 1e-8)
                    
                    # ã€ä¿®å¤ã€‘Precision: å¦‚æœæ²¡æœ‰é¢„æµ‹å‡ºä»»ä½•æ­£æ ·æœ¬(tp+fp=0)ï¼Œåˆ™ç²¾ç¡®ç‡è§†ä¸º1.0(æ— è¯¯æ£€)
                    prec_den = tp + fp
                    precision = float(tp / (prec_den + 1e-8)) if prec_den > 0 else 1.0
                    
                    # ã€ä¿®å¤ã€‘Recall: å¦‚æœGround Truthä¸ºç©º(æ— ç—…ç¶ï¼Œtp+fn=0)ï¼Œåˆ™å¬å›ç‡è§†ä¸º1.0(å®Œç¾è¡¨ç°)
                    rec_den = tp + fn
                    recall = float(tp / (rec_den + 1e-8)) if rec_den > 0 else 1.0
                    
                    specificity = tn / (tn + fp + 1e-8)
                    f1 = dice  # äºŒåˆ†ç±»ä¸‹F1=Diceï¼ˆä½¿ç”¨ä¸è®­ç»ƒä¸€è‡´çš„Diceå€¼ï¼‰
                    
                    # è®¡ç®—HD95ï¼ˆä½¿ç”¨TrainThreadçš„calculate_hd95æ–¹æ³•ï¼‰
                    hd95 = 0.0
                    if target_sum < 1e-7 and pred_sum < 1e-7:
                        # ä¸¤è€…éƒ½ä¸ºç©ºï¼ŒHD95ä¸º0
                        hd95 = 0.0
                    elif target_sum < 1e-7 or pred_sum < 1e-7:
                        # åªæœ‰ä¸€ä¸ªä¸ºç©ºï¼ŒHD95ä¸ºæ— ç©·å¤§ï¼ˆç”¨NaNè¡¨ç¤ºä¸å¯è®¡ç®—ï¼‰
                        hd95 = float('nan')
                    else:
                        # ä¸¤è€…éƒ½ä¸ä¸ºç©ºï¼Œè®¡ç®—HD95ï¼ˆä½¿ç”¨å…¨å±€å‡½æ•°ï¼‰
                        try:
                            hd95 = calculate_hd95(pred_mask, target_mask)
                            if np.isnan(hd95) or np.isinf(hd95) or hd95 >= 99.0:
                                hd95 = float('nan')
                        except Exception as e:
                            print(f"[è­¦å‘Š] è®¡ç®—HD95å¤±è´¥: {e}")
                            hd95 = float('nan')
                    
                    metrics['dice'].append(dice)
                    metrics['iou'].append(float(iou))
                    metrics['precision'].append(float(precision))
                    metrics['recall'].append(float(recall))
                    metrics['sensitivity'].append(float(recall))
                    metrics['specificity'].append(float(specificity))
                    metrics['f1'].append(float(f1))
                    metrics['hd95'].append(hd95)
                    
                    accum_tp += tp
                    accum_fp += fp
                    accum_fn += fn
                    accum_tn += tn
                    
                    # è®°å½•ä½Diceæ¡ˆä¾‹ï¼ˆDice < 0.7ï¼‰
                    if dice < 0.7 and image_idx < len(image_paths):
                        # ä¿å­˜åŸå§‹å›¾åƒã€é¢„æµ‹maskå’ŒçœŸå®mask
                        original_image = images[i, 0].cpu().numpy().copy()  # åŸå§‹è¾“å…¥å›¾åƒï¼Œç¡®ä¿è¿ç»­
                        # å°†å›¾åƒå½’ä¸€åŒ–åˆ°0-255èŒƒå›´ç”¨äºæ˜¾ç¤º
                        if original_image.max() > 1.0:
                            original_image = (original_image - original_image.min()) / (original_image.max() - original_image.min() + 1e-8) * 255
                        else:
                            original_image = original_image * 255
                        original_image = original_image.astype(np.uint8)
                        # ç¡®ä¿æ•°ç»„æ˜¯è¿ç»­çš„ï¼ˆCé¡ºåºï¼‰
                        if not original_image.flags['C_CONTIGUOUS']:
                            original_image = np.ascontiguousarray(original_image)
                        
                        # é¢„æµ‹maskï¼ˆå·²ç»æ˜¯äºŒå€¼åŒ–çš„ï¼‰
                        pred_mask_display = (pred_mask * 255).astype(np.uint8)
                        if not pred_mask_display.flags['C_CONTIGUOUS']:
                            pred_mask_display = np.ascontiguousarray(pred_mask_display)
                        
                        # çœŸå®maskï¼ˆè½¬æ¢ä¸º0-255ï¼‰
                        target_mask_display = (target_mask * 255).astype(np.uint8)
                        if not target_mask_display.flags['C_CONTIGUOUS']:
                            target_mask_display = np.ascontiguousarray(target_mask_display)
                        
                        low_dice_cases.append({
                            'image_path': image_paths[image_idx],
                            'dice': float(dice),
                            'iou': float(iou),
                            'precision': float(precision),
                            'recall': float(recall),
                            'specificity': float(specificity),
                            'original_image': original_image,  # numpyæ•°ç»„
                            'pred_mask': pred_mask_display,   # numpyæ•°ç»„
                            'target_mask': target_mask_display  # numpyæ•°ç»„
                        })
                    
                    image_idx += 1
        
        # æ‰“å°ç©ºmaskç»Ÿè®¡
        total_samples = len(metrics['dice'])
        print(f"\n[ç»Ÿè®¡] ç©ºmaskæƒ…å†µåˆ†æ:")
        print(f"  æ€»æ ·æœ¬æ•°: {total_samples}")
        print(f"  çœŸå®maskä¸ºç©ºçš„æ ·æœ¬: {empty_target_count} ({100*empty_target_count/total_samples:.1f}%)")
        print(f"  é¢„æµ‹maskä¸ºç©ºçš„æ ·æœ¬: {empty_pred_count} ({100*empty_pred_count/total_samples:.1f}%)")
        print(f"  ä¸¤è€…éƒ½ç©ºçš„æ ·æœ¬: {both_empty_count} ({100*both_empty_count/total_samples:.1f}%)")
        print(f"  ä¸¤è€…éƒ½ä¸ç©ºçš„æ ·æœ¬: {both_non_empty_count} ({100*both_non_empty_count/total_samples:.1f}%)")
        
        # æˆåˆ†åˆ†ææŠ¥å‘Šï¼šç”¨äºè§£é‡Š Overall Dice å·®å¼‚ï¼ˆç©ºmaskæ¯”ä¾‹/æ­£æ ·æœ¬èƒ½åŠ›ï¼‰
        pos_ratio = (test_pos_samples / test_total_samples) if test_total_samples > 0 else 0.0
        neg_ratio = (test_neg_samples / test_total_samples) if test_total_samples > 0 else 0.0
        test_dice_pos = (test_dice_pos_sum / test_pos_samples) if test_pos_samples > 0 else 0.0
        test_dice_neg = (test_dice_neg_sum / test_neg_samples) if test_neg_samples > 0 else 0.0
        print(f"\n[æˆåˆ†åˆ†æ] æµ‹è¯•é›†æ ·æœ¬åˆ†å¸ƒ:")
        print(f"  Total Samples   : {test_total_samples}")
        print(f"  Positive Samples: {test_pos_samples} ({pos_ratio:.1%})")
        print(f"  Negative Samples: {test_neg_samples} ({neg_ratio:.1%})")
        print(f"[æˆåˆ†åˆ†æ] åˆ†ç±» Dice:")
        print(f"  Test_Dice_Pos   : {test_dice_pos:.4f}")
        print(f"  Test_Dice_Neg   : {test_dice_neg:.4f}")
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡ï¼ˆå¯¹äºHD95ä½¿ç”¨nanmeanï¼Œå¿½ç•¥NaNå€¼ï¼‰
        avg_metrics = {}
        for k, v in metrics.items():
            if k == 'hd95':
                # HD95å¯èƒ½åŒ…å«NaNï¼Œä½¿ç”¨nanmean
                if v:
                    arr = np.array(v, dtype=float)
                    if np.all(np.isnan(arr)):
                        avg_metrics[k] = float('nan')
                    else:
                        avg_metrics[k] = float(np.nanmean(arr))
                else:
                    avg_metrics[k] = float('nan')
            else:
                avg_metrics[k] = float(np.mean(v)) if v else 0.0
        
        # ã€ä¿®æ”¹ã€‘å…¨å±€æŒ‡æ ‡è®¡ç®—ï¼šä»åƒç´ çº§å¾®å¹³å‡æ”¹ä¸ºæ ·æœ¬çº§å®å¹³å‡
        # ä½¿ç”¨æ¯ä¸ªæ ·æœ¬çš„æŒ‡æ ‡å€¼åˆ—è¡¨è¿›è¡Œå¹³å‡ï¼Œè€Œä¸æ˜¯åŸºäºæ€»TP/FP/FNè®¡ç®—
        # è¿™æ ·å¯ä»¥ç¡®ä¿æ¯ä¸ªæ ·æœ¬çš„æƒé‡ç›¸ç­‰ï¼Œä¸å—æ ·æœ¬å¤§å°å½±å“
        
        # Diceå’ŒF1ï¼ˆäºŒåˆ†ç±»ä¸‹F1=Diceï¼‰
        if metrics['dice']:
            avg_metrics['dice'] = float(np.mean(metrics['dice']))
        else:
            avg_metrics['dice'] = 0.0
        avg_metrics['f1'] = avg_metrics['dice']
        
        # IoUï¼šæ ·æœ¬çº§å®å¹³å‡
        if metrics['iou']:
            avg_metrics['iou'] = float(np.mean(metrics['iou']))
        else:
            avg_metrics['iou'] = 0.0
        
        # Precisionï¼šæ ·æœ¬çº§å®å¹³å‡
        if metrics['precision']:
            avg_metrics['precision'] = float(np.mean(metrics['precision']))
        else:
            avg_metrics['precision'] = 0.0
        
        # Recall/Sensitivityï¼šæ ·æœ¬çº§å®å¹³å‡
        if metrics['recall']:
            avg_metrics['recall'] = float(np.mean(metrics['recall']))
        else:
            avg_metrics['recall'] = 0.0
        avg_metrics['sensitivity'] = avg_metrics['recall']
        
        # Specificityï¼šæ ·æœ¬çº§å®å¹³å‡
        if metrics['specificity']:
            avg_metrics['specificity'] = float(np.mean(metrics['specificity']))
        else:
            avg_metrics['specificity'] = 0.0
        
        # è°ƒè¯•ï¼šæ‰“å°æ··æ·†çŸ©é˜µ
        print(f"[è°ƒè¯•] æœ€ç»ˆæ··æ·†çŸ©é˜µ: TP={accum_tp:.0f}, FP={accum_fp:.0f}, FN={accum_fn:.0f}, TN={accum_tn:.0f}")
        print(f"[è°ƒè¯•] æœ€ç»ˆæŒ‡æ ‡: Dice={avg_metrics['dice']:.4f}, IoU={avg_metrics['iou']:.4f}, Precision={avg_metrics['precision']:.4f}, Recall={avg_metrics['recall']:.4f}")
        
        # ã€ä¿®å¤ã€‘è®¡ç®—å®˜æ–¹æ€»åˆ†ï¼šä½¿ç”¨å®Œæ•´çš„å…¬å¼ï¼ŒåŒ…å«æ‰€æœ‰5ä¸ªæŒ‡æ ‡
        # å…¬å¼ï¼šTotal = 0.6*Dice + 0.1*IoU + 0.1/(1+HD95) + 0.1*Sens + 0.1*Spec
        hd95_for_score = avg_metrics['hd95'] if not (np.isnan(avg_metrics['hd95']) or np.isinf(avg_metrics['hd95'])) else 99.9
        official_total_score = calculate_official_total_score_global(
            dice=avg_metrics['dice'],
            iou=avg_metrics['iou'],
            hd95=hd95_for_score,
            sensitivity=avg_metrics['sensitivity'],
            specificity=avg_metrics['specificity']
        )
        
        print(f"[å®˜æ–¹æ€»åˆ†] Total Score = 0.6*Dice + 0.1*IoU + 0.1/(1+HD95) + 0.1*Sens + 0.1*Spec = {official_total_score:.4f}")
        hd95_str = f"{avg_metrics['hd95']:.4f}" if not (np.isnan(avg_metrics['hd95']) or np.isinf(avg_metrics['hd95'])) else "nan"
        print(f"  è¯¦ç»†: Dice={avg_metrics['dice']:.4f}, IoU={avg_metrics['iou']:.4f}, HD95={hd95_str}, Sens={avg_metrics['sensitivity']:.4f}, Spec={avg_metrics['specificity']:.4f}")
        
        # å°†å®˜æ–¹æ€»åˆ†æ·»åŠ åˆ° avg_metrics
        avg_metrics['official_total_score'] = official_total_score
        
        detailed_metrics = {
            'average': avg_metrics,
            'all_samples': metrics,
            'total_samples': len(metrics['dice'])
        }
        
        return detailed_metrics, low_dice_cases
    
    def _generate_attention_maps(self, model, dataloader, device):
        """ç”Ÿæˆæ³¨æ„åŠ›çƒ­å›¾"""
        try:
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒæ³¨æ„åŠ›å›¾
            actual_model = model
            if isinstance(actual_model, nn.DataParallel):
                actual_model = actual_model.module
            
            if not hasattr(actual_model, 'forward') or not callable(getattr(actual_model, 'forward', None)):
                return ""
            
            # å°è¯•è·å–æ³¨æ„åŠ›å›¾
            model.eval()
            attention_maps_list = []
            images_list = []
            
            with torch.no_grad():
                for batch_data in dataloader:
                    if len(batch_data) == 3:
                        images, masks, _ = batch_data
                    else:
                        images, masks = batch_data
                    images = images.to(device)
                    
                    try:
                        # å°è¯•è·å–æ³¨æ„åŠ›å›¾
                        if hasattr(actual_model, 'forward'):
                            result = actual_model(images, return_attention=True)
                            if isinstance(result, tuple) and len(result) == 2:
                                outputs, attention_maps = result
                                attention_maps_list.append(attention_maps)
                                images_list.append(images.cpu())
                    except Exception:
                        pass
                    
                    if len(images_list) >= 4:  # åªå–å‰4ä¸ªæ ·æœ¬
                        break
            
            if not attention_maps_list:
                return ""
            
            # å¯è§†åŒ–æ³¨æ„åŠ›å›¾
            import matplotlib.pyplot as plt
            import matplotlib
            matplotlib.use('Agg')
            
            fig, axes = plt.subplots(len(images_list), 5, figsize=(20, 4 * len(images_list)))
            if len(images_list) == 1:
                axes = axes.reshape(1, -1)
            
            for idx, (img, att_maps) in enumerate(zip(images_list, attention_maps_list)):
                img_np = img[0].permute(1, 2, 0).numpy()
                img_np = img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])
                img_np = np.clip(img_np, 0, 1)
                
                axes[idx, 0].imshow(img_np)
                axes[idx, 0].set_title("åŸå›¾")
                axes[idx, 0].axis('off')
                
                for i, (att_name, att_map) in enumerate(list(att_maps.items())[:4]):
                    att_np = att_map[0, 0].cpu().numpy()
                    axes[idx, i+1].imshow(att_np, cmap='hot')
                    axes[idx, i+1].set_title(f"{att_name}")
                    axes[idx, i+1].axis('off')
            
            plt.tight_layout()
            attention_path = os.path.join(self.temp_dir, "attention_maps.png")
            plt.savefig(attention_path, dpi=150, bbox_inches='tight')
            plt.close()
            
            return attention_path
        except Exception as e:
            print(f"[è­¦å‘Š] ç”Ÿæˆæ³¨æ„åŠ›çƒ­å›¾å¤±è´¥: {e}")
            return ""
    
    def _tta_inference(self, model, images):
        """
        ã€å†›ä»¤çŠ¶ï¼šTTAç»ˆæå‡çº§ã€‘å¤šå°ºåº¦ç½®ä¿¡åº¦èåˆæ¶æ„ (MSTTA)
        
        å¤šå°ºåº¦æ¨ç†ï¼š3ä¸ªå°ºåº¦ Ã— 8ç§å˜æ¢ = 24å€æ¨ç†
        - å°ºåº¦å› å­: [0.8, 1.0, 1.2]
        - 8ç§å˜æ¢: åŸå§‹ã€æ°´å¹³ç¿»è½¬ã€å‚ç›´ç¿»è½¬ã€æ—‹è½¬90/180/270åº¦ã€ç¿»è½¬+æ—‹è½¬ç»„åˆ
        
        åŠ æƒèåˆï¼šåŸºäºç½®ä¿¡åº¦çš„åŠ æƒå¹³å‡ï¼Œè€Œéç®€å•å¹³å‡
        æè‡´åå¤„ç†ï¼šGaussianæ»¤æ³¢ + LCC + remove_small_holes
        
        ç›®æ ‡ï¼šåˆ©ç”¨5080ç®—åŠ›ä¼˜åŠ¿ï¼Œé€šè¿‡24å€æ¨ç†æ¢å–0.01 Diceæå‡
        """
        import torch.nn.functional as F
        from scipy.ndimage import gaussian_filter
        
        B, C, H, W = images.shape
        scales = [0.8, 1.0, 1.2]  # å¤šå°ºåº¦å› å­
        all_predictions = []
        all_weights = []
        
        # ã€å¤šå°ºåº¦å¾ªç¯ã€‘
        for scale in scales:
            # Resizeåˆ°ç›®æ ‡å°ºåº¦
            if scale != 1.0:
                target_h, target_w = int(H * scale), int(W * scale)
                scaled_images = F.interpolate(images, size=(target_h, target_w), 
                                             mode='bilinear', align_corners=False)
            else:
                scaled_images = images
                target_h, target_w = H, W
            
            # ã€8ç§å˜æ¢å¾ªç¯ã€‘
            scale_predictions = []
            
            # 1. åŸå§‹å›¾åƒ
            pred = model(scaled_images)
            if isinstance(pred, tuple):
                pred = pred[0]
            if not (torch.any(torch.isnan(pred)) or torch.any(torch.isinf(pred))):
                if scale != 1.0:
                    pred = F.interpolate(pred, size=(H, W), mode='bilinear', align_corners=False)
                scale_predictions.append(pred)
            
            # 2. æ°´å¹³ç¿»è½¬
            pred = model(torch.flip(scaled_images, dims=[3]))
            if isinstance(pred, tuple):
                pred = pred[0]
            pred = torch.flip(pred, dims=[3])
            if not (torch.any(torch.isnan(pred)) or torch.any(torch.isinf(pred))):
                if scale != 1.0:
                    pred = F.interpolate(pred, size=(H, W), mode='bilinear', align_corners=False)
                scale_predictions.append(pred)
            
            # 3. å‚ç›´ç¿»è½¬
            pred = model(torch.flip(scaled_images, dims=[2]))
            if isinstance(pred, tuple):
                pred = pred[0]
            pred = torch.flip(pred, dims=[2])
            if not (torch.any(torch.isnan(pred)) or torch.any(torch.isinf(pred))):
                if scale != 1.0:
                    pred = F.interpolate(pred, size=(H, W), mode='bilinear', align_corners=False)
                scale_predictions.append(pred)
            
            # 4. æ—‹è½¬90åº¦
            pred = model(torch.rot90(scaled_images, k=1, dims=[2, 3]))
            if isinstance(pred, tuple):
                pred = pred[0]
            pred = torch.rot90(pred, k=-1, dims=[2, 3])
            if not (torch.any(torch.isnan(pred)) or torch.any(torch.isinf(pred))):
                if scale != 1.0:
                    pred = F.interpolate(pred, size=(H, W), mode='bilinear', align_corners=False)
                scale_predictions.append(pred)
            
            # 5. æ—‹è½¬180åº¦
            pred = model(torch.rot90(scaled_images, k=2, dims=[2, 3]))
            if isinstance(pred, tuple):
                pred = pred[0]
            pred = torch.rot90(pred, k=-2, dims=[2, 3])
            if not (torch.any(torch.isnan(pred)) or torch.any(torch.isinf(pred))):
                if scale != 1.0:
                    pred = F.interpolate(pred, size=(H, W), mode='bilinear', align_corners=False)
                scale_predictions.append(pred)
            
            # 6. æ—‹è½¬270åº¦
            pred = model(torch.rot90(scaled_images, k=3, dims=[2, 3]))
            if isinstance(pred, tuple):
                pred = pred[0]
            pred = torch.rot90(pred, k=-3, dims=[2, 3])
            if not (torch.any(torch.isnan(pred)) or torch.any(torch.isinf(pred))):
                if scale != 1.0:
                    pred = F.interpolate(pred, size=(H, W), mode='bilinear', align_corners=False)
                scale_predictions.append(pred)
            
            # 7. æ°´å¹³ç¿»è½¬+æ—‹è½¬90åº¦
            img_aug = torch.flip(scaled_images, dims=[3])
            img_aug = torch.rot90(img_aug, k=1, dims=[2, 3])
            pred = model(img_aug)
            if isinstance(pred, tuple):
                pred = pred[0]
            pred = torch.rot90(pred, k=-1, dims=[2, 3])
            pred = torch.flip(pred, dims=[3])
            if not (torch.any(torch.isnan(pred)) or torch.any(torch.isinf(pred))):
                if scale != 1.0:
                    pred = F.interpolate(pred, size=(H, W), mode='bilinear', align_corners=False)
                scale_predictions.append(pred)
            
            # 8. å‚ç›´ç¿»è½¬+æ—‹è½¬90åº¦
            img_aug = torch.flip(scaled_images, dims=[2])
            img_aug = torch.rot90(img_aug, k=1, dims=[2, 3])
            pred = model(img_aug)
            if isinstance(pred, tuple):
                pred = pred[0]
            pred = torch.rot90(pred, k=-1, dims=[2, 3])
            pred = torch.flip(pred, dims=[2])
            if not (torch.any(torch.isnan(pred)) or torch.any(torch.isinf(pred))):
                if scale != 1.0:
                    pred = F.interpolate(pred, size=(H, W), mode='bilinear', align_corners=False)
                scale_predictions.append(pred)
            
            # æ”¶é›†å½“å‰å°ºåº¦çš„æ‰€æœ‰é¢„æµ‹
            all_predictions.extend(scale_predictions)
        
        # ã€åŠ æƒèåˆã€‘è®¡ç®—æ¯ä¸ªé¢„æµ‹çš„ç½®ä¿¡åº¦æƒé‡
        if len(all_predictions) == 0:
            print(f"[ä¸¥é‡è­¦å‘Š] MSTTA: æ‰€æœ‰å˜æ¢çš„é¢„æµ‹éƒ½åŒ…å«NaN/Infï¼Œè¿”å›é›¶è¾“å‡º")
            return torch.zeros_like(model(images) if not isinstance(model(images), tuple) else model(images)[0])
        
        # ã€å…³é”®ä¿®å¤ã€‘ç»Ÿä¸€æ‰€æœ‰é¢„æµ‹çš„ç©ºé—´å°ºå¯¸åˆ°ç›®æ ‡å°ºå¯¸ (H, W)
        # ç¡®ä¿æ‰€æœ‰å¼ é‡åœ¨ stack ä¹‹å‰å…·æœ‰ç›¸åŒçš„ç©ºé—´ç»´åº¦
        target_size = (H, W)
        normalized_predictions = []
        for pred in all_predictions:
            if pred.dim() == 4:
                _, _, h, w = pred.shape
                if h != H or w != W:
                    # æ’å€¼åˆ°ç›®æ ‡å°ºå¯¸
                    pred = F.interpolate(pred, size=target_size, mode='bilinear', align_corners=False)
            normalized_predictions.append(pred)
        all_predictions = normalized_predictions
        
        # è®¡ç®—ç½®ä¿¡åº¦æƒé‡ï¼šä½¿ç”¨ p * log(p + eps) ä½œä¸ºç½®ä¿¡åº¦åº¦é‡
        weights = []
        eps = 1e-8
        for pred in all_predictions:
            # è½¬æ¢ä¸ºæ¦‚ç‡
            prob = torch.sigmoid(pred)
            # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦ï¼š-p * log(p) çš„å‡å€¼ï¼ˆç†µçš„è´Ÿå€¼ï¼Œè¶Šé«˜è¡¨ç¤ºè¶Šç¡®å®šï¼‰
            entropy = -prob * torch.log(prob + eps) - (1 - prob) * torch.log(1 - prob + eps)
            confidence = 1.0 - entropy.mean()  # è½¬æ¢ä¸ºç½®ä¿¡åº¦ï¼ˆ1 - ç†µï¼‰
            weights.append(float(confidence))
        
        # å½’ä¸€åŒ–æƒé‡
        weights = torch.tensor(weights, device=images.device, dtype=torch.float32)
        weights = weights / (weights.sum() + eps)
        
        # åŠ æƒå¹³å‡
        stacked_preds = torch.stack(all_predictions, dim=0)  # [N, B, C, H, W]
        weights_expanded = weights.view(-1, 1, 1, 1, 1)  # [N, 1, 1, 1, 1]
        weighted_pred = (stacked_preds * weights_expanded).sum(dim=0)  # [B, C, H, W]
        
        # ã€æè‡´åå¤„ç†ã€‘åº”ç”¨Gaussianæ»¤æ³¢
        weighted_pred_np = weighted_pred.detach().cpu().numpy()
        smoothed_pred_np = np.zeros_like(weighted_pred_np)
        for b in range(B):
            for c in range(C):
                smoothed_pred_np[b, c] = gaussian_filter(weighted_pred_np[b, c], sigma=0.5)
        
        # è½¬æ¢å›tensor
        smoothed_pred = torch.from_numpy(smoothed_pred_np).to(images.device).float()
        
        # ã€æè‡´åå¤„ç†ã€‘åœ¨æ¦‚ç‡å›¾ä¸Šåº”ç”¨LCCå’Œremove_small_holes
        # æ³¨æ„ï¼šè¿™é‡Œè¿”å›çš„æ˜¯logitsï¼Œåå¤„ç†ä¼šåœ¨sigmoidåçš„æ¦‚ç‡å›¾ä¸Šè¿›è¡Œ
        # ä½†ä¸ºäº†é›†æˆåˆ°TTAä¸­ï¼Œæˆ‘ä»¬åœ¨å†…éƒ¨è¿›è¡Œåå¤„ç†
        prob_pred = torch.sigmoid(smoothed_pred)
        prob_pred_np = prob_pred.detach().cpu().numpy()
        
        # å¯¹æ¯ä¸ªæ ·æœ¬åº”ç”¨æè‡´åå¤„ç†
        processed_pred_np = np.zeros_like(prob_pred_np)
        for b in range(B):
            for c in range(C):
                prob_map = prob_pred_np[b, c]
                # åº”ç”¨æè‡´åå¤„ç†æµæ°´çº¿
                processed_mask = ensemble_post_process_global(
                    prob_map,
                    use_lcc=True,  # ä¿ç•™æœ€å¤§è¿é€šåŸŸ
                    use_remove_holes=True,  # å¡«è¡¥å°å­”æ´
                    min_hole_size=100,
                    use_edge_smoothing=True  # è¾¹ç¼˜å¹³æ»‘
                )
                # è½¬æ¢å›logitsç©ºé—´ï¼ˆé€†sigmoidï¼‰
                processed_pred_np[b, c] = np.clip(np.log(processed_mask / (1 - processed_mask + eps) + eps), -10, 10)
        
        # è½¬æ¢å›tensor
        final_pred = torch.from_numpy(processed_pred_np).to(images.device).float()
        
        return final_pred



class TrainThread(QThread):
    update_progress = pyqtSignal(int, str)  # (è¿›åº¦ç™¾åˆ†æ¯”, çŠ¶æ€æ¶ˆæ¯)
    update_val_progress = pyqtSignal(int, str)  # éªŒè¯è¿›åº¦ä¿¡å·
    training_finished = pyqtSignal(str, str)  # (å®Œæˆæ¶ˆæ¯, æœ€ä½³æ¨¡å‹è·¯å¾„)
    model_saved = pyqtSignal(str)  # æ¨¡å‹ä¿å­˜é€šçŸ¥
    epoch_completed = pyqtSignal(int, float, float, float)  # (è½®æ¬¡, å¹³å‡æŸå¤±, éªŒè¯æŸå¤±, éªŒè¯Dice)
    visualization_ready = pyqtSignal(str)  # ä¿å­˜çš„å¯è§†åŒ–è·¯å¾„
    metrics_ready = pyqtSignal(dict)  # è¯„ä¼°æŒ‡æ ‡å­—å…¸
    visualization_requested = pyqtSignal(str, list, list)  # å‚æ•°ï¼š(ç»˜å›¾ç±»å‹, xè½´æ•°æ®, yè½´æ•°æ®)
    test_results_ready = pyqtSignal(str, dict)  # (å¯è§†åŒ–å›¾åƒè·¯å¾„, æ€§èƒ½åˆ†ææ•°æ®)
    epoch_analysis_ready = pyqtSignal(int, str, dict)  # (è½®æ¬¡, å¯è§†åŒ–å›¾åƒè·¯å¾„, æ€§èƒ½æŒ‡æ ‡)
    attention_analysis_ready = pyqtSignal(str, dict)  # (æ³¨æ„åŠ›å¯è§†åŒ–è·¯å¾„, æ³¨æ„åŠ›ç»Ÿè®¡ä¿¡æ¯)
    def __init__(self, data_dir, epochs, batch_size, model_path=None, save_best=True, use_gwo=False, optimizer_type="adam"):
        super().__init__()
        self.data_dir = data_dir
        self.epochs = epochs
        self.batch_size = batch_size
        self.model_path = model_path
        self.save_best = save_best
        self.use_gwo = use_gwo  # æ˜¯å¦ä½¿ç”¨GWOä¼˜åŒ–
        self.optimizer_type = optimizer_type.lower()
        
        # å®‰å…¨è¯»å–é¢„è®­ç»ƒé…ç½®
        try:
            self.pretrained_config = read_checkpoint_config(model_path) if model_path else None
        except Exception as e:
            print(f"[è­¦å‘Š] è¯»å–é¢„è®­ç»ƒé…ç½®å¤±è´¥: {e}")
            self.pretrained_config = None
        
        self.swin_params = None   # GWOä¼˜åŒ–åçš„SwinUNetå‚æ•°æˆ–æ¨¡å‹é…ç½®
        self.dstrans_params = None  # GWOä¼˜åŒ–åçš„DS-TransUNetå‚æ•°æˆ–æ¨¡å‹é…ç½®
        self.mamba_params = None  # Swin-U Mamba å·²ç§»é™¤ï¼Œå ä½å­—æ®µ
        # EMA å·²å¯ç”¨ï¼Œç”¨äºæå‡æ¨¡å‹ç¨³å®šæ€§å’ŒDiceæ€§èƒ½
        self.use_ema = True
        self.ema_decay = 0.995
        
        # å®‰å…¨è¯»å–ç¯å¢ƒå˜é‡å¹¶è½¬æ¢ä¸ºæ•´æ•°
        try:
            self.ema_eval_start_epoch = max(5, int(os.environ.get("SEG_EMA_EVAL_START", 8)))
        except (ValueError, TypeError):
            self.ema_eval_start_epoch = 8
        
        self.last_optimal_threshold = 0.5
        self.stop_requested = False
        self.best_model_path = None
        self.best_dice = -1.0
        
        # å®‰å…¨åˆ›å»ºä¸´æ—¶ç›®å½•
        try:
            self.temp_dir = tempfile.mkdtemp(prefix="med_seg_")
        except (OSError, PermissionError) as e:
            # å¦‚æœä¸´æ—¶ç›®å½•åˆ›å»ºå¤±è´¥ï¼Œä½¿ç”¨ç”¨æˆ·æ•°æ®ç›®å½•ä¸‹çš„ä¸´æ—¶ç›®å½•
            print(f"[è­¦å‘Š] ç³»ç»Ÿä¸´æ—¶ç›®å½•åˆ›å»ºå¤±è´¥: {e}ï¼Œä½¿ç”¨æ•°æ®ç›®å½•ä¸‹çš„ä¸´æ—¶ç›®å½•")
            fallback_temp = os.path.join(data_dir, "_temp_training")
            try:
                os.makedirs(fallback_temp, exist_ok=True)
                self.temp_dir = fallback_temp
            except Exception as e2:
                raise RuntimeError(f"æ— æ³•åˆ›å»ºä¸´æ—¶ç›®å½•: {e2}") from e2
        
        self.best_model_cache_dir = os.path.join(self.data_dir, "_best_model_cache")
        self.enable_matlab_cache = False
        self.matlab_cache_manager = None
        self.matlab_metrics_bridge = None
        self.enable_matlab_plots = False
        self.matlab_viz_bridge = None
        self.model_type = os.environ.get("SEG_MODEL", "improved_unet").lower()
        
        # å®‰å…¨è¯»å–ç¯å¢ƒå˜é‡å¹¶è½¬æ¢ä¸ºæ•´æ•°
        try:
            self.context_slices = int(os.environ.get("SEG_CONTEXT_SLICES", os.environ.get("SEG_CONTEXT", "0")))
        except (ValueError, TypeError):
            self.context_slices = 0
        
        try:
            self.context_gap = int(os.environ.get("SEG_CONTEXT_GAP", "1"))
        except (ValueError, TypeError):
            self.context_gap = 1
        
        self.extra_modalities_dirs = parse_extra_modalities_spec(os.environ.get("SEG_EXTRA_MODALITIES"))
        
        if self.pretrained_config:
            self.model_type = self.pretrained_config.get("model_type", self.model_type)
            # å®‰å…¨æ·±æ‹·è´é…ç½®å‚æ•°
            try:
                swin_params_raw = self.pretrained_config.get("swin_params")
                if swin_params_raw:
                    self.swin_params = copy.deepcopy(swin_params_raw)
            except Exception as e:
                print(f"[è­¦å‘Š] æ·±æ‹·è´ swin_params å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹å¼•ç”¨")
                self.swin_params = self.pretrained_config.get("swin_params")
            
            try:
                dstrans_params_raw = self.pretrained_config.get("dstrans_params")
                if dstrans_params_raw:
                    self.dstrans_params = copy.deepcopy(dstrans_params_raw)
            except Exception as e:
                print(f"[è­¦å‘Š] æ·±æ‹·è´ dstrans_params å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹å¼•ç”¨")
                self.dstrans_params = self.pretrained_config.get("dstrans_params")
            if self.swin_params or self.dstrans_params:
                self.use_gwo = False
            if "best_threshold" in self.pretrained_config:
                try:
                    self.last_optimal_threshold = float(self.pretrained_config.get("best_threshold", self.last_optimal_threshold))
                except (ValueError, TypeError):
                    pass  # ä¿æŒé»˜è®¤å€¼
            context_cfg = self.pretrained_config.get("context")
            if context_cfg:
                try:
                    self.context_slices = int(context_cfg.get("slices", self.context_slices))
                except (ValueError, TypeError):
                    pass  # ä¿æŒå½“å‰å€¼
                try:
                    self.context_gap = int(context_cfg.get("gap", self.context_gap))
                except (ValueError, TypeError):
                    pass  # ä¿æŒå½“å‰å€¼
            # ä»…ä¿ç•™æ¨¡æ€åç§°ï¼Œå…·ä½“è·¯å¾„ä»ç”±ç¯å¢ƒå˜é‡æä¾›
            extra_names = self.pretrained_config.get("extra_modalities")
            if extra_names and not self.extra_modalities_dirs:
                print(f"[æç¤º] æ¨¡å‹æœŸæœ›é¢å¤–æ¨¡æ€: {extra_names}ï¼Œè¯·é€šè¿‡ SEG_EXTRA_MODALITIES æŒ‡å®šå¯¹åº”è·¯å¾„ã€‚")
        # Skull Stripping é…ç½®
        self.use_skull_stripper = os.environ.get("SEG_USE_SKULL_STRIPPER", "0") == "1"
        self.skull_stripper_path = os.environ.get("SKULL_STRIPPER_PATH")
        
        # å®‰å…¨è¯»å–ç¯å¢ƒå˜é‡å¹¶è½¬æ¢ä¸ºæµ®ç‚¹æ•°
        try:
            self.skull_stripper_threshold = float(os.environ.get("SEG_SKULL_STRIP_THRESH", "0.5"))
        except (ValueError, TypeError):
            self.skull_stripper_threshold = 0.5
        self.skull_stripper = None
        if self.pretrained_config:
            skull_cfg = self.pretrained_config.get("skull_stripping")
            if skull_cfg:
                self.use_skull_stripper = skull_cfg.get("enabled", self.use_skull_stripper)
                self.skull_stripper_path = skull_cfg.get("model_path", self.skull_stripper_path)
                self.skull_stripper_threshold = skull_cfg.get("threshold", self.skull_stripper_threshold)
        # nnFormer é…ç½®
        self.use_nnformer = False
        
        # è·Ÿè¸ªè®­ç»ƒå†å²
        self.train_loss_history = []
        self.val_loss_history = []
        self.val_dice_history = []
        self.val_dice_pos_history = []  # ä»…ç»Ÿè®¡æœ‰å‰æ™¯maskæ ·æœ¬çš„Dice
        self.val_dice_neg_history = []  # ä»…ç»Ÿè®¡ç©ºmaskæ ·æœ¬çš„Dice
        # å¢åŠ æ·±åº¦ç›‘ç£æƒé‡,æå‡å¤šå°ºåº¦ç‰¹å¾å­¦ä¹ 
        self.aux_loss_weights = [0.3, 0.2, 0.1]  # ä»[0.2,0.1,0.05]æå‡
        self.split_metadata: Dict[str, Dict[str, List[str]]] = {}
        self.pos_weight_cache: Dict[str, float] = {}
        # éªŒè¯é˜¶æ®µåŠ¨æ€é˜ˆå€¼åˆ·æ–°è®¾ç½®
        try:
            self.threshold_refresh_interval = int(os.environ.get("SEG_THRESH_REFRESH", 1)) or 1
        except (ValueError, TypeError):
            self.threshold_refresh_interval = 1
        # é»˜è®¤é‡‡æ ·æ›´å¤šéªŒè¯æ‰¹æ¬¡, å¢å¼ºé˜ˆå€¼æœç´¢é²æ£’æ€§
        try:
            self.threshold_search_batches = int(os.environ.get("SEG_THRESH_BATCHES", 12)) or 6
        except (ValueError, TypeError):
            self.threshold_search_batches = 6
        # æ˜¯å¦å¯ç”¨ReduceLROnPlateau (é»˜è®¤å…³é—­ï¼Œé¿å…ä¸Cosineé‡å¤è°ƒåº¦å¯¼è‡´å­¦ä¹ ç‡åç¼©)
        self.use_plateau_scheduler = os.environ.get("SEG_USE_PLATEAU", "0") == "1"
        
        # ç¡®ä¿ä¸´æ—¶ç›®å½•å­˜åœ¨
        try:
            os.makedirs(self.temp_dir, exist_ok=True)
        except (OSError, PermissionError) as e:
            raise RuntimeError(f"æ— æ³•åˆ›å»ºä¸´æ—¶ç›®å½• {self.temp_dir}: {e}") from e
   
    def visualize_predictions(self, model, dataloader, device, save_name="predictions"):
        """å¯è§†åŒ–æ¨¡å‹é¢„æµ‹ç»“æœä¸çœŸå®æ ‡ç­¾"""
        save_path = os.path.join(self.temp_dir, f"{save_name}.png")
        model.eval()
        # å¤„ç†æ•°æ®ï¼šå¯èƒ½åŒ…å«åˆ†ç±»æ ‡ç­¾
        batch_data = next(iter(dataloader))
        if len(batch_data) == 3:
            images, masks, _ = batch_data
        else:
            images, masks = batch_data
        images, masks = images.to(device), masks.to(device)
        
        with torch.no_grad():
            outputs = model(images)
            preds = torch.sigmoid(outputs)
            preds = (preds > 0.5).float()
        
        num_samples = min(4, images.size(0))
        sample_triplets = []
        for i in range(num_samples):
            img = images[i].cpu().permute(1, 2, 0).numpy()
            img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])
            img = np.clip(img, 0, 1).astype(np.float32)
            true_mask = masks[i, 0].cpu().numpy().astype(np.float32)
            pred_mask = preds[i, 0].cpu().numpy().astype(np.float32)
            sample_triplets.append((img, true_mask, pred_mask))

        if self.enable_matlab_plots and self.matlab_viz_bridge:
            try:
                payload_path = self._save_matlab_viz_payload(
                    [triplet[0] for triplet in sample_triplets],
                    [triplet[1] for triplet in sample_triplets],
                    [triplet[2] for triplet in sample_triplets],
                    save_name
                )
                matlab_save_path = os.path.join(self.temp_dir, f"{save_name}_matlab.png")
                self.matlab_viz_bridge.render_prediction_grid(payload_path, matlab_save_path)
                return matlab_save_path
            except Exception as exc:
                print(f"[MATLAB Plot] ä½¿ç”¨matplotlibå›é€€: {exc}")

        plt.figure(figsize=(15, 10))
        for idx, (img, true_mask, pred_mask) in enumerate(sample_triplets):
            overlay = img.copy()
            overlay[true_mask == 1, 0] = 1
            overlay[pred_mask == 1, 1] = 1

            plt.subplot(num_samples, 4, idx * 4 + 1)
            plt.imshow(img)
            plt.title(f"æ ·æœ¬ {idx + 1}\nè¾“å…¥å›¾åƒ")
            plt.axis('off')

            plt.subplot(num_samples, 4, idx * 4 + 2)
            plt.imshow(true_mask, cmap='gray')
            plt.title("çœŸå®æ ‡ç­¾")
            plt.axis('off')

            plt.subplot(num_samples, 4, idx * 4 + 3)
            plt.imshow(pred_mask, cmap='gray')
            plt.title(f"é¢„æµ‹ç»“æœ\nDice: {self.calculate_dice(preds[idx], masks[idx]).item():.2f}")
            plt.axis('off')

            plt.subplot(num_samples, 4, idx * 4 + 4)
            plt.imshow(overlay)
            plt.title("å åŠ å›¾ï¼ˆçº¢:çœŸå®, ç»¿:é¢„æµ‹ï¼‰")
            plt.axis('off')

        plt.tight_layout()
        plt.savefig(save_path)
        plt.close()
        return save_path
    
    def plot_training_history(self):
        """ç»˜åˆ¶è®­ç»ƒå†å²æ›²çº¿"""
        save_path = os.path.join(self.temp_dir, "training_history.png")

        if self.enable_matlab_plots and self.matlab_viz_bridge:
            try:
                payload = self._save_training_history_payload()
                if payload:
                    matlab_path = os.path.join(self.temp_dir, "training_history_matlab.png")
                    self.matlab_viz_bridge.render_training_history(payload, matlab_path)
                    return matlab_path
            except Exception as exc:
                print(f"[MATLAB Plot] è®­ç»ƒå†å²å›é€€: {exc}")
        
        plt.figure(figsize=(12, 4))
        
        plt.subplot(1, 2, 1)
        plt.plot(self.train_loss_history, label='è®­ç»ƒæŸå¤±')
        plt.plot(self.val_loss_history, label='éªŒè¯æŸå¤±')
        plt.title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±')
        plt.xlabel('è½®æ¬¡')
        plt.ylabel('æŸå¤±')
        plt.legend()
        plt.grid(True)
        
        plt.subplot(1, 2, 2)
        plt.plot(self.val_dice_history, label='Diceç³»æ•°', color='green')
        plt.title('éªŒè¯Diceåˆ†æ•°')
        plt.xlabel('è½®æ¬¡')
        plt.ylabel('Diceåˆ†æ•°')
        plt.legend()
        plt.grid(True)
        
        plt.tight_layout()
        plt.savefig(save_path)
        plt.close()
        return save_path
    
    def find_optimal_threshold(self, model, dataloader, device, num_samples=50):
        """
        åœ¨éªŒè¯é›†ä¸Šå¯»æ‰¾æœ€ä¼˜äºŒå€¼åŒ–é˜ˆå€¼
        
        Args:
            num_samples: ç”¨äºæœç´¢çš„æ‰¹æ¬¡æ•°ï¼ˆNoneè¡¨ç¤ºä½¿ç”¨å…¨éƒ¨éªŒè¯é›†ï¼Œç¡®ä¿ä¸éªŒè¯é˜¶æ®µä¸€è‡´ï¼‰
        
        Returns:
            æœ€ä¼˜é˜ˆå€¼
        """
        model.eval()
        # å¦‚æœnum_samplesä¸ºNoneæˆ–0ï¼Œä½¿ç”¨å…¨éƒ¨éªŒè¯é›†ï¼ˆä¸éªŒè¯é˜¶æ®µä¿æŒä¸€è‡´ï¼‰
        use_all_samples = (num_samples is None or num_samples <= 0)
        if not use_all_samples:
            num_samples = max(1, int(num_samples))
        
        with torch.no_grad():
            all_probs = []
            all_masks = []
            
            for idx, batch_data in enumerate(dataloader):
                if not use_all_samples and idx >= num_samples:
                    break
                # å¤„ç†æ•°æ®ï¼šå¯èƒ½åŒ…å«åˆ†ç±»æ ‡ç­¾
                if len(batch_data) == 3:
                    images, masks, _ = batch_data
                else:
                    images, masks = batch_data
                images = images.to(device)
                masks = masks.to(device)
                
                # ä½¿ç”¨TTAè¿›è¡Œæ¨ç†ï¼ˆä¸éªŒè¯é˜¶æ®µä¸€è‡´ï¼‰
                # è¿™ç¡®ä¿é˜ˆå€¼ä¼˜åŒ–æ—¶ä½¿ç”¨çš„é¢„æµ‹ä¸éªŒè¯ç»Ÿè®¡æ—¶ä¸€è‡´
                outputs = self._tta_inference(model, images)
                probs = torch.sigmoid(outputs)
                # ç¡®ä¿ probs å’Œ masks çš„ç©ºé—´å°ºå¯¸åŒ¹é…
                if probs.shape[2:] != masks.shape[2:]:
                    probs = F.interpolate(probs, size=masks.shape[2:], mode='bilinear', align_corners=False)
                all_probs.append(probs.detach().cpu().numpy())
                all_masks.append(masks.detach().cpu().numpy())
            
            if not all_probs:
                return 0.5
            
            all_probs_np = np.concatenate(all_probs, axis=0)
            all_masks_np = np.concatenate(all_masks, axis=0)

            best_threshold, best_metrics = scan_best_threshold(all_probs_np, all_masks_np)

        sample_info = "å…¨éƒ¨éªŒè¯é›†" if use_all_samples else f"{num_samples}ä¸ªæ‰¹æ¬¡"
        score_val = best_metrics.get("score", 0.0) if isinstance(best_metrics, dict) else 0.0
        print(
            f"[é˜ˆå€¼ä¼˜åŒ–] ä½¿ç”¨æ ·æœ¬: {sample_info} | "
            f"æœ€ä¼˜é˜ˆå€¼: {best_threshold:.3f}, ç»¼åˆè¯„åˆ†: {score_val:.4f}, "
            f"Dice: {best_metrics.get('dice', float('nan')):.4f}, "
            f"IoU: {best_metrics.get('iou', float('nan')):.4f}"
        )
        return float(best_threshold)
    
    def evaluate_model(self, model, dataloader, device, use_tta=True, adaptive_threshold=True):
        """
        ç»¼åˆæ¨¡å‹è¯„ä¼°
        
        Args:
            use_tta: æ˜¯å¦ä½¿ç”¨æµ‹è¯•æ—¶å¢å¼º(TTA),å¯æå‡1-3%çš„Dice
            adaptive_threshold: æ˜¯å¦ä½¿ç”¨è‡ªé€‚åº”é˜ˆå€¼
        """
        # å¯»æ‰¾æœ€ä¼˜é˜ˆå€¼
        if adaptive_threshold:
            optimal_thresh = self.find_optimal_threshold(model, dataloader, device)
        else:
            optimal_thresh = 0.5
        self.last_optimal_threshold = float(optimal_thresh)
        
        model.eval()
        metrics = {
            'dice': [],
            'iou': [],
            'precision': [],
            'recall': [],
            'sensitivity': [],
            'specificity': [],
            'f1': [],
            'hd95': []
        }
        # å¾®å¹³å‡ç´¯ç§¯æ··æ·†çŸ©é˜µï¼Œä¿è¯æœ€ç»ˆæ˜¾ç¤ºçš„æŒ‡æ ‡ä¸€è‡´ï¼ˆDice=F1ï¼‰
        accum_tp = accum_fp = accum_fn = accum_tn = 0.0
        
        with torch.no_grad():
            for batch_data in tqdm(dataloader, desc="è¯„ä¼°ä¸­(TTA)" if use_tta else "è¯„ä¼°ä¸­"):
                # å¤„ç†æ•°æ®ï¼šå¯èƒ½åŒ…å«åˆ†ç±»æ ‡ç­¾
                if len(batch_data) == 3:
                    images, masks, _ = batch_data
                else:
                    images, masks = batch_data
                images, masks = images.to(device), masks.to(device)
                brain_mask = None
                if self.use_skull_stripper:
                    images, brain_mask = self._apply_skull_strip(images)
                
                if use_tta:
                    # æµ‹è¯•æ—¶å¢å¼º: 8ä¸ªå˜æ¢çš„å¹³å‡
                    outputs = self._tta_inference(model, images)
                else:
                    outputs = model(images)
                # ç¡®ä¿ outputs å’Œ masks çš„ç©ºé—´å°ºå¯¸åŒ¹é…
                if outputs.shape[2:] != masks.shape[2:]:
                    outputs = F.interpolate(outputs, size=masks.shape[2:], mode='bilinear', align_corners=False)
                if brain_mask is not None:
                    outputs = outputs * brain_mask
                
                preds = torch.sigmoid(outputs)
                preds = (preds > optimal_thresh).float()  # ä½¿ç”¨æœ€ä¼˜é˜ˆå€¼
                
                # åº”ç”¨åå¤„ç†ä¼˜åŒ–ï¼šå¡«å……å­”æ´ï¼Œä¸å†å¼ºåˆ¶åªä¿ç•™æœ€å¤§è¿é€šåŸŸ
                for i in range(preds.shape[0]):
                    preds[i, 0] = self.post_process_mask(
                        preds[i, 0], 
                        min_size=30, 
                        use_morphology=True,
                        keep_largest=False,  # å…è®¸å¤šå‘ç—…ç¶åŒæ—¶å­˜åœ¨
                        fill_holes=True     # å¡«å……å­”æ´ï¼Œå»é™¤å‡é˜´æ€§ç©ºæ´
                    )
                
                # è®¡ç®—æ‰¹æ¬¡ä¸­æ¯ä¸ªå›¾åƒçš„æŒ‡æ ‡
                for i in range(preds.shape[0]):
                    pred = preds[i, 0]
                    mask = masks[i, 0]
                    
                    # åŒé‡æ£€æŸ¥å°ºå¯¸åŒ¹é…ï¼ˆä»¥é˜²åå¤„ç†æ”¹å˜äº†å°ºå¯¸ï¼‰
                    if pred.shape != mask.shape:
                        pred = F.interpolate(pred.unsqueeze(0).unsqueeze(0), size=mask.shape, mode='bilinear', align_corners=False).squeeze(0).squeeze(0)

                    # è®¡ç®—æ··æ·†çŸ©é˜µçš„å››ä¸ªåŸºæœ¬å€¼
                    tp = float((pred * mask).sum().item())
                    pred_sum = float(pred.sum().item())   # TP + FP
                    mask_sum = float(mask.sum().item())   # TP + FN
                    fp = float((pred * (1 - mask)).sum().item())
                    fn = float(((1 - pred) * mask).sum().item())
                    tn = float(((1 - pred) * (1 - mask)).sum().item())
                    
                    # éªŒè¯: tp + fp = pred_sum, tp + fn = mask_sum
                    assert abs((tp + fp) - pred_sum) < 1e-5, f"TP+FPè®¡ç®—é”™è¯¯: {tp+fp} vs {pred_sum}"
                    assert abs((tp + fn) - mask_sum) < 1e-5, f"TP+FNè®¡ç®—é”™è¯¯: {tp+fn} vs {mask_sum}"
                    
                    # Dice = 2TP / (2TP + FP + FN)
                    dice_den = 2.0 * tp + fp + fn
                    if dice_den < 1e-7:
                        dice = 1.0 if (mask_sum < 1e-7 and pred_sum < 1e-7) else 0.0
                    else:
                        dice = (2.0 * tp) / dice_den
                    
                    # IoU = TP / (TP + FP + FN)
                    union = tp + fp + fn
                    if union < 1e-7:
                        iou = 1.0 if (mask_sum < 1e-7 and pred_sum < 1e-7) else 0.0
                    else:
                        iou = tp / union
                    
                    # Precision = TP / (TP + FP)
                    if tp + fp < 1e-7:
                        precision = 1.0 if mask_sum < 1e-7 else 0.0
                    else:
                        precision = tp / (tp + fp)
                    
                    # Recall/Sensitivity = TP / (TP + FN)
                    if tp + fn < 1e-7:
                        recall = 1.0 if pred_sum < 1e-7 else 0.0
                    else:
                        recall = tp / (tp + fn)
                    
                            # Specificity = TN / (TN + FP)
                    tn_plus_fp = tn + fp
                    specificity = 1.0 if tn_plus_fp < 1e-7 else tn / tn_plus_fp
                    
                    # F1åœ¨äºŒåˆ†ç±»ä¸‹åº”ä¸Diceä¸€è‡´ï¼Œè¿™é‡Œç›´æ¥å¤ç”¨
                    f1 = dice
                    
                    # è®¡ç®—HD95
                    if mask_sum < 1e-7:
                        hd95 = 0.0 if pred_sum < 1e-7 else float('inf')
                    elif pred_sum < 1e-7:
                        hd95 = float('inf')
                    else:
                        hd95 = calculate_hd95(
                            pred.detach().cpu().numpy(),
                            mask.detach().cpu().numpy()
                        )

                    metrics['dice'].append(float(dice))
                    metrics['iou'].append(float(iou))
                    metrics['precision'].append(float(precision))
                    metrics['recall'].append(float(recall))
                    metrics['sensitivity'].append(float(recall))
                    metrics['specificity'].append(float(specificity))
                    metrics['f1'].append(float(f1))
                    metrics['hd95'].append(hd95 if not np.isinf(hd95) else 0.0)
                    
                    accum_tp += tp
                    accum_fp += fp
                    accum_fn += fn
                    accum_tn += tn
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡ï¼Œå¿½ç•¥nanå€¼
        metrics_arrays = {k: np.array(v, dtype=float) for k, v in metrics.items()}
        avg_metrics = {}
        std_metrics = {}
        min_metrics = {}
        max_metrics = {}
        median_metrics = {}
        for k, arr in metrics_arrays.items():
            if arr.size == 0 or np.all(np.isnan(arr)):
                avg_metrics[k] = float('nan')
                std_metrics[k] = float('nan')
                min_metrics[k] = float('nan')
                max_metrics[k] = float('nan')
                median_metrics[k] = float('nan')
            else:
                avg_metrics[k] = float(np.nanmean(arr))
                std_metrics[k] = float(np.nanstd(arr))
                min_metrics[k] = float(np.nanmin(arr))
                max_metrics[k] = float(np.nanmax(arr))
                median_metrics[k] = float(np.nanmedian(arr))
        
        # å¾®å¹³å‡ï¼ˆglobalï¼‰æŒ‡æ ‡ï¼Œä½¿ç”¨ç´¯ç§¯çš„æ··æ·†çŸ©é˜µç¡®ä¿å„æŒ‡æ ‡ä¸€è‡´
        micro_metrics = {}
        dice_den = 2 * accum_tp + accum_fp + accum_fn
        micro_metrics['dice'] = 1.0 if dice_den < 1e-7 else (2 * accum_tp) / dice_den
        
        iou_den = accum_tp + accum_fp + accum_fn
        micro_metrics['iou'] = 1.0 if iou_den < 1e-7 else accum_tp / iou_den
        
        prec_den = accum_tp + accum_fp
        micro_metrics['precision'] = 1.0 if prec_den < 1e-7 else accum_tp / prec_den
        
        rec_den = accum_tp + accum_fn
        micro_metrics['recall'] = 1.0 if rec_den < 1e-7 else accum_tp / rec_den
        micro_metrics['sensitivity'] = micro_metrics['recall']
        
        spec_den = accum_tn + accum_fp
        micro_metrics['specificity'] = 1.0 if spec_den < 1e-7 else accum_tn / spec_den
        
        micro_metrics['f1'] = micro_metrics['dice']  # äºŒåˆ†ç±»ä¸‹F1=Dice
        micro_metrics['hd95'] = float(np.nanmean(metrics_arrays['hd95'])) if metrics_arrays['hd95'].size > 0 else float('nan')
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        detailed_metrics = {
            'average': avg_metrics,
            'std': std_metrics,
            'min': min_metrics,
            'max': max_metrics,
            'median': median_metrics,
            'all_samples': metrics
        }
        # è¦†ç›–å¹³å‡å€¼ä¸ºå¾®å¹³å‡ï¼Œç¡®ä¿æ˜¾ç¤ºä¸€è‡´
        for k, v in micro_metrics.items():
            detailed_metrics['average'][k] = float(v)
        
        # ä¿å­˜æŒ‡æ ‡åˆ°CSV
        metrics_path = os.path.join(self.temp_dir, 'performance_metrics.csv')
        pd.DataFrame(metrics).to_csv(metrics_path, index=False)
        
        return detailed_metrics, metrics_path
    
    def evaluate_model_ensemble(self, models, dataloader, device, use_tta=True, adaptive_threshold=True):
        """æ¨¡å‹é›†æˆåŠŸèƒ½å·²å–æ¶ˆã€‚"""
        raise RuntimeError("æ¨¡å‹é›†æˆåŠŸèƒ½å·²å–æ¶ˆ")
    
    def find_optimal_threshold_ensemble(self, *args, **kwargs):
        """æ¨¡å‹é›†æˆåŠŸèƒ½å·²å–æ¶ˆã€‚"""
        raise RuntimeError("æ¨¡å‹é›†æˆåŠŸèƒ½å·²å–æ¶ˆ")
    
    def evaluate_per_volume(self, model, dataloader, device, patient_slice_index=None, patients=None, use_tta=True):
        """
        æŒ‰volumeè¯„ä¼°ï¼ˆå‚è€ƒæ ‡å‡†ä»£ç ï¼‰
        å°†åŒä¸€ç—…äººçš„æ‰€æœ‰sliceç»„ç»‡æˆvolumeï¼Œç„¶åè®¡ç®—æ¯ä¸ªvolumeçš„Dice
        è¿™ç§æ–¹å¼æ›´ç¬¦åˆä¸´åºŠè¯„ä¼°ä¹ æƒ¯
        
        Args:
            model: æ¨¡å‹
            dataloader: æ•°æ®åŠ è½½å™¨
            patient_slice_index: ç—…äºº-åˆ‡ç‰‡ç´¢å¼•åˆ—è¡¨ [(patient_idx, slice_idx), ...]
            patients: ç—…äººIDåˆ—è¡¨
            use_tta: æ˜¯å¦ä½¿ç”¨æµ‹è¯•æ—¶å¢å¼º
        
        Returns:
            volume_metrics: æ¯ä¸ªvolumeçš„æŒ‡æ ‡å­—å…¸
            avg_dice: å¹³å‡Diceï¼ˆæŒ‰volumeï¼‰
        """
        model.eval()
        
        # å¦‚æœæ²¡æœ‰æä¾›patient_slice_indexï¼Œå°è¯•ä»datasetè·å–
        if patient_slice_index is None:
            if hasattr(dataloader.dataset, 'patient_slice_index'):
                patient_slice_index = dataloader.dataset.patient_slice_index
            elif hasattr(dataloader.dataset, 'image_paths'):
                # ä»è·¯å¾„æ¨æ–­ç—…äººID
                patient_slice_index = []
                for i, path in enumerate(dataloader.dataset.image_paths):
                    # å°è¯•ä»è·¯å¾„æå–ç—…äººIDå’Œåˆ‡ç‰‡åºå·
                    base = os.path.splitext(os.path.basename(path))[0]
                    parts = base.split('_')
                    if len(parts) >= 2:
                        patient_id = '_'.join(parts[:-1])
                        try:
                            slice_idx = int(parts[-1])
                            patient_slice_index.append((patient_id, slice_idx))
                        except ValueError:
                            patient_slice_index.append((base, 0))
                    else:
                        patient_slice_index.append((base, i))
        
        if patients is None:
            if hasattr(dataloader.dataset, 'patients'):
                patients = dataloader.dataset.patients
            else:
                # ä»patient_slice_indexæå–å”¯ä¸€ç—…äººID
                patients = sorted(list(set([p[0] for p in patient_slice_index])))
        
        # æ”¶é›†æ‰€æœ‰é¢„æµ‹å’ŒçœŸå®å€¼
        all_preds = []
        all_trues = []
        all_inputs = []
        
        with torch.no_grad():
            for batch_data in tqdm(dataloader, desc="æŒ‰volumeè¯„ä¼°"):
                if len(batch_data) == 3:
                    images, masks, _ = batch_data
                else:
                    images, masks = batch_data
                images, masks = images.to(device), masks.to(device)
                
                if use_tta:
                    outputs = self._tta_inference(model, images)
                else:
                    outputs = model(images)
                
                probs = torch.sigmoid(outputs)
                preds = (probs > self.last_optimal_threshold).float()
                
                # æ™ºèƒ½åå¤„ç†ï¼šå…ˆæŒ‰é¢ç§¯+æ¦‚ç‡è¿‡æ»¤å¾®å°ç—…ç¶/å™ªç‚¹ï¼Œå†è¿›è¡Œå½¢æ€å­¦ä¼˜åŒ–
                for i in range(preds.shape[0]):
                    pred_mask_tensor = preds[i, 0]
                    prob_map_tensor = probs[i, 0]
                    # å…ˆæ‰§è¡Œæ™ºèƒ½åå¤„ç†ï¼ˆä¸å†ç®€å•æŒ‰min_sizeè£å‰ªï¼‰
                    pred_mask_tensor = self.smart_post_processing(pred_mask_tensor, prob_map_tensor)
                    # å†æ‰§è¡Œä¼ ç»Ÿå½¢æ€å­¦åå¤„ç†ï¼Œä½†ä¸ç§»é™¤å°åŒºåŸŸï¼ˆmin_size=0ï¼‰
                    pred_mask_processed = self.post_process_mask(
                        pred_mask_tensor,
                        min_size=0,
                        use_morphology=True,
                        keep_largest=False,  # å…è®¸å¤šå‘ç—…ç¶åŒæ—¶å­˜åœ¨
                        fill_holes=True     # å¡«å……å­”æ´ï¼Œå»é™¤å‡é˜´æ€§ç©ºæ´
                    )
                    preds[i, 0] = pred_mask_processed
                
                all_preds.extend([preds[i].cpu().numpy() for i in range(preds.shape[0])])
                all_trues.extend([masks[i].cpu().numpy() for i in range(masks.shape[0])])
                all_inputs.extend([images[i].cpu().numpy() for i in range(images.shape[0])])
        
        # æŒ‰volumeç»„ç»‡æ•°æ®
        if patient_slice_index:
            from collections import OrderedDict
            slice_counter = OrderedDict()
            for pid, _ in patient_slice_index:
                slice_counter[pid] = slice_counter.get(pid, 0) + 1
            patient_order = list(slice_counter.keys())
            num_slices = [slice_counter[pid] for pid in patient_order]
            patients = patient_order
        else:
            # å¦‚æœæ— æ³•æ¨æ–­ï¼Œå‡è®¾æ¯ä¸ªæ ·æœ¬æ˜¯ä¸€ä¸ªvolume
            num_slices = np.ones(len(all_preds), dtype=int)
        
        # è®¡ç®—æ¯ä¸ªvolumeçš„Dice
        volume_dice_list = []
        volume_metrics = {}
        index = 0
        
        for p_idx, patient_id in enumerate(patients):
            if p_idx >= len(num_slices):
                break
            num_s = num_slices[p_idx] if p_idx < len(num_slices) else 1
            
            volume_pred = np.array(all_preds[index:index + num_s])
            volume_true = np.array(all_trues[index:index + num_s])
            
            # è®¡ç®—volumeçº§åˆ«çš„Dice
            volume_dice = self._dice_per_volume(volume_pred, volume_true)
            volume_dice_list.append(volume_dice)
            volume_metrics[patient_id] = {
                'dice': float(volume_dice),
                'num_slices': int(num_s)
            }
            
            index += num_s
        
        avg_dice = np.mean(volume_dice_list) if volume_dice_list else 0.0
        
        return volume_metrics, avg_dice
    
    def _dice_per_volume(self, y_pred, y_true):
        """
        è®¡ç®—volumeçº§åˆ«çš„Diceç³»æ•°ï¼ˆå‚è€ƒæ ‡å‡†ä»£ç ï¼‰
        
        Args:
            y_pred: é¢„æµ‹maskæ•°ç»„ (N, C, H, W) æˆ– (N, H, W)
            y_true: çœŸå®maskæ•°ç»„ (N, C, H, W) æˆ– (N, H, W)
        
        Returns:
            diceç³»æ•°
        """
        # å±•å¹³å¹¶äºŒå€¼åŒ–
        if len(y_pred.shape) == 4:
            y_pred = y_pred[:, 0]  # å–ç¬¬ä¸€ä¸ªé€šé“
        if len(y_true.shape) == 4:
            y_true = y_true[:, 0]
        
        y_pred = np.round(y_pred).astype(int).flatten()
        y_true = np.round(y_true).astype(int).flatten()
        
        # è®¡ç®—Dice
        intersection = np.sum(y_pred * y_true)
        union = np.sum(y_pred) + np.sum(y_true)
        
        if union == 0:
            return 1.0  # å¦‚æœä¸¤è€…éƒ½æ˜¯å…¨é›¶ï¼ŒDice=1
        
        dice = 2.0 * intersection / union
        return float(dice)

    def evaluate_classification_model(self, model, dataloader, device):
        """è¯„ä¼°åˆ†ç±»æ¨¡å‹ï¼Œå¹¶è‡ªåŠ¨å¯»æ‰¾æœ€ä¼˜é˜ˆå€¼"""
        model.eval()
        correct = 0
        total = 0
        all_preds = []
        all_labels = []
        all_probs = []  # å­˜å‚¨æ‰€æœ‰æ¦‚ç‡å€¼ï¼Œç”¨äºå¯»æ‰¾æœ€ä¼˜é˜ˆå€¼
        
        with torch.no_grad():
            for batch_data in tqdm(dataloader, desc="è¯„ä¼°åˆ†ç±»æ¨¡å‹"):
                # å¤„ç†æ•°æ®ï¼šå¯èƒ½åŒ…å«åˆ†ç±»æ ‡ç­¾
                if len(batch_data) == 3:
                    images, masks, labels = batch_data
                    images, labels = images.to(device), labels.to(device)
                else:
                    # å¦‚æœæ²¡æœ‰åˆ†ç±»æ ‡ç­¾ï¼Œä»maskç”Ÿæˆï¼ˆmaskæœ‰åƒç´ åˆ™label=1ï¼Œå¦åˆ™label=0ï¼‰
                    images, masks = batch_data
                    images = images.to(device)
                    labels = (masks.sum(dim=[1, 2, 3]) > 0).long().to(device)
                outputs = model(images)
                probs = torch.softmax(outputs, dim=1)
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
                all_preds.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                all_probs.extend(probs[:, 1].cpu().numpy())  # æœ‰ç—…å˜çš„æ¦‚ç‡
        
        accuracy = 100.0 * correct / total if total > 0 else 0.0
        
        # è®¡ç®—æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(all_labels, all_preds)
        report = classification_report(all_labels, all_preds, target_names=['æ— ç—…å˜', 'æœ‰ç—…å˜'], output_dict=True)
        
        # è‡ªåŠ¨å¯»æ‰¾æœ€ä¼˜åˆ†ç±»é˜ˆå€¼ï¼ˆåŸºäºF1åˆ†æ•°ï¼‰
        optimal_threshold = 0.5
        best_f1 = 0.0
        if len(all_probs) > 0 and len(all_labels) > 0:
            thresholds = np.arange(0.3, 0.8, 0.05)
            for thresh in thresholds:
                thresh_preds = (np.array(all_probs) > thresh).astype(int)
                if len(np.unique(thresh_preds)) > 1:  # ç¡®ä¿æœ‰æ­£è´Ÿæ ·æœ¬
                    from sklearn.metrics import f1_score
                    f1 = f1_score(all_labels, thresh_preds)
                    if f1 > best_f1:
                        best_f1 = f1
                        optimal_threshold = thresh
        
        metrics = {
            'accuracy': accuracy,
            'confusion_matrix': cm.tolist(),
            'classification_report': report,
            'optimal_threshold': float(optimal_threshold),
            'best_f1_at_threshold': float(best_f1)
        }
        
        return metrics

    def evaluate_two_stage_system(self, classification_model, segmentation_model, dataloader, device, 
                                   classification_threshold=0.5, segmentation_threshold=0.5, use_tta=True,
                                   use_adaptive_strategy=True, confidence_threshold=0.9):
        """
        è¯„ä¼°ä¸¤é˜¶æ®µç³»ç»Ÿï¼ˆåˆ†ç±»+åˆ†å‰²ï¼‰- æ”¹è¿›çš„çº§è”ç­–ç•¥
        
        Args:
            classification_model: åˆ†ç±»æ¨¡å‹
            segmentation_model: åˆ†å‰²æ¨¡å‹
            dataloader: æ•°æ®åŠ è½½å™¨ï¼ˆéœ€è¦è¿”å›åˆ†ç±»æ ‡ç­¾ï¼‰
            device: è®¾å¤‡
            classification_threshold: åˆ†ç±»é˜ˆå€¼ï¼ˆlogitsçš„softmaxåï¼Œç±»åˆ«1çš„æ¦‚ç‡ï¼‰
            segmentation_threshold: åˆ†å‰²é˜ˆå€¼
            use_tta: æ˜¯å¦ä½¿ç”¨æµ‹è¯•æ—¶å¢å¼º
            use_adaptive_strategy: æ˜¯å¦ä½¿ç”¨è‡ªé€‚åº”ç­–ç•¥ï¼ˆåªå¯¹é«˜ç½®ä¿¡åº¦çš„æ— ç—…å˜æ ·æœ¬è·³è¿‡åˆ†å‰²ï¼‰
            confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆåªæœ‰æ— ç—…å˜æ¦‚ç‡>æ­¤å€¼æ‰è·³è¿‡åˆ†å‰²ï¼‰
        """
        classification_model.eval()
        segmentation_model.eval()
        
        # åˆ†ç±»æŒ‡æ ‡
        cls_correct = 0
        cls_total = 0
        cls_preds = []
        cls_labels = []
        
        # åˆ†å‰²æŒ‡æ ‡ï¼ˆåªå¯¹åˆ†ç±»ä¸ºæœ‰ç—…å˜çš„å›¾åƒè®¡ç®—ï¼Œç”¨äºè¯„ä¼°åˆ†å‰²æ¨¡å‹æœ¬èº«ï¼‰
        seg_metrics = {
            'dice': [],
            'iou': [],
            'precision': [],
            'recall': [],
            'f1': []
        }
        
        # ç³»ç»Ÿæ•´ä½“æŒ‡æ ‡ï¼ˆè®¡ç®—æ‰€æœ‰æ ·æœ¬çš„æœ€ç»ˆè¾“å‡ºï¼ŒåŒ…æ‹¬åˆ†ç±»é”™è¯¯çš„æƒ…å†µï¼‰
        system_dice_list = []  # ç³»ç»Ÿæ•´ä½“Diceï¼ˆæ‰€æœ‰æ ·æœ¬ï¼‰
        system_iou_list = []
        system_precision_list = []
        system_recall_list = []
        
        # ç»Ÿè®¡ä¿¡æ¯
        skip_count = 0  # è·³è¿‡åˆ†å‰²çš„æ ·æœ¬æ•°
        total_count = 0
        
        # æ•´ä½“ç³»ç»ŸæŒ‡æ ‡
        system_metrics = {
            'true_positive': 0,  # æ­£ç¡®åˆ†ç±»ä¸ºæœ‰ç—…å˜ä¸”åˆ†å‰²æ­£ç¡®
            'false_positive': 0,   # é”™è¯¯åˆ†ç±»ä¸ºæœ‰ç—…å˜
            'false_negative': 0,  # é”™è¯¯åˆ†ç±»ä¸ºæ— ç—…å˜ï¼ˆæ¼æ£€ï¼‰
            'true_negative': 0   # æ­£ç¡®åˆ†ç±»ä¸ºæ— ç—…å˜
        }
        
        with torch.no_grad():
            for batch in tqdm(dataloader, desc="è¯„ä¼°ä¸¤é˜¶æ®µç³»ç»Ÿï¼ˆæ”¹è¿›çº§è”ç­–ç•¥ï¼‰"):
                if len(batch) == 3:
                    images, masks, labels = batch
                    images, masks, labels = images.to(device), masks.to(device), labels.to(device)
                else:
                    images, masks = batch
                    images, masks = images.to(device), masks.to(device)
                    # ä»maskç”Ÿæˆæ ‡ç­¾
                    labels = (masks.sum(dim=[1, 2, 3]) > 0).long()
                
                # ç¬¬ä¸€é˜¶æ®µï¼šåˆ†ç±»
                cls_outputs = classification_model(images)
                cls_probs = torch.softmax(cls_outputs, dim=1)
                cls_prob_lesion = cls_probs[:, 1]  # æœ‰ç—…å˜çš„æ¦‚ç‡
                cls_prob_normal = cls_probs[:, 0]  # æ— ç—…å˜çš„æ¦‚ç‡
                
                # æ”¹è¿›çš„çº§è”ç­–ç•¥
                if use_adaptive_strategy:
                    # è‡ªé€‚åº”ç­–ç•¥ï¼šåªå¯¹é«˜ç½®ä¿¡åº¦çš„æ— ç—…å˜æ ·æœ¬è·³è¿‡åˆ†å‰²
                    # 1. æœ‰ç—…å˜æ¦‚ç‡ > classification_threshold â†’ è¿›è¡Œåˆ†å‰²
                    # 2. æ— ç—…å˜æ¦‚ç‡ > confidence_threshold â†’ è·³è¿‡åˆ†å‰²ï¼ˆé«˜ç½®ä¿¡åº¦æ— ç—…å˜ï¼‰
                    # 3. å…¶ä»–æƒ…å†µï¼ˆä¸ç¡®å®šï¼‰â†’ ä»ç„¶è¿›è¡Œåˆ†å‰²ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
                    need_segmentation = (cls_prob_lesion > classification_threshold) | (cls_prob_normal < confidence_threshold)
                    cls_predicted = (cls_prob_lesion > classification_threshold).long()
                else:
                    # åŸå§‹ç­–ç•¥ï¼šåªå¯¹åˆ†ç±»ä¸ºæœ‰ç—…å˜çš„è¿›è¡Œåˆ†å‰²
                    cls_predicted = (cls_prob_lesion > classification_threshold).long()
                    need_segmentation = cls_predicted == 1
                
                cls_total += labels.size(0)
                cls_correct += (cls_predicted == labels).sum().item()
                cls_preds.extend(cls_predicted.cpu().numpy())
                cls_labels.extend(labels.cpu().numpy())
                
                batch_size = images.size(0)
                total_count += batch_size
                
                # åˆå§‹åŒ–ç³»ç»Ÿæœ€ç»ˆè¾“å‡ºï¼ˆå…¨é›¶maskï¼‰
                system_final_preds = torch.zeros_like(masks)
                
                # ç»Ÿè®¡è·³è¿‡çš„æ ·æœ¬
                skip_count += (need_segmentation == False).sum().item()
                
                if need_segmentation.any():
                    seg_images = images[need_segmentation]
                    seg_masks = masks[need_segmentation]
                    seg_labels = labels[need_segmentation]
                    
                    if use_tta:
                        seg_outputs = self._tta_inference(segmentation_model, seg_images)
                    else:
                        seg_outputs = segmentation_model(seg_images)
                    
                    # ç¡®ä¿ seg_outputs å’Œ seg_masks çš„ç©ºé—´å°ºå¯¸åŒ¹é…
                    if seg_outputs.shape[2:] != seg_masks.shape[2:]:
                        seg_outputs = F.interpolate(seg_outputs, size=seg_masks.shape[2:], mode='bilinear', align_corners=False)
                    
                    seg_preds = torch.sigmoid(seg_outputs)
                    seg_preds = (seg_preds > segmentation_threshold).float()
                    
                    # å°†åˆ†å‰²ç»“æœå¡«å…¥ç³»ç»Ÿæœ€ç»ˆè¾“å‡º
                    seg_idx = 0
                    for i in range(batch_size):
                        if need_segmentation[i]:
                            system_final_preds[i] = seg_preds[seg_idx]
                            seg_idx += 1
                        # å¦‚æœè·³è¿‡åˆ†å‰²ï¼Œä¿æŒå…¨é›¶maskï¼ˆç³»ç»Ÿæœ€ç»ˆè¾“å‡ºï¼‰
                    
                    # è®¡ç®—åˆ†å‰²æŒ‡æ ‡ï¼ˆåªå¯¹è¿›è¡Œåˆ†å‰²çš„æ ·æœ¬ï¼Œç”¨äºè¯„ä¼°åˆ†å‰²æ¨¡å‹æœ¬èº«ï¼‰
                    for i in range(seg_preds.shape[0]):
                        pred = seg_preds[i, 0]
                        mask = seg_masks[i, 0]
                        
                        # åŒé‡æ£€æŸ¥å°ºå¯¸åŒ¹é…ï¼ˆä»¥é˜²ä¸‡ä¸€ï¼‰
                        if pred.shape != mask.shape:
                            pred = F.interpolate(pred.unsqueeze(0).unsqueeze(0), size=mask.shape, mode='bilinear', align_corners=False).squeeze(0).squeeze(0)
                        
                        pred_sum = float(pred.sum().item())
                        mask_sum = float(mask.sum().item())
                        intersection = float((pred * mask).sum().item())
                        
                    if mask_sum > 1e-7 or pred_sum > 1e-7:
                        # æ ‡å‡†æ··æ·†çŸ©é˜µå®šä¹‰ï¼Œç¡®ä¿ä¸ä¸»è¯„ä¼°ä¸€è‡´
                        tp = intersection
                        fp = float((pred * (1 - mask)).sum().item())
                        fn = float(((1 - pred) * mask).sum().item())
                        tn = float(((1 - pred) * (1 - mask)).sum().item())
                        
                        dice_den = 2.0 * tp + fp + fn
                        dice = 1.0 if dice_den < 1e-7 else (2.0 * tp) / dice_den
                        
                        union = tp + fp + fn
                        iou = 1.0 if union < 1e-7 else tp / union
                        
                        precision = 1.0 if (tp + fp) < 1e-7 else tp / (tp + fp)
                        recall = 1.0 if (tp + fn) < 1e-7 else tp / (tp + fn)
                        specificity = 1.0 if (tn + fp) < 1e-7 else tn / (tn + fp)
                        f1 = dice
                        
                        seg_metrics['dice'].append(float(dice))
                        seg_metrics['iou'].append(float(iou))
                        seg_metrics['precision'].append(float(precision))
                        seg_metrics['recall'].append(float(recall))
                        seg_metrics['f1'].append(float(f1))
                
                # è®¡ç®—ç³»ç»Ÿæ•´ä½“Diceï¼ˆæ‰€æœ‰æ ·æœ¬ï¼ŒåŒ…æ‹¬åˆ†ç±»é”™è¯¯çš„æƒ…å†µï¼‰
                for i in range(batch_size):
                    system_pred = system_final_preds[i, 0]
                    true_mask = masks[i, 0]
                    
                    # åŒé‡æ£€æŸ¥å°ºå¯¸åŒ¹é…ï¼ˆä»¥é˜²ä¸‡ä¸€ï¼‰
                    if system_pred.shape != true_mask.shape:
                        system_pred = F.interpolate(system_pred.unsqueeze(0).unsqueeze(0), size=true_mask.shape, mode='bilinear', align_corners=False).squeeze(0).squeeze(0)
                    
                    pred_sum = float(system_pred.sum().item())
                    mask_sum = float(true_mask.sum().item())
                    intersection = float((system_pred * true_mask).sum().item())
                    
                    # è®¡ç®—ç³»ç»Ÿæ•´ä½“Diceï¼ˆåŒ…æ‹¬ç©ºmaskçš„æƒ…å†µï¼‰
                    if mask_sum > 1e-7 or pred_sum > 1e-7:
                        dice = self._safe_dice_score(system_pred, true_mask)
                        total = pred_sum + mask_sum
                        union = total - intersection
                        iou = (intersection + 1e-7) / (union + 1e-7) if union > 1e-7 else 0.0
                        precision = (intersection + 1e-7) / (pred_sum + 1e-7) if pred_sum > 1e-7 else 0.0
                        recall = (intersection + 1e-7) / (mask_sum + 1e-7) if mask_sum > 1e-7 else 0.0
                        
                        system_dice_list.append(float(dice))
                        system_iou_list.append(float(iou))
                        system_precision_list.append(float(precision))
                        system_recall_list.append(float(recall))
                
                # è®¡ç®—æ•´ä½“ç³»ç»ŸæŒ‡æ ‡
                for i in range(labels.size(0)):
                    true_label = labels[i].item()
                    pred_label = cls_predicted[i].item()
                    
                    if true_label == 1 and pred_label == 1:
                        system_metrics['true_positive'] += 1
                    elif true_label == 0 and pred_label == 1:
                        system_metrics['false_positive'] += 1
                    elif true_label == 1 and pred_label == 0:
                        system_metrics['false_negative'] += 1
                    else:
                        system_metrics['true_negative'] += 1
        
        # è®¡ç®—åˆ†ç±»å‡†ç¡®ç‡
        cls_accuracy = 100.0 * cls_correct / cls_total if cls_total > 0 else 0.0
        
        # è®¡ç®—åˆ†ç±»æ··æ·†çŸ©é˜µ
        cls_labels_arr = np.array(cls_labels)
        cls_preds_arr = np.array(cls_preds)
        cls_confusion_matrix = {
            'true_positive': int(((cls_labels_arr == 1) & (cls_preds_arr == 1)).sum()),
            'false_positive': int(((cls_labels_arr == 0) & (cls_preds_arr == 1)).sum()),
            'false_negative': int(((cls_labels_arr == 1) & (cls_preds_arr == 0)).sum()),
            'true_negative': int(((cls_labels_arr == 0) & (cls_preds_arr == 0)).sum())
        }
        
        # è®¡ç®—åˆ†å‰²å¹³å‡æŒ‡æ ‡ï¼ˆåªå¯¹åˆ†ç±»ä¸ºæœ‰ç—…å˜çš„æ ·æœ¬ï¼Œç”¨äºè¯„ä¼°åˆ†å‰²æ¨¡å‹æœ¬èº«ï¼‰
        seg_avg_metrics = {}
        for k, v in seg_metrics.items():
            if v:
                seg_avg_metrics[k] = float(np.mean(v))
            else:
                seg_avg_metrics[k] = 0.0
        
        # è®¡ç®—ç³»ç»Ÿæ•´ä½“DiceæŒ‡æ ‡ï¼ˆæ‰€æœ‰æ ·æœ¬ï¼ŒåŒ…æ‹¬åˆ†ç±»é”™è¯¯çš„æƒ…å†µï¼‰
        system_dice_avg = float(np.mean(system_dice_list)) if system_dice_list else 0.0
        system_iou_avg = float(np.mean(system_iou_list)) if system_iou_list else 0.0
        system_precision_avg = float(np.mean(system_precision_list)) if system_precision_list else 0.0
        system_recall_avg = float(np.mean(system_recall_list)) if system_recall_list else 0.0
        
        # è®¡ç®—æ•ˆç‡æå‡
        skip_ratio = skip_count / total_count if total_count > 0 else 0.0
        
        # è®¡ç®—æ•´ä½“ç³»ç»ŸæŒ‡æ ‡
        total_samples = (system_metrics['true_positive'] + system_metrics['false_positive'] + 
                         system_metrics['false_negative'] + system_metrics['true_negative'])
        
        system_accuracy = 100.0 * (system_metrics['true_positive'] + system_metrics['true_negative']) / total_samples if total_samples > 0 else 0.0
        system_precision = system_metrics['true_positive'] / (system_metrics['true_positive'] + system_metrics['false_positive'] + 1e-7)
        system_recall = system_metrics['true_positive'] / (system_metrics['true_positive'] + system_metrics['false_negative'] + 1e-7)
        system_f1 = 2 * system_precision * system_recall / (system_precision + system_recall + 1e-7)
        
        results = {
            'classification': {
                'accuracy': cls_accuracy,
                'confusion_matrix': cls_confusion_matrix
            },
            'segmentation': seg_avg_metrics,  # åˆ†å‰²æ¨¡å‹æŒ‡æ ‡ï¼ˆåªå¯¹è¿›è¡Œåˆ†å‰²çš„æ ·æœ¬ï¼‰
            'system': {
                'accuracy': system_accuracy,
                'precision': system_precision,
                'recall': system_recall,
                'f1': system_f1,
                'dice': system_dice_avg,  # ç³»ç»Ÿæ•´ä½“Diceï¼ˆæ‰€æœ‰æ ·æœ¬ï¼‰
                'iou': system_iou_avg,
                'segmentation_precision': system_precision_avg,
                'segmentation_recall': system_recall_avg,
                'confusion_matrix': system_metrics,
                'efficiency': {
                    'skip_ratio': skip_ratio,  # è·³è¿‡åˆ†å‰²çš„æ ·æœ¬æ¯”ä¾‹
                    'computation_saved': skip_ratio * 100  # èŠ‚çœçš„è®¡ç®—ç™¾åˆ†æ¯”
                }
            }
        }
        
        return results
    
    def visualize_test_results(self, model, dataloader, device, num_samples=8, use_tta=True):
        """å¯è§†åŒ–æµ‹è¯•é›†ä¸Šçš„åˆ†å‰²ç»“æœï¼ŒåŒ…å«åŸå›¾ã€çœŸå®maskã€é¢„æµ‹maskå’Œå¯¹æ¯”å›¾
        
        Args:
            use_tta: æ˜¯å¦ä½¿ç”¨æµ‹è¯•æ—¶å¢å¼ºï¼ˆé»˜è®¤Trueï¼Œè®­ç»ƒç»“æŸåçš„æµ‹è¯•æ¨èä½¿ç”¨ï¼‰
        """
        save_path = os.path.join(self.temp_dir, "test_results_visualization.png")
        model.eval()
        
        # æ”¶é›†æ ·æœ¬
        all_images = []
        all_masks = []
        all_preds = []
        all_metrics = []
        
        with torch.no_grad():
            for batch_data in dataloader:
                # å¤„ç†æ•°æ®ï¼šå¯èƒ½åŒ…å«åˆ†ç±»æ ‡ç­¾
                if len(batch_data) == 3:
                    images, masks, _ = batch_data
                else:
                    images, masks = batch_data
                images, masks = images.to(device), masks.to(device)
                
                # ä½¿ç”¨TTAè¿›è¡Œé¢„æµ‹ï¼ˆè®­ç»ƒç»“æŸåçš„æµ‹è¯•æ¨èä½¿ç”¨ï¼‰
                if use_tta:
                    outputs = self._tta_inference(model, images)
                else:
                    outputs = model(images)
                # ç¡®ä¿ outputs å’Œ masks çš„ç©ºé—´å°ºå¯¸åŒ¹é…
                if outputs.shape[2:] != masks.shape[2:]:
                    outputs = F.interpolate(outputs, size=masks.shape[2:], mode='bilinear', align_corners=False)
                preds = torch.sigmoid(outputs)
                preds_binary = (preds > 0.5).float()
                
                for i in range(images.size(0)):
                    if len(all_masks) >= num_samples:
                        break
                    
                    img = images[i].cpu().permute(1, 2, 0).numpy()
                    img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])
                    img = np.clip(img, 0, 1).astype(np.float32)
                    mask = masks[i, 0].cpu().numpy().astype(np.float32)
                    pred = preds_binary[i, 0].cpu().numpy().astype(np.float32)
                    
                    # ç¡®ä¿ pred å’Œ mask çš„å°ºå¯¸åŒ¹é…ï¼ˆåŒé‡æ£€æŸ¥ï¼Œä»¥é˜²ä¸‡ä¸€ï¼‰
                    if pred.shape != mask.shape:
                        from scipy.ndimage import zoom
                        if len(pred.shape) == 2 and len(mask.shape) == 2:
                            zoom_factors = (mask.shape[0] / pred.shape[0], mask.shape[1] / pred.shape[1])
                            pred = zoom(pred, zoom_factors, order=1)
                    
                    # è®¡ç®—æŒ‡æ ‡ - ä½¿ç”¨æ”¹è¿›çš„ç©ºmaskå¤„ç†
                    pred_sum = pred.sum()
                    mask_sum = mask.sum()
                    intersection = (pred * mask).sum()
                    
                    # ä½¿ç”¨_safe_dice_scoreç»Ÿä¸€å¤„ç†
                    dice = self._safe_dice_score(pred, mask)
                    
                    # IoUè®¡ç®—ä¹Ÿéœ€è¦ç‰¹æ®Šå¤„ç†
                    if mask_sum <= 1e-7:
                        if pred_sum <= 1e-7:
                            iou = 1.0  # å®Œç¾åŒ¹é…
                        else:
                            iou = 0.0  # æœ‰è¯¯æ£€
                    elif pred_sum <= 1e-7:
                        iou = 0.0  # å®Œå…¨æ¼æ£€
                    else:
                        total = pred_sum + mask_sum
                        union = total - intersection
                        iou = (intersection + 1e-7) / (union + 1e-7)
                    
                    all_images.append(img)
                    all_masks.append(mask)
                    all_preds.append(pred)
                    all_metrics.append({'dice': dice, 'iou': iou})
                
                if len(all_masks) >= num_samples:
                    break

        if self.enable_matlab_plots and self.matlab_viz_bridge:
            try:
                payload = self._save_test_results_payload(all_images, all_masks, all_preds, all_metrics, "test_results")
                matlab_path = os.path.join(self.temp_dir, "test_results_visualization_matlab.png")
                self.matlab_viz_bridge.render_test_results(payload, matlab_path)
                return matlab_path
            except Exception as exc:
                print(f"[MATLAB Plot] æµ‹è¯•å¯è§†åŒ–å›é€€: {exc}")
        
        # åˆ›å»ºå¯è§†åŒ–
        num_samples = min(num_samples, len(all_images))
        cols = 4  # åŸå›¾ã€çœŸå®maskã€é¢„æµ‹maskã€å¯¹æ¯”å›¾
        rows = num_samples
        
        fig, axes = plt.subplots(rows, cols, figsize=(16, 4 * rows))
        if rows == 1:
            axes = axes.reshape(1, -1)
        
        for i in range(num_samples):
            img = all_images[i]
            true_mask = all_masks[i]
            pred_mask = all_preds[i]
            metrics = all_metrics[i]
            
            # åˆ›å»ºå¯¹æ¯”å›¾ï¼šçº¢è‰²=çœŸå®ï¼Œç»¿è‰²=é¢„æµ‹ï¼Œé»„è‰²=é‡å 
            overlay = img.copy()
            overlay[true_mask == 1, 0] = 1  # çº¢è‰²ï¼šçœŸå®åŒºåŸŸ
            overlay[pred_mask == 1, 1] = 1  # ç»¿è‰²ï¼šé¢„æµ‹åŒºåŸŸ
            overlay[(true_mask == 1) & (pred_mask == 1), 2] = 1  # é»„è‰²ï¼šé‡å åŒºåŸŸ
            
            # åŸå›¾
            axes[i, 0].imshow(img)
            axes[i, 0].set_title(f"æ ·æœ¬ {i+1}\nåŸå§‹å›¾åƒ", fontsize=10)
            axes[i, 0].axis('off')
            
            # çœŸå®mask
            axes[i, 1].imshow(true_mask, cmap='gray')
            axes[i, 1].set_title("çœŸå®Mask\n(çœŸå®æ ‡ç­¾)", fontsize=10)
            axes[i, 1].axis('off')
            
            # é¢„æµ‹mask
            axes[i, 2].imshow(pred_mask, cmap='gray')
            axes[i, 2].set_title(f"é¢„æµ‹Mask\nDice: {metrics['dice']:.3f}\nIoU: {metrics['iou']:.3f}", 
                               fontsize=10)
            axes[i, 2].axis('off')
            
            # å¯¹æ¯”å›¾
            axes[i, 3].imshow(overlay)
            axes[i, 3].set_title("å¯¹æ¯”å›¾\n(çº¢:çœŸå®, ç»¿:é¢„æµ‹, é»„:é‡å )", fontsize=10)
            axes[i, 3].axis('off')
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        return save_path
    
    def generate_performance_analysis(self, detailed_metrics):
        """ç”Ÿæˆæ€§èƒ½åˆ†ææŠ¥å‘Šçš„å¯è§†åŒ–"""
        save_path = os.path.join(self.temp_dir, "performance_analysis.png")

        if self.enable_matlab_plots and self.matlab_viz_bridge:
            try:
                payload = self._save_performance_payload(detailed_metrics)
                matlab_path = os.path.join(self.temp_dir, "performance_analysis_matlab.png")
                self.matlab_viz_bridge.render_performance_analysis(payload, matlab_path)
                return matlab_path
            except Exception as exc:
                print(f"[MATLAB Plot] æ€§èƒ½åˆ†æå›é€€: {exc}")
        
        metrics = detailed_metrics['all_samples']
        avg_metrics = detailed_metrics['average']
        
        fig = plt.figure(figsize=(16, 10))
        
        # 1. æŒ‡æ ‡åˆ†å¸ƒç›´æ–¹å›¾
        ax1 = plt.subplot(2, 3, 1)
        ax1.hist(metrics['dice'], bins=20, alpha=0.7, color='blue', edgecolor='black')
        ax1.axvline(avg_metrics['dice'], color='red', linestyle='--', linewidth=2, label=f'å¹³å‡: {avg_metrics["dice"]:.3f}')
        ax1.set_xlabel('Diceç³»æ•°')
        ax1.set_ylabel('æ ·æœ¬æ•°é‡')
        ax1.set_title('Diceç³»æ•°åˆ†å¸ƒ')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        ax2 = plt.subplot(2, 3, 2)
        ax2.hist(metrics['iou'], bins=20, alpha=0.7, color='green', edgecolor='black')
        ax2.axvline(avg_metrics['iou'], color='red', linestyle='--', linewidth=2, label=f'å¹³å‡: {avg_metrics["iou"]:.3f}')
        ax2.set_xlabel('IoU')
        ax2.set_ylabel('æ ·æœ¬æ•°é‡')
        ax2.set_title('IoUåˆ†å¸ƒ')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        ax3 = plt.subplot(2, 3, 3)
        ax3.hist(metrics['precision'], bins=20, alpha=0.7, color='orange', edgecolor='black')
        ax3.axvline(avg_metrics['precision'], color='red', linestyle='--', linewidth=2, label=f'å¹³å‡: {avg_metrics["precision"]:.3f}')
        ax3.set_xlabel('ç²¾ç¡®ç‡')
        ax3.set_ylabel('æ ·æœ¬æ•°é‡')
        ax3.set_title('ç²¾ç¡®ç‡åˆ†å¸ƒ')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 2. æŒ‡æ ‡å¯¹æ¯”æŸ±çŠ¶å›¾
        ax4 = plt.subplot(2, 3, 4)
        metric_names = ['Diceç³»æ•°', 'IoU', 'ç²¾ç¡®ç‡', 'æ•æ„Ÿåº¦(å¬å›ç‡)', 'ç‰¹å¼‚åº¦', 'F1åˆ†æ•°']
        metric_values = [
            avg_metrics['dice'],
            avg_metrics['iou'],
            avg_metrics['precision'],
            avg_metrics.get('sensitivity', avg_metrics.get('recall', 0)),
            avg_metrics.get('specificity', 0),
            avg_metrics['f1']
        ]
        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8e44ad']
        bars = ax4.bar(metric_names, metric_values, color=colors, alpha=0.7, edgecolor='black')
        ax4.set_ylabel('åˆ†æ•°')
        ax4.set_title('å¹³å‡æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”')
        ax4.set_ylim([0, 1])
        ax4.grid(True, alpha=0.3, axis='y')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, val in zip(bars, metric_values):
            height = bar.get_height()
            ax4.text(bar.get_x() + bar.get_width()/2., height,
                    f'{val:.3f}', ha='center', va='bottom', fontsize=9)
        
        # 3. æŒ‡æ ‡ç®±çº¿å›¾
        ax5 = plt.subplot(2, 3, 5)
        box_data = [
            metrics['dice'],
            metrics['iou'],
            metrics['precision'],
            metrics.get('sensitivity', metrics['recall']),
            metrics['specificity'],
            metrics['f1']
        ]
        bp = ax5.boxplot(box_data, tick_labels=metric_names, patch_artist=True)  # ä½¿ç”¨tick_labelsæ›¿ä»£labelsï¼ˆå·²ç¿»è¯‘ä¸ºä¸­æ–‡ï¼‰
        for patch, color in zip(bp['boxes'], colors):
            patch.set_facecolor(color)
            patch.set_alpha(0.7)
        ax5.set_ylabel('åˆ†æ•°')
        ax5.set_title('æŒ‡æ ‡åˆ†å¸ƒç®±çº¿å›¾')
        ax5.grid(True, alpha=0.3, axis='y')
        
        # 4. ç»Ÿè®¡ä¿¡æ¯è¡¨æ ¼
        ax6 = plt.subplot(2, 3, 6)
        ax6.axis('tight')
        ax6.axis('off')
        
        stats_data = []
        for metric in ['dice', 'iou', 'precision', 'sensitivity', 'specificity', 'f1', 'hd95']:
            stats_data.append([
                metric.upper(),
                f"{detailed_metrics['average'][metric]:.4f}",
                f"{detailed_metrics['std'][metric]:.4f}",
                f"{detailed_metrics['min'][metric]:.4f}",
                f"{detailed_metrics['max'][metric]:.4f}",
                f"{detailed_metrics['median'][metric]:.4f}"
            ])
        
        table = ax6.table(cellText=stats_data,
                         colLabels=['æŒ‡æ ‡', 'å¹³å‡å€¼', 'æ ‡å‡†å·®', 'æœ€å°å€¼', 'æœ€å¤§å€¼', 'ä¸­ä½æ•°'],
                         cellLoc='center',
                         loc='center',
                         bbox=[0, 0, 1, 1])
        table.auto_set_font_size(False)
        table.set_fontsize(9)
        table.scale(1, 2)
        
        # è®¾ç½®è¡¨å¤´æ ·å¼
        for i in range(6):
            table[(0, i)].set_facecolor('#4CAF50')
            table[(0, i)].set_text_props(weight='bold', color='white')
        
        plt.suptitle('æ¨¡å‹æ€§èƒ½åˆ†ææŠ¥å‘Š', fontsize=16, fontweight='bold', y=0.995)
        plt.tight_layout(rect=[0, 0, 1, 0.99])
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        return save_path
    
    def visualize_attention_maps(self, model, dataloader, device, num_samples=4):
        """å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡å›¾ï¼Œç”¨äºæ¨¡å‹å¯è§£é‡Šæ€§åˆ†æ - ä¼˜åŒ–ç‰ˆ"""
        if not self._supports_attention_maps(model):
            raise RuntimeError("å½“å‰æ¨¡å‹ä¸æ”¯æŒæ³¨æ„åŠ›å¯è§†åŒ–")
        save_path = os.path.join(self.temp_dir, "attention_visualization.png")
        model.eval()
        
        # æ”¶é›†æ ·æœ¬å’Œæ³¨æ„åŠ›å›¾
        all_images = []
        all_masks = []
        all_preds = []
        all_attention_maps = []
        
        with torch.no_grad():
            for batch_data in dataloader:
                # å¤„ç†æ•°æ®ï¼šå¯èƒ½åŒ…å«åˆ†ç±»æ ‡ç­¾
                if len(batch_data) == 3:
                    images, masks, _ = batch_data
                else:
                    images, masks = batch_data
                images, masks = images.to(device), masks.to(device)
                # è·å–é¢„æµ‹ç»“æœå’Œæ³¨æ„åŠ›æƒé‡
                outputs, attention_maps = model(images, return_attention=True)
                preds = torch.sigmoid(outputs)
                preds_binary = (preds > 0.5).float()
                
                for i in range(images.size(0)):
                    if len(all_images) >= num_samples:
                        break
                    
                    img = images[i].cpu().permute(1, 2, 0).numpy()
                    img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])
                    img = np.clip(img, 0, 1).astype(np.float32)
                    mask = masks[i, 0].cpu().numpy().astype(np.float32)
                    pred = preds_binary[i, 0].cpu().numpy().astype(np.float32)
                    
                    # æ”¶é›†æ‰€æœ‰å±‚çš„æ³¨æ„åŠ›å›¾ï¼Œå¹¶ä¸Šé‡‡æ ·åˆ°åŸå§‹å›¾åƒå¤§å°
                    att_dict = {}
                    for att_name, att_map in attention_maps.items():
                        att_np = att_map[i, 0].cpu().numpy()
                        # ä¸Šé‡‡æ ·åˆ°256x256ï¼ˆä¸è¾“å…¥å›¾åƒå¤§å°ä¸€è‡´ï¼‰
                        from scipy.ndimage import zoom
                        target_size = (256, 256)
                        if att_np.shape != target_size:
                            zoom_factors = (target_size[0] / att_np.shape[0], target_size[1] / att_np.shape[1])
                            att_np = zoom(att_np, zoom_factors, order=1)
                        att_dict[att_name] = att_np
                    
                    all_images.append(img)
                    all_masks.append(mask)
                    all_preds.append(pred)
                    all_attention_maps.append(att_dict)
                
                if len(all_images) >= num_samples:
                    break

        att_layer_payload = {'att1': [], 'att2': [], 'att3': [], 'att4': []}
        for att_dict in all_attention_maps:
            for key in att_layer_payload.keys():
                if key in att_dict:
                    att_layer_payload[key].append(att_dict[key])

        if self.enable_matlab_plots and self.matlab_viz_bridge:
            try:
                payload_path = self._save_attention_payload(all_images, all_masks, all_preds, att_layer_payload, "attention_visualization")
                matlab_path = os.path.join(self.temp_dir, "attention_visualization_matlab.png")
                self.matlab_viz_bridge.render_attention_maps(payload_path, matlab_path)
                return matlab_path
            except Exception as exc:
                print(f"[MATLAB Plot] æ³¨æ„åŠ›å¯è§†åŒ–å›é€€: {exc}")
        
        # åˆ›å»ºå¯è§†åŒ– - ä¼˜åŒ–å¸ƒå±€
        num_samples = min(num_samples, len(all_images))
        cols = 7  # åŸå›¾ã€çœŸå®maskã€é¢„æµ‹maskã€att1å åŠ ã€att2å åŠ ã€att3å åŠ ã€att4å åŠ 
        rows = num_samples
        
        fig, axes = plt.subplots(rows, cols, figsize=(24, 4.5 * rows))
        if rows == 1:
            axes = axes.reshape(1, -1)
        
        for i in range(num_samples):
            img = all_images[i]
            true_mask = all_masks[i]
            pred_mask = all_preds[i]
            att_maps = all_attention_maps[i]
            
            # åŸå›¾
            axes[i, 0].imshow(img)
            axes[i, 0].set_title(f"æ ·æœ¬ {i+1}\nåŸå§‹å›¾åƒ", fontsize=11, fontweight='bold', pad=8)
            axes[i, 0].axis('off')
            
            # çœŸå®mask
            axes[i, 1].imshow(true_mask, cmap='gray')
            axes[i, 1].set_title("çœŸå®Mask\n(Ground Truth)", fontsize=11, fontweight='bold', pad=8)
            axes[i, 1].axis('off')
            
            # é¢„æµ‹mask
            axes[i, 2].imshow(pred_mask, cmap='gray')
            axes[i, 2].set_title("é¢„æµ‹Mask\n(Prediction)", fontsize=11, fontweight='bold', pad=8)
            axes[i, 2].axis('off')
            
            # æ³¨æ„åŠ›å›¾å åŠ æ˜¾ç¤ºï¼ˆåœ¨åŸå›¾ä¸Šå åŠ æ³¨æ„åŠ›çƒ­åŠ›å›¾ï¼‰
            col_idx = 3
            for att_name in ['att1', 'att2', 'att3', 'att4']:
                if att_name in att_maps and col_idx < cols:
                    att = att_maps[att_name]
                    layer_num = att_name[-1]
                    
                    # å½’ä¸€åŒ–æ³¨æ„åŠ›å›¾
                    att_norm = (att - att.min()) / (att.max() - att.min() + 1e-8)
                    
                    overlay = img.copy()
                    
                    import matplotlib.cm as cm
                    heatmap = cm.jet(att_norm)[:, :, :3]
                    
                    alpha = 0.5  # é€æ˜åº¦
                    blended = overlay * (1 - alpha) + heatmap * alpha
                    
                    # æ˜¾ç¤ºå åŠ å›¾åƒ
                    im = axes[i, col_idx].imshow(blended)
                    axes[i, col_idx].set_title(f"æ³¨æ„åŠ›å±‚{layer_num}\n(å åŠ æ˜¾ç¤º)", 
                                             fontsize=11, fontweight='bold', pad=8)
                    axes[i, col_idx].axis('off')
                    
                    # æ·»åŠ é¢œè‰²æ¡æ˜¾ç¤ºæ³¨æ„åŠ›å¼ºåº¦ï¼ˆä½¿ç”¨åŸå§‹æ³¨æ„åŠ›å›¾ï¼‰
                    im_cbar = axes[i, col_idx].imshow(att_norm, cmap='hot', alpha=0.0)  # ä»…ç”¨äºcolorbar
                    cbar = plt.colorbar(im_cbar, ax=axes[i, col_idx], fraction=0.046, pad=0.02)
                    cbar.set_label('æ³¨æ„åŠ›å¼ºåº¦', fontsize=9, rotation=270, labelpad=15)
                    
                    col_idx += 1
        
        # ä½¿ç”¨æ™®é€šæ–‡æœ¬æ›¿ä»£emojiï¼Œé¿å…å­—ä½“è­¦å‘Š
        plt.suptitle('æ¨¡å‹æ³¨æ„åŠ›æƒé‡å¯è§†åŒ– - å¯è§£é‡Šæ€§åˆ†æ', 
                    fontsize=18, fontweight='bold', y=0.995, color='#1e293b')
        plt.tight_layout(rect=[0, 0, 1, 0.99])
        plt.savefig(save_path, dpi=200, bbox_inches='tight', facecolor='white')
        plt.close()
        
        return save_path
    
    def analyze_attention_statistics(self, model, dataloader, device, num_samples=20):
        """åˆ†ææ³¨æ„åŠ›æƒé‡çš„ç»Ÿè®¡ç‰¹æ€§ - å¢å¼ºç‰ˆï¼Œæ”¯æŒåŠ¨æ€æ£€æµ‹æ³¨æ„åŠ›å±‚"""
        if not self._supports_attention_maps(model):
            raise RuntimeError("å½“å‰æ¨¡å‹ä¸æ”¯æŒæ³¨æ„åŠ›ç»Ÿè®¡åˆ†æ")
        model.eval()
        # å…ˆè¿è¡Œä¸€æ¬¡è·å–å®é™…çš„æ³¨æ„åŠ›å±‚åç§°
        attention_stats = {}
        
        with torch.no_grad():
            eval_count = 0
            for batch_data in dataloader:
                if eval_count >= num_samples:
                    break
                
                # å¤„ç†æ•°æ®ï¼šå¯èƒ½åŒ…å«åˆ†ç±»æ ‡ç­¾
                if len(batch_data) == 3:
                    images, masks, _ = batch_data
                else:
                    images, masks = batch_data
                images, masks = images.to(device), masks.to(device)
                outputs, attention_maps = model(images, return_attention=True)
                
                # åˆå§‹åŒ–ç»Ÿè®¡å­—å…¸ï¼ˆåªåˆå§‹åŒ–å®é™…å­˜åœ¨çš„å±‚ï¼‰
                if not attention_stats:
                    for att_name in attention_maps.keys():
                        attention_stats[att_name] = {
                            'mean': [], 'std': [], 'max': [], 'min': [], 
                            'entropy': [], 'concentration': []
                        }
                
                preds = torch.sigmoid(outputs)
                preds_binary = (preds > 0.5).float()
                
                for i in range(images.size(0)):
                    if eval_count >= num_samples:
                        break
                    
                    mask_np = masks[i, 0].cpu().numpy()
                    pred_np = preds_binary[i, 0].cpu().numpy()
                    
                    for att_name, att_map in attention_maps.items():
                        if att_name not in attention_stats:
                            continue
                            
                        att_np = att_map[i, 0].cpu().numpy()
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰æ— æ•ˆå€¼
                        if np.any(np.isnan(att_np)) or np.any(np.isinf(att_np)):
                            # è·³è¿‡åŒ…å«nan/infçš„æ ·æœ¬
                            continue
                        
                        # åŸºç¡€ç»Ÿè®¡
                        att_mean = float(att_np.mean())
                        att_std = float(att_np.std())
                        att_max = float(att_np.max())
                        att_min = float(att_np.min())
                        
                        if not (np.isnan(att_mean) or np.isinf(att_mean)):
                            attention_stats[att_name]['mean'].append(att_mean)
                        if not (np.isnan(att_std) or np.isinf(att_std)):
                            attention_stats[att_name]['std'].append(att_std)
                        if not (np.isnan(att_max) or np.isinf(att_max)):
                            attention_stats[att_name]['max'].append(att_max)
                        if not (np.isnan(att_min) or np.isinf(att_min)):
                            attention_stats[att_name]['min'].append(att_min)
                        
                        # è®¡ç®—ç†µï¼ˆè¡¡é‡æ³¨æ„åŠ›åˆ†å¸ƒçš„åˆ†æ•£ç¨‹åº¦ï¼‰
                        att_flat = att_np.flatten()
                        att_sum = att_flat.sum()
                        if att_sum > 1e-8:  # ç¡®ä¿ä¸æ˜¯å…¨é›¶
                            att_flat = att_flat / att_sum  # å½’ä¸€åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒ
                            att_flat = att_flat[att_flat > 1e-8]  # å»é™¤æ¥è¿‘é›¶çš„å€¼
                            if len(att_flat) > 0:
                                entropy = -np.sum(att_flat * np.log(att_flat + 1e-8))
                                if not (np.isnan(entropy) or np.isinf(entropy)):
                                    attention_stats[att_name]['entropy'].append(float(entropy))
                        else:
                            # å…¨é›¶æƒ…å†µï¼Œç†µä¸º0
                            attention_stats[att_name]['entropy'].append(0.0)
                        
                        # è®¡ç®—é›†ä¸­åº¦ï¼ˆé«˜æ³¨æ„åŠ›å€¼åŒºåŸŸçš„å æ¯”ï¼‰
                        if att_np.size > 0:
                            threshold = np.percentile(att_np, 90)  # å‰10%çš„é˜ˆå€¼
                            if not np.isnan(threshold):
                                concentration = float(np.sum(att_np >= threshold) / att_np.size)
                                if not (np.isnan(concentration) or np.isinf(concentration)):
                                    attention_stats[att_name]['concentration'].append(concentration)
                    
                    eval_count += 1
                
                if eval_count >= num_samples:
                    break
        
        # è®¡ç®—å¹³å‡ç»Ÿè®¡ï¼Œå¤„ç†ç©ºåˆ—è¡¨æƒ…å†µ
        avg_stats = {}
        for att_name, stats in attention_stats.items():
            avg_stats[att_name] = {}
            for stat_name, values in stats.items():
                if len(values) > 0:
                    avg_val = np.mean(values)
                    if not (np.isnan(avg_val) or np.isinf(avg_val)):
                        avg_stats[att_name][stat_name] = float(avg_val)
                    else:
                        avg_stats[att_name][stat_name] = 0.0
                else:
                    # ç©ºåˆ—è¡¨ï¼Œè¿”å›é»˜è®¤å€¼
                    avg_stats[att_name][stat_name] = 0.0 if stat_name in ['mean', 'std', 'max', 'min'] else (0.0 if stat_name == 'entropy' else 0.0)
        
        return avg_stats
    


    def run(self):
        try:
            # åˆå§‹åŒ–è®¾å¤‡
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            self.update_progress.emit(0, f"ä½¿ç”¨è®¾å¤‡: {device}")
            
            # æ•°æ®å‡†å¤‡
            patient_ids = [pid for pid in os.listdir(self.data_dir) 
                         if os.path.isdir(os.path.join(self.data_dir, pid))]
            
            # å•æ¨¡å‹è®­ç»ƒ
            train_ids, val_ids = train_test_split(patient_ids, test_size=0.3, random_state=42)
            
            # æ•°æ®å¢å¼ºï¼ˆå¢å¼ºå¯¹æ¯”åº¦ã€å…‰ç…§å’Œå½¢å˜ï¼Œæå‡æ³›åŒ–èƒ½åŠ›ï¼‰
            # ä¼˜åŒ–æ•°æ®å¢å¼º - é’ˆå¯¹åŒ»å­¦å½±åƒçš„éåˆšä½“å½¢å˜ç‰¹æ€§
            # é‡ç‚¹å¢å¼ºï¼šGrid Distortion + Elastic Transformï¼ˆæ¨¡æ‹Ÿå™¨å®˜æŒ¤å‹å’Œå˜å½¢ï¼‰
            # MixUp å°†åœ¨è®­ç»ƒå¾ªç¯ä¸­å®ç°ï¼ˆéœ€è¦ä¸¤å¼ å›¾åƒæ··åˆï¼‰
            train_transform = A.Compose([
                A.Resize(256, 256),
                A.HorizontalFlip(p=0.5),
                A.VerticalFlip(p=0.1),
                A.Affine(translate_percent=0.05, scale=(0.9, 1.1), rotate=(-10, 10), mode=cv2.BORDER_REFLECT_101, p=0.6),
                # Grid Distortionï¼šæ¨¡æ‹Ÿéåˆšä½“å½¢å˜ï¼Œå¯¹åŒ»å­¦å½±åƒéå¸¸æœ‰æ•ˆ
                A.GridDistortion(
                    num_steps=5,
                    distort_limit=0.3,  # å¢å¼ºå½¢å˜å¹…åº¦
                    interpolation=cv2.INTER_LINEAR,
                    border_mode=cv2.BORDER_REFLECT_101,
                    p=0.3  # 30%æ¦‚ç‡åº”ç”¨
                ),
                # Elastic Transformï¼šæ¨¡æ‹Ÿå™¨å®˜çš„æŒ¤å‹å’Œå˜å½¢ï¼ˆåŒ»å­¦å½±åƒæœ€å¼ºå¢å¼ºï¼‰
                A.ElasticTransform(
                    alpha=50,  # å¢å¼ºå½¢å˜å¼ºåº¦ï¼ˆä»10æå‡åˆ°50ï¼‰
                    sigma=5,   # å¢å¼ºå¹³æ»‘åº¦ï¼ˆä»3æå‡åˆ°5ï¼‰
                    interpolation=cv2.INTER_LINEAR,
                    border_mode=cv2.BORDER_REFLECT_101,
                    p=0.4  # æé«˜æ¦‚ç‡ï¼ˆä»0.15æå‡åˆ°0.4ï¼‰
                ),
                A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.4),
                A.RandomGamma(gamma_limit=(80, 120), p=0.3),
                A.CLAHE(clip_limit=2.5, tile_grid_size=(8, 8), p=0.3),
                A.GaussianBlur(blur_limit=(3, 5), p=0.15),
                # GaussNoise å·²ç§»é™¤ï¼ˆå‚æ•°ä¸å…¼å®¹ï¼‰ï¼Œå¦‚éœ€å™ªå£°å¢å¼ºå¯ä½¿ç”¨å…¶ä»–æ–¹å¼
                A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
                ToTensorV2()
            ])
            
            # éªŒè¯é›†ä»…åšå‡ ä½•å½’ä¸€åŒ–ï¼Œé¿å…å¼•å…¥è¿‡å¤šéšæœºæ€§
            val_transform = A.Compose([
                A.Resize(256, 256),
                A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
                ToTensorV2()
            ])
            
            # åŠ è½½åˆ†å‰²è®­ç»ƒæ•°æ®
            self.update_progress.emit(5, "æ­£åœ¨åŠ è½½åˆ†å‰²è®­ç»ƒæ•°æ®...")
            # æ ¹æ®CPUæ ¸å¿ƒæ•°å’Œæ“ä½œç³»ç»Ÿè®¾ç½®åˆé€‚çš„num_workers
            # Windowsä¸Šä½¿ç”¨å¤šè¿›ç¨‹å¯èƒ½å¯¼è‡´å¡æ­»ï¼Œå»ºè®®ä½¿ç”¨0æˆ–1
            import platform
            is_windows = platform.system() == 'Windows'
            cpu_count = os.cpu_count() or 1
            if is_windows:
                # Windowsä¸Šä½¿ç”¨å•è¿›ç¨‹æˆ–0ï¼Œé¿å…å¡æ­»
                num_workers = 0
                use_persistent_workers = False
            else:
                # Linux/Macå¯ä»¥ä½¿ç”¨å¤šè¿›ç¨‹
                num_workers = max(0, min(4, cpu_count - 1))
                use_persistent_workers = num_workers > 0
            
            self.update_progress.emit(6, f"æ•°æ®åŠ è½½å™¨é…ç½®: num_workers={num_workers}")
            
            train_dataset = self.load_dataset(train_ids, train_transform, split_name="train", return_classification=False)
            train_sampler = None
            if getattr(train_dataset, "use_weighted_sampling", False):
                weights = train_dataset.get_sampling_weights()
                if weights is not None:
                    weight_tensor = torch.as_tensor(weights, dtype=torch.double)
                    train_sampler = WeightedRandomSampler(weight_tensor, num_samples=len(weight_tensor), replacement=True)

            train_loader = DataLoader(
                train_dataset,
                batch_size=self.batch_size,
                shuffle=(train_sampler is None),
                sampler=train_sampler,
                num_workers=num_workers,
                pin_memory=(device.type == 'cuda' and not is_windows),  # Windowsä¸Špin_memoryå¯èƒ½å¯¼è‡´é—®é¢˜
                persistent_workers=use_persistent_workers,
                prefetch_factor=2 if num_workers > 0 else None
            )
            
            self.update_progress.emit(10, "æ­£åœ¨åŠ è½½åˆ†å‰²éªŒè¯æ•°æ®...")
            val_dataset = self.load_dataset(val_ids, val_transform, split_name="val", return_classification=False, use_weighted_sampling=False)
            val_loader = DataLoader(
                val_dataset,
                batch_size=self.batch_size,
                shuffle=False,
                num_workers=num_workers,
                pin_memory=(device.type == 'cuda' and not is_windows),
                persistent_workers=use_persistent_workers,
                prefetch_factor=2 if num_workers > 0 else None
            )
            
            train_pos_weight = self.pos_weight_cache.get('train')
            if train_pos_weight is None:
                mask_paths = self.split_metadata.get('train', {}).get('mask_paths', [])
                train_pos_weight = self._estimate_pos_weight(mask_paths)
                self.pos_weight_cache['train'] = train_pos_weight
            self.update_progress.emit(12, f"ä¼°è®¡å‰æ™¯æƒé‡: {train_pos_weight:.2f}")
            
            # å¦‚æœæœ‰é¢„è®­ç»ƒæ¨¡å‹ï¼Œå…ˆè¯»å–é…ç½®ä»¥ç¡®ä¿æ¶æ„åŒ¹é…
            if self.model_path and os.path.exists(self.model_path):
                # è‹¥ç”¨æˆ·é€‰æ‹©çš„æ˜¯ last_model.pthï¼Œä¼˜å…ˆå›é€€åˆ°åŒç›®å½•ä¸‹çš„ best_model_dice_*.pth
                model_path_to_use = self.model_path
                base_name = os.path.basename(self.model_path)
                if base_name.startswith("last_model"):
                    parent = os.path.dirname(self.model_path)
                    try:
                        cand = sorted(
                            [p for p in os.listdir(parent) if p.startswith("best_model_dice_") and p.endswith(".pth")],
                            reverse=True,
                        )
                        if cand:
                            model_path_to_use = os.path.join(parent, cand[0])
                            print(f"[æç¤º] æ£€æµ‹åˆ° last_model.pthï¼Œè‡ªåŠ¨åˆ‡æ¢ä¸ºæœ€ä½³æ¨¡å‹æƒé‡: {os.path.basename(model_path_to_use)}")
                    except Exception:
                        pass

                ckpt_config = read_checkpoint_config(model_path_to_use)
                if ckpt_config:
                    # ä»checkpointæ¨æ–­çš„é…ç½®è¦†ç›–å½“å‰è®¾ç½®
                    if 'model_type' in ckpt_config:
                        self.model_type = ckpt_config['model_type']
                    if 'swin_params' in ckpt_config and ckpt_config['swin_params']:
                        self.swin_params = copy.deepcopy(ckpt_config['swin_params'])
                        self.use_gwo = False  # å·²æœ‰å‚æ•°ï¼Œç¦ç”¨GWO
                        self.update_progress.emit(13, f"ä»checkpointæ¨æ–­SwinUNetå‚æ•°: embed_dim={self.swin_params.get('embed_dim')}")
                    if 'dstrans_params' in ckpt_config and ckpt_config['dstrans_params']:
                        self.dstrans_params = copy.deepcopy(ckpt_config['dstrans_params'])
                        self.use_gwo = False
                        self.update_progress.emit(13, f"ä»checkpointæ¨æ–­DS-TransUNetå‚æ•°: embed_dim={self.dstrans_params.get('embed_dim')}")
            
            # GWOä¼˜åŒ–ï¼ˆSwinUNet / DS-TransUNetï¼‰
            if self.use_gwo and self.swin_params is None and (self.model_type == "swin_unet" or self.model_type == "swinunet"):
                self.update_progress.emit(13, "å¼€å§‹GWOä¼˜åŒ–SwinUNetè¶…å‚æ•°...")
                self.swin_params = self._gwo_optimize_swin_params(train_loader, val_loader, device)
                self.update_progress.emit(14, f"GWOä¼˜åŒ–å®Œæˆï¼Œæœ€ä½³å‚æ•°: {self.swin_params}")
            if self.use_gwo and self.dstrans_params is None and self.model_type in ("ds_trans_unet", "dstransunet", "ds-transunet"):
                self.update_progress.emit(13, "å¼€å§‹GWOä¼˜åŒ–DS-TransUNetè¶…å‚æ•°...")
                self.dstrans_params = self._gwo_optimize_dstrans_params(train_loader, val_loader, device)
                self.update_progress.emit(14, f"GWOä¼˜åŒ–å®Œæˆï¼Œæœ€ä½³å‚æ•°: {self.dstrans_params}")
            
            # åˆå§‹åŒ–æ¨¡å‹
            self.update_progress.emit(15, f"æ­£åœ¨æ„å»ºæ¨¡å‹ ({self.model_type})...")
            try:
                model = self._build_model(device, swin_params=self.swin_params, dstrans_params=self.dstrans_params)
                self.update_progress.emit(16, "æ¨¡å‹æ„å»ºå®Œæˆ")
            except Exception as e:
                self.update_progress.emit(0, f"æ¨¡å‹æ„å»ºå¤±è´¥: {str(e)}")
                import traceback
                traceback.print_exc()
                return
            if self.model_path and os.path.exists(self.model_path):
                # ä¸ä¸Šæ–¹ä¸€è‡´ï¼šè‹¥ä¸º last_model.pthï¼Œåˆ™ä¼˜å…ˆåŠ è½½åŒç›®å½•ä¸‹åˆ†æ•°æœ€é«˜çš„ best_model_dice_*.pth
                model_path_to_use = self.model_path
                base_name = os.path.basename(self.model_path)
                if base_name.startswith("last_model"):
                    parent = os.path.dirname(self.model_path)
                    try:
                        cand = sorted(
                            [p for p in os.listdir(parent) if p.startswith("best_model_dice_") and p.endswith(".pth")],
                            reverse=True,
                        )
                        if cand:
                            model_path_to_use = os.path.join(parent, cand[0])
                    except Exception:
                        pass

                # ä½¿ç”¨å…¼å®¹åŠ è½½å‡½æ•°
                success, msg = load_model_compatible(model, model_path_to_use, device, verbose=False)
                self.update_progress.emit(15, msg)
            ema_model = None
            if self.use_ema:
                ema_model = self._init_ema_model(model, device)
            
            # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
            # é»˜è®¤å­¦ä¹ ç‡ï¼šé¢„è®­ç»ƒæ¨¡å‹ï¼ˆResNetï¼‰ä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡è¿›è¡Œå¾®è°ƒ
            # SwinUNet å’Œ Transformer æ¨¡å‹ä»å¤´è®­ç»ƒï¼Œå¯ä»¥ä½¿ç”¨ç¨å¤§çš„å­¦ä¹ ç‡
            if self.model_type in ("swin_unet", "swinunet"):
                default_lr = 5e-5
            elif self.model_type == "resnet_unet":
                # ResNet101ä½¿ç”¨é¢„è®­ç»ƒæƒé‡ï¼Œéœ€è¦æ›´å°çš„å­¦ä¹ ç‡è¿›è¡Œå¾®è°ƒ
                # ä»5e-5è¿›ä¸€æ­¥é™ä½åˆ°2e-5ï¼Œé¿å…æ¢¯åº¦çˆ†ç‚¸å’Œæ•°å€¼ä¸ç¨³å®š
                default_lr = 2e-5
            else:
                default_lr = 1e-4

            # è‹¥è®¾ç½®äº†ç¯å¢ƒå˜é‡ SEG_LRï¼Œåˆ™ä¼˜å…ˆä½¿ç”¨ï¼Œä¾¿äºåœ¨è®­ç»ƒç“¶é¢ˆæ—¶æ‰‹åŠ¨é™ä½å­¦ä¹ ç‡
            env_lr = os.environ.get("SEG_LR")
            try:
                initial_lr = float(env_lr) if env_lr is not None else default_lr
            except ValueError:
                print(f"[è­¦å‘Š] æ— æ³•è§£æ SEG_LR='{env_lr}'ï¼Œå›é€€åˆ°é»˜è®¤å­¦ä¹ ç‡ {default_lr}")
                initial_lr = default_lr

            optimizer = self._create_optimizer(model.parameters(), lr=initial_lr)
            # å¢å¼ºå‰æ™¯æƒé‡ä»¥å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
            adjusted_pos_weight = min(train_pos_weight * 1.5, 20.0)
            bce_weight_tensor = torch.tensor([adjusted_pos_weight], device=device)
            bce_criterion = nn.BCEWithLogitsLoss(pos_weight=bce_weight_tensor)

            # Polyå­¦ä¹ ç‡ + Warmup: lr = base_lr * (1 - epoch / max_epochs) ** power
            warmup_epochs_lr = 5
            poly_power = float(os.environ.get("SEG_POLY_POWER", "0.9"))
            scheduler = None
            # ä½¿ç”¨ ReduceLROnPlateau åœ¨éªŒè¯Diceé•¿æœŸä¸æå‡æ—¶è‡ªåŠ¨é™ä½å­¦ä¹ ç‡
            # å…¼å®¹è¾ƒæ—§ç‰ˆæœ¬çš„PyTorchï¼Œè¿™é‡Œä¸ä½¿ç”¨verboseå‚æ•°
            plateau_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                optimizer, mode='max', factor=0.1, patience=3
            )

            # SWAä¸æ—©åœé…ç½® - è‹¥å¯ç”¨EMAåˆ™é»˜è®¤å…³é—­SWAé¿å…å†²çª
            swa_enabled = (not self.use_ema) and self.epochs >= 15
            swa_start_epoch = max(int(self.epochs * 0.5), 1)  # æ›´æ—©å¯ç”¨
            swa_model = AveragedModel(model) if swa_enabled else None
            swa_scheduler = SWALR(
                optimizer,
                swa_lr=2e-5,  # æ›´ä½çš„SWAå­¦ä¹ ç‡
                anneal_epochs=3,
                anneal_strategy='cos'
            ) if swa_enabled else None
            swa_active_epochs = 0

            warmup_epochs = min(8, max(3, self.epochs // 5))
            # å°æ•°æ®é›†ï¼šæ›´å®½æ¾çš„æ—©åœç­–ç•¥ï¼Œç»™æ¨¡å‹å……åˆ†å­¦ä¹ æ—¶é—´
            early_stopping = EarlyStopping(
                patience=max(12, self.epochs // 3),  # æ›´å¤§è€å¿ƒ
                min_delta=1e-4,  # æ›´ä½é˜ˆå€¼
                min_rel_improve=0.003,  # æ›´ä½ç›¸å¯¹æå‡è¦æ±‚
                warmup_epochs=warmup_epochs + 5,  # æ›´é•¿é¢„çƒ­
                cooldown=3,  # æ›´é•¿å†·å´
                smoothing=0.3,
            )
            early_stop_triggered = False

            # AMP æ··åˆç²¾åº¦è®­ç»ƒï¼ˆä»…CUDAå¯ç”¨ï¼‰
            amp_device_type = 'cuda' if device.type == 'cuda' else 'cpu'
            amp_enabled = (amp_device_type == 'cuda')
            # SwinUNetåœ¨åŠç²¾åº¦ä¸‹æ›´å®¹æ˜“å‡ºç°æº¢å‡ºï¼Œé»˜è®¤å…³é—­AMPæˆ–ä½¿ç”¨æ›´å°çš„ç¼©æ”¾
            if self.model_type in ("swin_unet", "swinunet"):
                amp_enabled = False
            scaler = GradScaler('cuda', enabled=amp_enabled, init_scale=2.0 ** 7, growth_interval=200, growth_factor=1.5, backoff_factor=0.5)
            
            # è®­ç»ƒå¾ªç¯
            # å†»ç»“/è§£å†»ç­–ç•¥ï¼šå‰50% epochå†»ç»“ç¼–ç å™¨ï¼Œå50%è§£å†»è¿›è¡Œå¾®è°ƒ
            freeze_epochs = int(self.epochs * 0.5)
            encoder_frozen = False
            # è®­ç»ƒè¿‡ç¨‹ä¸­ç”¨äºå­¦ä¹ ç‡è°ƒåº¦çš„åŸºå‡†LRï¼ˆè§£å†»æ—¶ä¼šåŠ¨æ€ä¸‹è°ƒï¼‰
            base_lr = float(initial_lr)
            
            for epoch in range(self.epochs):
                if self.stop_requested:
                    self.update_progress.emit(0, "è®­ç»ƒå·²ç”±ç”¨æˆ·åœæ­¢")
                    # ã€ä¿®å¤ã€‘ç”¨æˆ·åœæ­¢æ—¶ä¹Ÿè¦å‘é€å®Œæˆä¿¡å·ï¼Œç¡®ä¿UIæ­£ç¡®æ›´æ–°
                    self.training_finished.emit("è®­ç»ƒå·²è¢«ç”¨æˆ·åœæ­¢", self.best_model_path if self.save_best else None)
                    return
                
                # å†»ç»“/è§£å†»ç¼–ç å™¨é€»è¾‘ï¼ˆä»…å¯¹ ResNetUNet æœ‰æ•ˆï¼‰
                if self.model_type == "resnet_unet":
                    actual_model = self._unwrap_model(model)
                    if isinstance(actual_model, ResNetUNet):
                        if epoch < freeze_epochs:
                            # å‰50% epochï¼šå†»ç»“ç¼–ç å™¨
                            if not encoder_frozen:
                                actual_model._freeze_encoder()
                                encoder_frozen = True
                                # é‡æ–°åˆ›å»ºä¼˜åŒ–å™¨ï¼Œåªä¼˜åŒ–å¯è®­ç»ƒå‚æ•°
                                trainable_params = [p for p in model.parameters() if p.requires_grad]
                                optimizer = self._create_optimizer(trainable_params, initial_lr)
                                print(f"[è®­ç»ƒç­–ç•¥] Epoch {epoch+1}/{self.epochs}: ç¼–ç å™¨å·²å†»ç»“ï¼Œä»…è®­ç»ƒè§£ç å™¨")
                        else:
                            # å50% epochï¼šè§£å†»ç¼–ç å™¨è¿›è¡Œå¾®è°ƒ
                            if encoder_frozen:
                                actual_model._unfreeze_encoder()
                                encoder_frozen = False
                                # é‡æ–°åˆ›å»ºä¼˜åŒ–å™¨ï¼Œä¼˜åŒ–æ‰€æœ‰å‚æ•°ï¼ˆä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡è¿›è¡Œå¾®è°ƒï¼‰
                                # è§£å†»ç¬é—´ï¼šæŠŠâ€œå½“å‰å­¦ä¹ ç‡â€å¼ºåˆ¶é™ä½åˆ° 1/10ï¼Œé¿å… ResNet101 å…¨é‡å¾®è°ƒéœ‡è¡
                                current_lr = float(optimizer.param_groups[0]['lr'])
                                fine_tune_lr = current_lr * 0.1
                                base_lr = fine_tune_lr  # åŒæ—¶æ›´æ–°åç»­Polyè°ƒåº¦çš„åŸºå‡†LRï¼Œé¿å…è¢«initial_lrè¦†ç›–å›å»
                                trainable_params = [p for p in model.parameters() if p.requires_grad]
                                optimizer = self._create_optimizer(trainable_params, fine_tune_lr)
                                print(f"[è®­ç»ƒç­–ç•¥] Epoch {epoch+1}/{self.epochs}: ç¼–ç å™¨å·²è§£å†»ï¼Œå¼€å§‹ç«¯åˆ°ç«¯å¾®è°ƒ (LR={fine_tune_lr:.6f})")
                
                epoch_loss_weights = self._get_loss_weights(epoch, self.epochs)
                
                # æ¯ä¸ªepochå¼€å§‹æ—¶é‡ç½®æ¢¯åº¦æ¶ˆå¤±è®¡æ•°å™¨
                if hasattr(self, '_zero_grad_count'):
                    self._zero_grad_count = 0
                
                # Warmup + Polyå­¦ä¹ ç‡è°ƒæ•´
                if epoch < warmup_epochs_lr:
                    # çº¿æ€§Warmupåˆ° base_lr
                    warmup_factor = (epoch + 1) / warmup_epochs_lr
                    for param_group in optimizer.param_groups:
                        param_group['lr'] = base_lr * warmup_factor
                else:
                    # Warmupç»“æŸåï¼ŒæŒ‰epochä½¿ç”¨Polyç­–ç•¥è¡°å‡å­¦ä¹ ç‡
                    t = (epoch - warmup_epochs_lr) / max(1, self.epochs - warmup_epochs_lr)
                    lr = base_lr * (1.0 - t) ** poly_power
                    # ResNet50éœ€è¦æ›´å¤§çš„æœ€å°å­¦ä¹ ç‡ï¼Œé¿å…æ¢¯åº¦æ¶ˆå¤±
                    min_lr = 1e-5 if self.model_type == "resnet_unet" else 1e-6
                    lr = max(lr, min_lr)
                    for param_group in optimizer.param_groups:
                        param_group['lr'] = lr
                
                # è®­ç»ƒé˜¶æ®µ
                model.train()
                # ç¡®ä¿EMAæ¨¡å‹ä¹Ÿå¤„äºtrainæ¨¡å¼ï¼ˆä»¥ä¾¿BNç»Ÿè®¡é‡èƒ½æ­£ç¡®æ›´æ–°ï¼‰
                if self.use_ema and ema_model is not None:
                    ema_model.train()
                epoch_loss = 0.0
                train_samples = 0
                
                # æ·»åŠ è¿›åº¦æç¤ºï¼Œé¿å…çœ‹èµ·æ¥å¡æ­»
                if epoch == 0:
                    self.update_progress.emit(20, "å¼€å§‹ç¬¬ä¸€ä¸ªè®­ç»ƒæ‰¹æ¬¡ï¼ˆé¦–æ¬¡è¿è¡Œå¯èƒ½è¾ƒæ…¢ï¼Œè¯·è€å¿ƒç­‰å¾…ï¼‰...")
                
                for batch_idx, batch_data in enumerate(tqdm(train_loader, desc=f'è®­ç»ƒè½®æ¬¡ {epoch+1}/{self.epochs}')):
                    if self.stop_requested:
                        # ã€ä¿®å¤ã€‘ç”¨æˆ·åœæ­¢æ—¶ä¹Ÿè¦å‘é€å®Œæˆä¿¡å·ï¼Œç¡®ä¿UIæ­£ç¡®æ›´æ–°
                        self.training_finished.emit("è®­ç»ƒå·²è¢«ç”¨æˆ·åœæ­¢", self.best_model_path if self.save_best else None)
                        return
                    
                    # å¤„ç†æ•°æ®
                    images, masks = batch_data
                    images, masks = images.to(device), masks.float().to(device)
                    
                    batch_size = images.size(0)
                    
                    # MixUp æ•°æ®å¢å¼ºï¼ˆå°æ•°æ®é›†å¢å¼ºæ³›åŒ–èƒ½åŠ›ï¼Œé˜²æ­¢å¯¹ç‰¹å®šçº¹ç†è¿‡æ‹Ÿåˆï¼‰
                    # ä»ç¬¬3ä¸ªepochå¼€å§‹ï¼Œ50%æ¦‚ç‡ä½¿ç”¨MixUp
                    use_mixup = (epoch >= 3) and (np.random.rand() < 0.5) and (batch_size > 1)
                    if use_mixup:
                        # éšæœºæ‰“ä¹±ç´¢å¼•ï¼Œåˆ›å»ºæ··åˆå¯¹
                        indices = torch.randperm(batch_size).to(device)
                        # Betaåˆ†å¸ƒç”Ÿæˆæ··åˆç³»æ•° lambdaï¼ˆalpha=0.2 ä½¿å¾—æ··åˆæ›´ä¿å®ˆï¼Œé€‚åˆåŒ»å­¦å½±åƒï¼‰
                        lam = np.random.beta(0.2, 0.2)
                        lam = max(lam, 1.0 - lam)  # ç¡®ä¿ä¸»è¦æ ·æœ¬æƒé‡æ›´å¤§
                        
                        # æ··åˆå›¾åƒ
                        mixed_images = lam * images + (1.0 - lam) * images[indices]
                        # æ··åˆmaskï¼ˆä¿æŒç›¸åŒçš„lambdaï¼‰
                        mixed_masks = lam * masks + (1.0 - lam) * masks[indices]
                        
                        images = mixed_images
                        masks = mixed_masks
                    
                    # å®šæœŸæ¸…ç†GPUç¼“å­˜ï¼Œé™ä½æ˜¾å­˜å³°å€¼
                    if batch_idx % 10 == 0 and torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    brain_mask = None
                    if self.use_skull_stripper:
                        images, brain_mask = self._apply_skull_strip(images)

                    # è¾“å…¥æ•°æ®éªŒè¯ï¼šæ£€æŸ¥NaN/Infï¼ˆåœ¨å¢åŠ train_samplesä¹‹å‰ï¼‰
                    if torch.any(torch.isnan(images)) or torch.any(torch.isinf(images)):
                        print(f"[è­¦å‘Š] Epoch {epoch+1}, Batch {batch_idx+1}: è¾“å…¥å›¾åƒåŒ…å«NaN/Infï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                        continue
                    if torch.any(torch.isnan(masks)) or torch.any(torch.isinf(masks)):
                        print(f"[è­¦å‘Š] Epoch {epoch+1}, Batch {batch_idx+1}: è¾“å…¥æ©è†œåŒ…å«NaN/Infï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                        continue
                    
                    # åªæœ‰åœ¨æ‰€æœ‰æ£€æŸ¥é€šè¿‡åæ‰å¢åŠ train_samples
                    train_samples += batch_size
                    
                    # æ£€æŸ¥è¾“å…¥æ•°æ®èŒƒå›´æ˜¯å¦åˆç†
                    # ImageNetå½’ä¸€åŒ–åï¼Œç†è®ºä¸Šå€¼åŸŸåœ¨-2.5åˆ°2.5å·¦å³
                    # è€ƒè™‘æ•°æ®å¢å¼ºï¼ˆColorJitterã€RandomBrightnessContrastç­‰ï¼‰ï¼Œåˆç†èŒƒå›´æ‰©å±•åˆ°-5åˆ°5
                    # åªæœ‰åœ¨æç«¯æƒ…å†µä¸‹ï¼ˆè¶…å‡º-10åˆ°10ï¼‰æ‰è­¦å‘Šå¹¶è£å‰ª
                    image_min, image_max = images.min().item(), images.max().item()
                    if image_min < -10.0 or image_max > 10.0:
                        # åªåœ¨çœŸæ­£æç«¯çš„æƒ…å†µä¸‹æ‰æ‰“å°è­¦å‘Šï¼ˆé¿å…è¿‡å¤šæ—¥å¿—ï¼‰
                        if image_min < -15.0 or image_max > 15.0:
                            print(f"[è­¦å‘Š] Epoch {epoch+1}, Batch {batch_idx+1}: è¾“å…¥å›¾åƒå€¼åŸŸå¼‚å¸¸ (min={image_min:.4f}, max={image_max:.4f})ï¼Œè¿›è¡Œè£å‰ª")
                        # è£å‰ªåˆ°åˆç†èŒƒå›´ï¼ˆImageNetå½’ä¸€åŒ– + æ•°æ®å¢å¼ºçš„åˆç†èŒƒå›´ï¼‰
                        images = torch.clamp(images, min=-5.0, max=5.0)
                    elif image_min < -5.0 or image_max > 5.0:
                        # é™é»˜è£å‰ªåˆ°åˆç†èŒƒå›´ï¼Œä¸æ‰“å°è­¦å‘Šï¼ˆè¿™æ˜¯æ•°æ®å¢å¼ºçš„æ­£å¸¸ç»“æœï¼‰
                        images = torch.clamp(images, min=-5.0, max=5.0)
                    
                    if masks.min() < 0.0 or masks.max() > 1.0:
                        mask_min, mask_max = masks.min().item(), masks.max().item()
                        # åªåœ¨æç«¯æƒ…å†µä¸‹æ‰è­¦å‘Š
                        if mask_min < -0.1 or mask_max > 1.1:
                            print(f"[è­¦å‘Š] Epoch {epoch+1}, Batch {batch_idx+1}: æ©è†œå€¼åŸŸå¼‚å¸¸ (min={mask_min:.4f}, max={mask_max:.4f})ï¼Œè¿›è¡Œè£å‰ª")
                        masks = torch.clamp(masks, min=0.0, max=1.0)

                    optimizer.zero_grad(set_to_none=True)
                    with autocast(device_type=amp_device_type, enabled=amp_enabled):
                        supports_aux = self._supports_aux_outputs(model)
                        supports_attention = self._supports_attention_maps(model)
                        forward_kwargs = {}
                        if supports_aux:
                            forward_kwargs['return_aux'] = True
                        if supports_attention:
                            forward_kwargs['return_attention'] = True
                        
                        if forward_kwargs:
                            forward_out = model(images, **forward_kwargs)
                            if supports_aux and supports_attention:
                                outputs, aux_outputs, attention_maps = forward_out
                            elif supports_aux:
                                outputs, aux_outputs = forward_out
                                attention_maps = {}
                            else:
                                outputs, attention_maps = forward_out
                                aux_outputs = []
                        else:
                            outputs = model(images)
                            aux_outputs = []
                            attention_maps = {}
                        if brain_mask is not None:
                            outputs = outputs * brain_mask

                        # æ£€æŸ¥æ¨¡å‹è¾“å‡ºæ˜¯å¦åŒ…å«NaN/Infï¼Œå¦‚æœä¸¥é‡åˆ™è·³è¿‡è¯¥batch
                        if torch.any(torch.isnan(outputs)) or torch.any(torch.isinf(outputs)):
                            nan_ratio = (torch.isnan(outputs).sum() + torch.isinf(outputs).sum()).float() / outputs.numel()
                            if nan_ratio > 0.1:  # å¦‚æœè¶…è¿‡10%çš„å€¼ä¸ºNaN/Infï¼Œè·³è¿‡è¯¥batch
                                print(f"[ä¸¥é‡è­¦å‘Š] Epoch {epoch+1}, Batch {batch_idx+1}: æ¨¡å‹è¾“å‡ºNaN/Infæ¯”ä¾‹è¿‡é«˜({nan_ratio:.2%})ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                                continue
                            else:
                                # å°‘é‡NaN/Infæ—¶å°è¯•ä¿®æ­£
                                outputs = torch.nan_to_num(outputs, nan=0.0, posinf=1.0, neginf=-1.0)
                        # åœ¨è®¡ç®—æŸå¤±å‰ï¼Œå…ˆæ£€æŸ¥å¹¶è£å‰ªlogitsåˆ°åˆç†èŒƒå›´ï¼Œé˜²æ­¢æ•°å€¼ä¸ç¨³å®š
                        outputs = torch.clamp(outputs, min=-10.0, max=10.0)
                        
                        # åŸºç¡€åˆ†å‰²æŸå¤±
                        loss = self.compute_seg_loss(outputs, masks, bce_criterion, weights=epoch_loss_weights)
                        
                        # æ£€æŸ¥æŸå¤±æ˜¯å¦ä¸ºNaN/Infï¼Œå¦‚æœæ˜¯åˆ™è·³è¿‡è¯¥batch
                        if not torch.isfinite(loss):
                            print(f"[ä¸¥é‡è­¦å‘Š] Epoch {epoch+1}, Batch {batch_idx+1}: æŸå¤±ä¸ºNaN/Infï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                            continue
                        
                        # ç¡®ä¿åŸºç¡€æŸå¤±éè´Ÿä¸”æœ‰é™
                        loss = torch.clamp(loss, min=0.0, max=1000.0)  # é™åˆ¶æœ€å¤§æŸå¤±å€¼
                        
                        # æ£€æŸ¥æŸå¤±æ˜¯å¦ä¸ºNaN/Infï¼ˆåœ¨åå‘ä¼ æ’­ä¹‹å‰ï¼‰
                        if not torch.isfinite(loss):
                            print(f"[ä¸¥é‡è­¦å‘Š] Epoch {epoch+1}, Batch {batch_idx+1}: æŸå¤±ä¸ºNaN/Infï¼Œå°è¯•ä¿®å¤...")
                            # å°è¯•ä½¿ç”¨ç®€å•çš„BCEæŸå¤±
                            loss = bce_criterion(outputs, masks)
                            loss = torch.clamp(loss, min=0.0, max=1000.0)
                            
                            # å¦‚æœä»ç„¶æ˜¯NaN/Infï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡
                            if not torch.isfinite(loss):
                                print(f"[ä¸¥é‡è­¦å‘Š] Epoch {epoch+1}, Batch {batch_idx+1}: ä¿®å¤å¤±è´¥ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                                continue
                        
                        # è¾…åŠ©è¾“å‡ºæŸå¤±
                        if aux_outputs:
                            for weight, aux_logits in zip(self.aux_loss_weights, aux_outputs):
                                loss += weight * self.compute_seg_loss(aux_logits, masks, bce_criterion, weights=epoch_loss_weights)
                        
                        # æ³¨æ„åŠ›é›†ä¸­åº¦æŸå¤±
                        if attention_maps:
                            att_loss = self.attention_concentration_loss(attention_maps, masks, weight=0.005)
                            if att_loss > 0 and torch.isfinite(att_loss):
                                loss += att_loss
                        
                        # æœ€ç»ˆæ£€æŸ¥ï¼šåœ¨åå‘ä¼ æ’­ä¹‹å‰ç¡®ä¿lossæ˜¯æœ‰æ•ˆçš„
                        if not torch.isfinite(loss):
                            print(f"[ä¸¥é‡è­¦å‘Š] Epoch {epoch+1}, Batch {batch_idx+1}: æœ€ç»ˆlossä¸ºNaN/Infï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                            continue
                    
                    scaler.scale(loss).backward()
                    scaler.unscale_(optimizer)
                    
                    # æ¸…ç†å¼‚å¸¸æ¢¯åº¦ï¼Œé˜²æ­¢NaN/Infä¼ æ’­
                    grad_clamp = 1.0 if self.model_type in ("swin_unet", "swinunet") else 5.0
                    grad_sanitized = self._sanitize_gradients(model, clamp_value=grad_clamp)
                    if grad_sanitized:
                        print(f"[è­¦å‘Š] Epoch {epoch+1}, Batch {batch_idx+1}: æ£€æµ‹åˆ°å¼‚å¸¸æ¢¯åº¦ï¼Œå·²è‡ªåŠ¨ä¿®å¤")
                    
                    # æ£€æŸ¥æ¢¯åº¦ä¸­çš„NaN/Inf
                    has_nan_grad = False
                    for name, param in model.named_parameters():
                        if param.grad is not None:
                            if torch.any(torch.isnan(param.grad)) or torch.any(torch.isinf(param.grad)):
                                print(f"[ä¸¥é‡è­¦å‘Š] Epoch {epoch+1}, Batch {batch_idx+1}: å‚æ•° {name} çš„æ¢¯åº¦åŒ…å«NaN/Infï¼Œæ¸…é›¶æ¢¯åº¦")
                                param.grad.zero_()
                                has_nan_grad = True
                    
                    if has_nan_grad:
                        print(f"[è­¦å‘Š] Epoch {epoch+1}, Batch {batch_idx+1}: æ£€æµ‹åˆ°NaN/Infæ¢¯åº¦ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                        scaler.update()
                        continue
                    
                    # è®¡ç®—æ¢¯åº¦èŒƒæ•°å¹¶æ£€æŸ¥
                    total_grad_norm = 0.0
                    param_count = 0
                    for p in model.parameters():
                        if p.grad is not None:
                            param_norm = p.grad.data.norm(2)
                            if torch.isfinite(param_norm):
                                total_grad_norm += param_norm.item() ** 2
                                param_count += 1
                            else:
                                print(f"[è­¦å‘Š] å‚æ•°æ¢¯åº¦èŒƒæ•°ä¸ºNaN/Infï¼Œæ¸…é›¶è¯¥æ¢¯åº¦")
                                p.grad.zero_()
                    
                    if param_count > 0:
                        total_grad_norm = total_grad_norm ** (1. / 2)
                    else:
                        total_grad_norm = 0.0
                    
                    # è°ƒè¯•ï¼šæ£€æŸ¥æ¢¯åº¦ï¼ˆä»…åœ¨ç¬¬ä¸€ä¸ªepochçš„å‰å‡ ä¸ªbatchæˆ–æ¢¯åº¦å¼‚å¸¸æ—¶ï¼‰
                    if (epoch == 0 and batch_idx < 3) or total_grad_norm > 100.0 or total_grad_norm < 1e-6:
                        print(f"[è°ƒè¯•] Epoch {epoch+1}, Batch {batch_idx+1}: Loss={loss.item():.4f}, GradNorm={total_grad_norm:.6f}, LR={optimizer.param_groups[0]['lr']:.8f}")
                        if total_grad_norm < 1e-6:
                            print(f"[è­¦å‘Š] æ¢¯åº¦è¿‡å°ï¼Œæ¨¡å‹å¯èƒ½æ— æ³•æ­£å¸¸æ›´æ–°ï¼")
                        if total_grad_norm > 100.0:
                            print(f"[è­¦å‘Š] æ¢¯åº¦è¿‡å¤§ï¼Œå¯èƒ½å‘ç”Ÿæ¢¯åº¦çˆ†ç‚¸ï¼")
                    
                    # æ¢¯åº¦è£å‰ªï¼šç»Ÿä¸€ä½¿ç”¨æ ‡å‡† max_norm=1.0ï¼ˆ0.05 è¿‡å°ä¼šå¯¼è‡´è®­ç»ƒä¸ç¨³å®š/éš¾ä»¥æ”¶æ•›ï¼‰
                    max_grad_norm = 1.0
                    if total_grad_norm > 10.0:
                        print(f"[ä¸¥é‡è­¦å‘Š] æ¢¯åº¦è¿‡å¤§({total_grad_norm:.2f})ï¼Œæ‰§è¡Œæ¢¯åº¦è£å‰ª(max_norm={max_grad_norm})")
                    
                    # å¦‚æœæ¢¯åº¦ä¸º0ï¼Œå°è¯•ä¸´æ—¶æé«˜å­¦ä¹ ç‡æˆ–è·³è¿‡è¯¥batch
                    if total_grad_norm < 1e-8:
                        print(f"[ä¸¥é‡è­¦å‘Š] Epoch {epoch+1}, Batch {batch_idx+1}: æ¢¯åº¦å®Œå…¨æ¶ˆå¤±(GradNorm={total_grad_norm:.8f})")
                        # å¦‚æœè¿ç»­å¤šä¸ªbatchæ¢¯åº¦ä¸º0ï¼Œä¸´æ—¶æé«˜å­¦ä¹ ç‡
                        if not hasattr(self, '_zero_grad_count'):
                            self._zero_grad_count = 0
                        self._zero_grad_count += 1
                        if self._zero_grad_count > 5:
                            # ä¸´æ—¶å°†å­¦ä¹ ç‡æé«˜2å€
                            current_lr = optimizer.param_groups[0]['lr']
                            new_lr = min(current_lr * 2.0, initial_lr * 0.1)  # æœ€é«˜ä¸è¶…è¿‡åˆå§‹å­¦ä¹ ç‡çš„10%
                            for param_group in optimizer.param_groups:
                                param_group['lr'] = new_lr
                            print(f"[ä¿®å¤] ä¸´æ—¶æé«˜å­¦ä¹ ç‡: {current_lr:.8f} -> {new_lr:.8f}")
                            self._zero_grad_count = 0
                        scaler.update()
                        continue
                    else:
                        # æ¢¯åº¦æ­£å¸¸æ—¶é‡ç½®è®¡æ•°å™¨
                        if hasattr(self, '_zero_grad_count'):
                            self._zero_grad_count = 0
                    
                    clip_grad_norm_(model.parameters(), max_norm=max_grad_norm)
                    
                    # å†æ¬¡æ£€æŸ¥è£å‰ªåçš„æ¢¯åº¦
                    for p in model.parameters():
                        if p.grad is not None:
                            if torch.any(torch.isnan(p.grad)) or torch.any(torch.isinf(p.grad)):
                                print(f"[ä¸¥é‡è­¦å‘Š] æ¢¯åº¦è£å‰ªåä»æœ‰NaN/Infï¼Œæ¸…é›¶æ¢¯åº¦")
                                p.grad.zero_()
                    
                    scaler.step(optimizer)
                    scaler.update()
                    
                    # æ£€æŸ¥æ¨¡å‹å‚æ•°æ˜¯å¦åŒ…å«NaN/Inf
                    for name, param in model.named_parameters():
                        if torch.any(torch.isnan(param.data)) or torch.any(torch.isinf(param.data)):
                            print(f"[ä¸¥é‡è­¦å‘Š] Epoch {epoch+1}, Batch {batch_idx+1}: å‚æ•° {name} åŒ…å«NaN/Infï¼")
                            # å°è¯•ä»EMAæ¨¡å‹æ¢å¤ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                            if hasattr(self, 'use_ema') and self.use_ema and ema_model is not None:
                                print(f"[å°è¯•æ¢å¤] ä»EMAæ¨¡å‹æ¢å¤å‚æ•° {name}")
                                with torch.no_grad():
                                    actual_model = self._unwrap_model(model)
                                    actual_ema = self._unwrap_model(ema_model)
                                    if name in actual_ema.state_dict():
                                        param.data.copy_(actual_ema.state_dict()[name])
                    if self.use_ema and ema_model is not None:
                        self._update_ema_model(ema_model, model)
                    
                    # æ£€æŸ¥æŸå¤±å€¼æ˜¯å¦æœ‰æ•ˆ
                    loss_value = loss.item()
                    if not np.isfinite(loss_value):
                        print(f"[è­¦å‘Š] Epoch {epoch+1}, Batch {batch_idx+1}: æŸå¤±å€¼ä¸ºNaN/Infï¼Œä½¿ç”¨0.0")
                        loss_value = 0.0
                    
                    epoch_loss += loss_value * batch_size
                    
                    # å®šæœŸæ¸…ç†GPUç¼“å­˜
                    if batch_idx % 10 == 0 and torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    
                    # æ›´æ–°è®­ç»ƒè¿›åº¦
                    train_progress = 20 + int(50 * (batch_idx + 1) / len(train_loader))
                    self.update_progress.emit(
                        train_progress,
                        f"è½®æ¬¡ {epoch+1}/{self.epochs} | æ‰¹æ¬¡ {batch_idx+1}/{len(train_loader)} | æŸå¤±: {loss_value:.4f}"
                    )
                
                # éªŒè¯é˜¶æ®µ
                model.eval()
                val_dice = 0.0
                val_iou = 0.0
                val_loss = 0.0
                val_samples = 0
                val_pred_fg_pixels = 0.0
                val_gt_fg_pixels = 0.0
                val_total_pixels = 0.0
                # ã€è¯Šæ–­ã€‘æ·»åŠ ç©ºmaskæ ·æœ¬ç»Ÿè®¡ï¼Œå¸®åŠ©è¯Šæ–­Diceè™šé«˜é—®é¢˜
                val_empty_mask_count = 0  # ç›®æ ‡ä¸ºç©ºmaskçš„æ ·æœ¬æ•°
                val_empty_mask_dice_sum = 0.0  # ç©ºmaskæ ·æœ¬çš„Diceæ€»å’Œ
                val_non_empty_mask_count = 0  # ç›®æ ‡æœ‰å‰æ™¯çš„æ ·æœ¬æ•°
                val_non_empty_mask_dice_sum = 0.0  # æœ‰å‰æ™¯æ ·æœ¬çš„Diceæ€»å’Œ
                # IoUåˆ†ç±»ç»Ÿè®¡
                val_empty_mask_iou_sum = 0.0
                val_non_empty_mask_iou_sum = 0.0
                
                self.update_val_progress.emit(0, f"å¼€å§‹éªŒè¯è½®æ¬¡ {epoch+1}...")
                # å¦‚æœå¯ç”¨EMAä¸”è®­ç»ƒäº†è¶³å¤Ÿè½®æ¬¡ï¼Œä½¿ç”¨EMAæ¨¡å‹è¿›è¡Œè¯„ä¼°
                eval_model_for_epoch = model
                if self.use_ema and ema_model is not None and epoch >= self.ema_eval_start_epoch:
                    # EMAæ¨¡å‹åœ¨è¯„ä¼°æ—¶éœ€è¦è®¾ç½®ä¸ºevalæ¨¡å¼
                    ema_model.eval()
                    eval_model_for_epoch = ema_model
                    # å¦‚æœåŸæ¨¡å‹æ˜¯DataParallelï¼Œéœ€è¦åŒ…è£…EMAæ¨¡å‹
                    if isinstance(model, nn.DataParallel):
                        eval_model_for_epoch = nn.DataParallel(ema_model)
                # ç¡®ä¿æ¨¡å‹å¤„äºevalæ¨¡å¼ï¼ˆæ— è®ºæ˜¯æ™®é€šæ¨¡å‹è¿˜æ˜¯EMAæ¨¡å‹ï¼‰
                if not isinstance(eval_model_for_epoch, nn.DataParallel):
                    eval_model_for_epoch.eval()
                else:
                    eval_model_for_epoch.module.eval()
                
                # åŠ¨æ€åˆ·æ–°é˜ˆå€¼ï¼Œé¿å…Diceé•¿æœŸå¡åœ¨å›ºå®šå€¼
                allow_refresh = (epoch >= 1)
                refresh_threshold = (
                    allow_refresh and (
                        epoch == 1
                        or self.threshold_refresh_interval <= 1
                        or ((epoch + 1) % self.threshold_refresh_interval == 0)
                    )
                )
                if refresh_threshold:
                    try:
                        # ä½¿ç”¨å…¨éƒ¨éªŒè¯é›†è¿›è¡Œé˜ˆå€¼ä¼˜åŒ–ï¼Œç¡®ä¿ä¸éªŒè¯é˜¶æ®µç»“æœä¸€è‡´
                        val_threshold = float(self.find_optimal_threshold(
                            eval_model_for_epoch,
                            val_loader,
                            device,
                            num_samples=None,  # Noneè¡¨ç¤ºä½¿ç”¨å…¨éƒ¨éªŒè¯é›†
                        ))
                        self.last_optimal_threshold = val_threshold
                    except Exception as threshold_err:
                        print(f"[è­¦å‘Š] é˜ˆå€¼æœç´¢å¤±è´¥ï¼Œä½¿ç”¨ä¸Šä¸€æ¬¡çš„é˜ˆå€¼ã€‚åŸå› : {threshold_err}")
                        val_threshold = float(getattr(self, "last_optimal_threshold", 0.5))
                else:
                    if epoch == 0:
                        val_threshold = 0.5
                    else:
                        val_threshold = float(getattr(self, "last_optimal_threshold", 0.5))
                
                with torch.no_grad():
                    for val_idx, val_batch in enumerate(val_loader):
                        if self.stop_requested:
                            # ã€ä¿®å¤ã€‘ç”¨æˆ·åœæ­¢æ—¶ä¹Ÿè¦å‘é€å®Œæˆä¿¡å·ï¼Œç¡®ä¿UIæ­£ç¡®æ›´æ–°
                            self.training_finished.emit("è®­ç»ƒå·²è¢«ç”¨æˆ·åœæ­¢", self.best_model_path if self.save_best else None)
                            return
                        
                        # å¤„ç†æ•°æ®
                        images, masks = val_batch
                        images = images.to(device)
                        masks = masks.float().to(device)
                            
                        batch_size = images.size(0)
                        brain_mask = None
                        if self.use_skull_stripper:
                            images, brain_mask = self._apply_skull_strip(images)
                        
                        # éªŒè¯é˜¶æ®µè¾“å…¥æ•°æ®æ£€æŸ¥ï¼ˆåœ¨å¢åŠ val_samplesä¹‹å‰ï¼‰
                        if torch.any(torch.isnan(images)) or torch.any(torch.isinf(images)):
                            print(f"[è­¦å‘Š] éªŒè¯é˜¶æ®µ: Batch {val_idx+1}: è¾“å…¥å›¾åƒåŒ…å«NaN/Infï¼Œè·³è¿‡")
                            continue
                        if torch.any(torch.isnan(masks)) or torch.any(torch.isinf(masks)):
                            print(f"[è­¦å‘Š] éªŒè¯é˜¶æ®µ: Batch {val_idx+1}: è¾“å…¥æ©è†œåŒ…å«NaN/Infï¼Œè·³è¿‡")
                            continue
                        
                        with autocast(device_type=amp_device_type, enabled=amp_enabled):
                            # åœ¨forwardä¹‹å‰æ£€æŸ¥è¾“å…¥
                            if torch.any(torch.isnan(images)) or torch.any(torch.isinf(images)):
                                print(f"[è­¦å‘Š] éªŒè¯é˜¶æ®µ: Batch {val_idx+1}: è¾“å…¥å›¾åƒåŒ…å«NaN/Infï¼Œè·³è¿‡")
                                continue
                            
                            # ã€å…³é”®ä¿®å¤ã€‘éªŒè¯é˜¶æ®µä¸ä½¿ç”¨TTAï¼Œä¸è®­ç»ƒæŸå¤±è®¡ç®—ä¿æŒä¸€è‡´
                            # åŸå› ï¼šè®­ç»ƒæŸå¤±åŸºäºå•æ¬¡å‰å‘ä¼ æ’­ï¼Œå¦‚æœéªŒè¯ä½¿ç”¨TTAä¼šå¯¼è‡´Diceè™šé«˜
                            # å¦‚æœéœ€è¦TTAè¯„ä¼°ï¼Œåº”è¯¥åœ¨è®­ç»ƒç»“æŸåçš„æœ€ç»ˆæµ‹è¯•é˜¶æ®µä½¿ç”¨
                            # å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡ SEG_USE_TTA_IN_VAL=1 å¯ç”¨ï¼ˆä¸æ¨èï¼‰
                            use_tta_in_val = os.environ.get("SEG_USE_TTA_IN_VAL", "0") == "1"
                            
                            if use_tta_in_val:
                                # ä»…åœ¨æ˜ç¡®å¯ç”¨æ—¶ä½¿ç”¨TTAï¼ˆä¸æ¨èï¼Œä¼šå¯¼è‡´è®­ç»ƒå’ŒéªŒè¯ä¸ä¸€è‡´ï¼‰
                                try:
                                    outputs = self._tta_inference(eval_model_for_epoch, images)
                                    if brain_mask is not None:
                                        outputs = outputs * brain_mask
                                except RuntimeError as e:
                                    if "out of memory" in str(e).lower() or "nan" in str(e).lower() or "inf" in str(e).lower():
                                        print(f"[ä¸¥é‡è­¦å‘Š] éªŒè¯é˜¶æ®µ: Batch {val_idx+1}: TTAæ¨ç†å¤±è´¥ ({str(e)[:100]})ï¼Œè·³è¿‡è¯¥batch")
                                        continue
                                    else:
                                        raise
                            else:
                                # æ ‡å‡†éªŒè¯ï¼šå•æ¬¡å‰å‘ä¼ æ’­ï¼ˆä¸è®­ç»ƒæŸå¤±è®¡ç®—ä¸€è‡´ï¼‰
                                outputs = eval_model_for_epoch(images)
                                if isinstance(outputs, tuple):
                                    outputs = outputs[0]
                                if brain_mask is not None:
                                    outputs = outputs * brain_mask
                            
                            # æ£€æŸ¥æ¨¡å‹è¾“å‡ºï¼Œå¦‚æœå‡ºç°NaN/Infï¼Œè¯´æ˜æ¨¡å‹å·²ç»å´©æºƒï¼Œè·³è¿‡è¯¥batch
                            if torch.any(torch.isnan(outputs)) or torch.any(torch.isinf(outputs)):
                                nan_ratio = (torch.isnan(outputs).sum() + torch.isinf(outputs).sum()).float() / outputs.numel()
                                print(f"[ä¸¥é‡è­¦å‘Š] éªŒè¯é˜¶æ®µ: Batch {val_idx+1}: æ¨¡å‹è¾“å‡ºåŒ…å«NaN/Inf (æ¯”ä¾‹: {nan_ratio:.2%})ï¼Œè·³è¿‡è¯¥batch")
                                # å¦‚æœNaN/Infæ¯”ä¾‹è¿‡é«˜ï¼Œè¯´æ˜æ¨¡å‹å·²å´©æºƒï¼Œè·³è¿‡è¯¥batch
                                continue
                            
                            # åœ¨è®¡ç®—æŸå¤±å‰ï¼Œå…ˆæ£€æŸ¥å¹¶è£å‰ªlogitsåˆ°åˆç†èŒƒå›´ï¼Œé˜²æ­¢æ•°å€¼ä¸ç¨³å®š
                            outputs = torch.clamp(outputs, min=-10.0, max=10.0)
                            
                            # è®¡ç®—æŸå¤±ï¼ˆåŸºäºå•æ¬¡å‰å‘ä¼ æ’­ï¼Œä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
                            loss = self.compute_seg_loss(outputs, masks, bce_criterion, weights=epoch_loss_weights)
                            
                            # æ£€æŸ¥æŸå¤±å€¼
                            loss_value = loss.item()
                            if not np.isfinite(loss_value):
                                print(f"[è­¦å‘Š] éªŒè¯é˜¶æ®µ: Batch {val_idx+1}: æŸå¤±ä¸ºNaN/Infï¼Œä½¿ç”¨0.0")
                                loss_value = 0.0
                        
                        # åªæœ‰åœ¨æ‰€æœ‰æ£€æŸ¥é€šè¿‡åæ‰å¢åŠ val_sampleså’Œç´¯åŠ æŒ‡æ ‡
                        val_samples += batch_size
                        val_loss += loss_value * batch_size

                        # ã€å…³é”®ä¿®å¤ã€‘è®¡ç®—Diceç³»æ•°æ—¶ï¼Œä½¿ç”¨ä¸æµ‹è¯•é˜¶æ®µå®Œå…¨ç›¸åŒçš„æµç¨‹
                        # ä½†æ³¨æ„ï¼šä¸ºäº†ä¸è®­ç»ƒæŸå¤±ä¿æŒä¸€è‡´ï¼Œè¿™é‡Œä¸ä½¿ç”¨TTAï¼ˆå·²åœ¨ä¸Šé¢ä¿®å¤ï¼‰
                        # å¦‚æœéœ€è¦åœ¨éªŒè¯æ—¶ä¹Ÿä½¿ç”¨TTAè¯„ä¼°ï¼Œåº”è¯¥å•ç‹¬è®¡ç®—ä¸€ä¸ª"TTA Dice"ç”¨äºå‚è€ƒ
                        
                        probs = torch.sigmoid(outputs)
                        # è°ƒè¯•ï¼šæ£€æŸ¥æ¨¡å‹è¾“å‡ºèŒƒå›´å’Œmaskï¼ˆä»…åœ¨ç¬¬ä¸€ä¸ªepochçš„ç¬¬ä¸€ä¸ªbatchï¼‰
                        if epoch == 0 and val_idx == 0:
                            print(f"[è°ƒè¯•] éªŒè¯é˜¶æ®µ - æ¨¡å‹è¾“å‡ºèŒƒå›´: min={outputs.min().item():.4f}, max={outputs.max().item():.4f}, mean={outputs.mean().item():.4f}")
                            print(f"[è°ƒè¯•] éªŒè¯é˜¶æ®µ - SigmoidåèŒƒå›´: min={probs.min().item():.4f}, max={probs.max().item():.4f}, mean={probs.mean().item():.4f}")
                            print(f"[è°ƒè¯•] éªŒè¯é˜¶æ®µ - ä½¿ç”¨é˜ˆå€¼: {val_threshold:.4f}, é¢„æµ‹å‰æ™¯åƒç´ æ•°: {(probs > val_threshold).sum().item()}")
                            print(f"[è°ƒè¯•] éªŒè¯é˜¶æ®µ - Maskå‰æ™¯åƒç´ æ•°: {masks.sum().item():.0f}, æ€»åƒç´ æ•°: {masks.numel()}")
                        
                        # ç¡®ä¿ probs å’Œ masks çš„ç©ºé—´å°ºå¯¸åŒ¹é…
                        if probs.shape[2:] != masks.shape[2:]:
                            probs = F.interpolate(probs, size=masks.shape[2:], mode='bilinear', align_corners=False)
                        
                        # ä½¿ç”¨æœ€ä¼˜é˜ˆå€¼è¿›è¡ŒäºŒå€¼åŒ–ï¼ˆä¸æµ‹è¯•æ—¶ä¸€è‡´ï¼‰
                        preds = (probs > val_threshold).float()
                        
                        # ã€æ™ºèƒ½åå¤„ç†ã€‘å…ˆæŒ‰é¢ç§¯+æ¦‚ç‡è¿‡æ»¤å¾®å°ç—…ç¶/å™ªç‚¹ï¼Œå†è¿›è¡Œå½¢æ€å­¦ä¼˜åŒ–
                        # æ³¨æ„ï¼šåå¤„ç†åªå½±å“Diceè®¡ç®—ï¼Œä¸å½±å“æŸå¤±è®¡ç®—ï¼ˆæŸå¤±åŸºäºåŸå§‹logitsï¼‰
                        for i in range(preds.shape[0]):
                            pred_mask_tensor = preds[i, 0]
                            prob_map_tensor = probs[i, 0]
                            # å…ˆæ‰§è¡Œæ™ºèƒ½åå¤„ç†ï¼ˆä¸å†ç®€å•æŒ‰min_sizeè£å‰ªï¼‰
                            pred_mask_tensor = self.smart_post_processing(pred_mask_tensor, prob_map_tensor)
                            # å†æ‰§è¡Œä¼ ç»Ÿå½¢æ€å­¦åå¤„ç†ï¼Œä½†ä¸ç§»é™¤å°åŒºåŸŸï¼ˆmin_size=0ï¼‰
                            pred_mask_processed = self.post_process_mask(
                                pred_mask_tensor,
                                min_size=0,
                                use_morphology=True,
                                keep_largest=False,  # å…è®¸å¤šå‘ç—…ç¶åŒæ—¶å­˜åœ¨
                                fill_holes=True     # å¡«å……å­”æ´ï¼Œå»é™¤å‡é˜´æ€§ç©ºæ´
                            )
                            # post_process_maskä¼šè¿”å›tensoræˆ–numpyï¼Œéœ€è¦ç¡®ä¿æ˜¯tensor
                            if isinstance(pred_mask_processed, torch.Tensor):
                                preds[i, 0] = pred_mask_processed.to(preds.device)
                            else:
                                preds[i, 0] = torch.from_numpy(pred_mask_processed).float().to(preds.device)
                        
                        # ä½¿ç”¨ä¸è®­ç»ƒè¿‡ç¨‹ç›¸åŒçš„calculate_batch_diceå‡½æ•°è®¡ç®—Dice
                        batch_dice = self.calculate_batch_dice(preds.float(), masks)
                        val_dice += batch_dice.sum().item()
                        val_pred_fg_pixels += preds.sum().item()
                        val_gt_fg_pixels += masks.sum().item()
                        val_total_pixels += float(masks.numel())
                        
                        # è®¡ç®—æ‰¹æ¬¡ IoUï¼ˆé€æ ·æœ¬ï¼‰ï¼Œå¹¶åˆ†ç±»ç»Ÿè®¡
                        # è®¡ç®—æ‰¹æ¬¡ IoUï¼ˆé€æ ·æœ¬ï¼‰ï¼Œå¹¶åˆ†ç±»ç»Ÿè®¡
                        batch_size = masks.shape[0]
                        for i in range(batch_size):
                            mask_i = masks[i, 0]
                            mask_sum = mask_i.sum().item()
                            pred_i = preds[i, 0]
                            
                            # è®¡ç®—æ··æ·†çŸ©é˜µ
                            tp = torch.sum((pred_i > 0.5) & (mask_i > 0.5)).item()
                            fp = torch.sum((pred_i > 0.5) & (mask_i <= 0.5)).item()
                            fn = torch.sum((pred_i <= 0.5) & (mask_i > 0.5)).item()
                            tn = torch.sum((pred_i <= 0.5) & (mask_i <= 0.5)).item()
                            
                            # ã€ä¿®å¤ã€‘åˆ†åˆ«è®¡ç®—å‰æ™¯ç±»å’ŒèƒŒæ™¯ç±»çš„IoU
                            # å‰æ™¯ç±»IoUï¼ˆPositive Classï¼‰
                            iou_pos_den = tp + fp + fn
                            iou_pos_i = 1.0 if iou_pos_den < 1e-8 else tp / (iou_pos_den + 1e-8)
                            
                            # èƒŒæ™¯ç±»IoUï¼ˆNegative Classï¼‰
                            iou_neg_den = tn + fp + fn
                            iou_neg_i = 1.0 if iou_neg_den < 1e-8 else tn / (iou_neg_den + 1e-8)
                            
                            # æ•´ä½“IoUï¼ˆä½¿ç”¨å‰æ™¯ç±»IoUï¼Œä¸æ ‡å‡†å®šä¹‰ä¸€è‡´ï¼‰
                            val_iou += iou_pos_i
                            
                            # åˆ¤æ–­æ˜¯å¦ä¸ºç©ºmask
                            total_pixels = mask_i.numel()
                            avg_fg_ratio = val_gt_fg_pixels / max(1.0, val_total_pixels) if val_total_pixels > 0 else 0.0
                            adaptive_empty_threshold = max(1e-7, avg_fg_ratio * 0.001)
                            empty_threshold_pixels = adaptive_empty_threshold * total_pixels
                            
                            if mask_sum <= empty_threshold_pixels:
                                val_empty_mask_count += 1
                                val_empty_mask_dice_sum += batch_dice[i].item()
                                val_empty_mask_iou_sum += iou_neg_i  # âœ… ä½¿ç”¨èƒŒæ™¯ç±»IoU
                            else:
                                val_non_empty_mask_count += 1
                                val_non_empty_mask_dice_sum += batch_dice[i].item()
                                val_non_empty_mask_iou_sum += iou_pos_i  # âœ… ä½¿ç”¨å‰æ™¯ç±»IoU
                        
                        # æ›´æ–°éªŒè¯è¿›åº¦
                        val_progress = int(100 * (val_idx + 1) / len(val_loader))
                        current_avg_loss = val_loss / max(1, val_samples)
                        current_avg_dice = val_dice / max(1, val_samples)
                        # è®¡ç®—å½“å‰æ‰¹æ¬¡çš„ Dice_Pos å’Œ Dice_Negï¼ˆç”¨äºè¿›åº¦æ˜¾ç¤ºï¼‰
                        current_dice_pos = val_non_empty_mask_dice_sum / max(1, val_non_empty_mask_count) if val_non_empty_mask_count > 0 else 0.0
                        current_dice_neg = val_empty_mask_dice_sum / max(1, val_empty_mask_count) if val_empty_mask_count > 0 else 0.0
                        
                        self.update_val_progress.emit(
                            val_progress,
                            f"éªŒè¯è½®æ¬¡ {epoch+1} | æ‰¹æ¬¡ {val_idx+1}/{len(val_loader)}\n"
                            f"æŸå¤±: {current_avg_loss:.4f} | Dice_Pos: {current_dice_pos:.4f} | Dice_Neg: {current_dice_neg:.4f} | æ•´ä½“Dice: {current_avg_dice:.4f}"
                        )
                        
                        # æ¯5ä¸ªæ‰¹æ¬¡å¼ºåˆ¶æ›´æ–°UI
                        if val_idx % 5 == 0:
                            QApplication.processEvents()
                
                # è®¡ç®—å¹³å‡å€¼ï¼ˆç¡®ä¿æ²¡æœ‰NaN/Infï¼‰
                avg_train_loss = epoch_loss / max(1, train_samples)
                if not np.isfinite(avg_train_loss):
                    print(f"[è­¦å‘Š] Epoch {epoch+1}: è®­ç»ƒå¹³å‡æŸå¤±ä¸ºNaN/Infï¼Œä½¿ç”¨0.0")
                    avg_train_loss = 0.0
                
                val_dice /= max(1, val_samples)
                val_iou /= max(1, val_samples)
                if not np.isfinite(val_dice):
                    print(f"[è­¦å‘Š] Epoch {epoch+1}: éªŒè¯Diceä¸ºNaN/Infï¼Œä½¿ç”¨0.0")
                    val_dice = 0.0
                if not np.isfinite(val_iou):
                    print(f"[è­¦å‘Š] Epoch {epoch+1}: éªŒè¯IoUä¸ºNaN/Infï¼Œä½¿ç”¨0.0")
                    val_iou = 0.0

                # ä½¿ç”¨ ReduceLROnPlateau æ ¹æ®éªŒè¯Diceè‡ªåŠ¨è°ƒæ•´å­¦ä¹ ç‡ï¼ˆä¼˜å…ˆæå‡ç¨³å®šæ€§ï¼‰
                if plateau_scheduler is not None:
                    plateau_scheduler.step(val_dice)
                
                avg_val_loss = val_loss / max(1, val_samples)
                if not np.isfinite(avg_val_loss):
                    print(f"[è­¦å‘Š] Epoch {epoch+1}: éªŒè¯å¹³å‡æŸå¤±ä¸ºNaN/Infï¼Œä½¿ç”¨0.0")
                    avg_val_loss = 0.0
                
                pred_fg_ratio = val_pred_fg_pixels / max(1.0, val_total_pixels)
                gt_fg_ratio = val_gt_fg_pixels / max(1.0, val_total_pixels)
                
                # ã€å…³é”®ä¿®æ”¹ã€‘åˆ†åˆ«ç»Ÿè®¡æœ‰å‰æ™¯maskå’Œç©ºmaskçš„Dice/IoU
                dice_pos = val_non_empty_mask_dice_sum / max(1, val_non_empty_mask_count) if val_non_empty_mask_count > 0 else 0.0
                dice_neg = val_empty_mask_dice_sum / max(1, val_empty_mask_count) if val_empty_mask_count > 0 else 0.0
                iou_pos = val_non_empty_mask_iou_sum / max(1, val_non_empty_mask_count) if val_non_empty_mask_count > 0 else 0.0
                iou_neg = val_empty_mask_iou_sum / max(1, val_empty_mask_count) if val_empty_mask_count > 0 else 0.0
                empty_mask_ratio = val_empty_mask_count / max(1, val_samples) if val_samples > 0 else 0.0
                
                # è®°å½•åˆ°å†å²ä¸­
                self.val_dice_pos_history.append(dice_pos)
                self.val_dice_neg_history.append(dice_neg)
                
                print(
                    f"[éªŒè¯ç»Ÿè®¡] Epoch {epoch+1}: threshold={val_threshold:.3f}, "
                    f"pred_fg_ratio={pred_fg_ratio:.4f}, gt_fg_ratio={gt_fg_ratio:.4f}, "
                    f"val_dice={val_dice:.4f}, val_iou={val_iou:.4f}"
                )
                print(
                    f"[Dice/IoUåˆ†ç±»ç»Ÿè®¡] "
                    f"Dice_Pos: {dice_pos:.4f}, IoU_Pos: {iou_pos:.4f} ({val_non_empty_mask_count}/{val_samples}æ ·æœ¬) | "
                    f"Dice_Neg: {dice_neg:.4f}, IoU_Neg: {iou_neg:.4f} ({val_empty_mask_count}/{val_samples}æ ·æœ¬) | "
                    f"æ•´ä½“Dice: {val_dice:.4f}, æ•´ä½“IoU: {val_iou:.4f}"
                )

                # æ ¹æ®éªŒè¯Diceæˆ–SWAé˜¶æ®µè°ƒæ•´å­¦ä¹ ç‡ï¼ˆPolyç­–ç•¥ä¸‹ä»…ä¿ç•™SWAè°ƒåº¦ï¼‰
                swa_epoch_active = swa_enabled and epoch >= swa_start_epoch
                if swa_epoch_active and swa_model is not None:
                    swa_model.update_parameters(model)
                    if swa_scheduler is not None:
                        swa_scheduler.step()
                    swa_active_epochs += 1
                # Polyå­¦ä¹ ç‡å·²åœ¨epochå¼€å§‹æ—¶ç›´æ¥è®¾ç½®ï¼Œä¸å†ä½¿ç”¨scheduler/plateau_scheduler
                
                current_lr = optimizer.param_groups[0]['lr']
                
                # æ›´æ–°è®­ç»ƒå†å²
                self.train_loss_history.append(avg_train_loss)
                self.val_loss_history.append(avg_val_loss)
                self.val_dice_history.append(val_dice)
                
                # å‘é€è½®æ¬¡å®Œæˆä¿¡å·
                self.epoch_completed.emit(epoch + 1, avg_train_loss, avg_val_loss, val_dice)
                
                # æ¯ä¸ªepochç»“æŸåæ¸…ç†GPUç¼“å­˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                    import gc
                    gc.collect()
                
                # æ¯ä¸ªè½®æ¬¡ç»“æŸåç”Ÿæˆæ€§èƒ½åˆ†æå¯è§†åŒ–
                self.update_progress.emit(
                    int(70 + 20 * (epoch + 1) / self.epochs),
                    f"è½®æ¬¡ {epoch+1} å®Œæˆ (LR={current_lr:.6f})ï¼Œç”Ÿæˆæ€§èƒ½åˆ†æ..."
                )
                
                # ç”Ÿæˆæµ‹è¯•é›†åˆ†å‰²ç»“æœå¯è§†åŒ– - ä½¿ç”¨TTAæå‡æ€§èƒ½
                test_viz_path = self.visualize_test_results(
                    eval_model_for_epoch, 
                    val_loader, 
                    device, 
                    num_samples=6,  # æ¯ä¸ªè½®æ¬¡æ˜¾ç¤º6ä¸ªæ ·æœ¬
                    use_tta=True    # è®­ç»ƒç»“æŸåçš„æµ‹è¯•ä½¿ç”¨TTA
                )
                
                # è®¡ç®—å½“å‰è½®æ¬¡çš„æ€§èƒ½æŒ‡æ ‡ï¼ˆå¿«é€Ÿè¯„ä¼°ï¼‰
                model.eval()
                epoch_metrics = {
                    'dice': [],
                    'iou': [],
                    'precision': [],
                    'recall': [],
                    'sensitivity': [],
                    'specificity': [],
                    'f1': [],
                    'hd95': []
                }
                
                with torch.no_grad():
                    # åªè¯„ä¼°éƒ¨åˆ†éªŒè¯é›†ä»¥åŠ å¿«é€Ÿåº¦
                    eval_samples = min(20, len(val_dataset))  # æœ€å¤šè¯„ä¼°20ä¸ªæ ·æœ¬
                    eval_count = 0
                    
                    for batch_data in val_loader:
                        if eval_count >= eval_samples:
                            break
                        
                        # å¤„ç†æ•°æ®ï¼šå¯èƒ½åŒ…å«åˆ†ç±»æ ‡ç­¾
                        if len(batch_data) == 3:
                            images, masks, _ = batch_data
                        else:
                            images, masks = batch_data
                        images, masks = images.to(device), masks.to(device)
                        outputs = eval_model_for_epoch(images)
                        # ç¡®ä¿ outputs å’Œ masks çš„ç©ºé—´å°ºå¯¸åŒ¹é…
                        if outputs.shape[2:] != masks.shape[2:]:
                            outputs = F.interpolate(outputs, size=masks.shape[2:], mode='bilinear', align_corners=False)
                        preds = torch.sigmoid(outputs)
                        preds = (preds > val_threshold).float()
                        
                        for i in range(preds.shape[0]):
                            if eval_count >= eval_samples:
                                break
                                
                            pred = preds[i, 0]
                            mask = masks[i, 0]
                            
                            # åŒé‡æ£€æŸ¥å°ºå¯¸åŒ¹é…ï¼ˆä»¥é˜²ä¸‡ä¸€ï¼‰
                            if pred.shape != mask.shape:
                                pred = F.interpolate(pred.unsqueeze(0).unsqueeze(0), size=mask.shape, mode='bilinear', align_corners=False).squeeze(0).squeeze(0)
                            
                            # æ ‡å‡†æ··æ·†çŸ©é˜µå®šä¹‰ï¼Œç¡®ä¿ä¸ä¸»è¯„ä¼°ä¸€è‡´
                            tp = float((pred * mask).sum().item())
                            pred_sum = float(pred.sum().item())   # TP + FP
                            mask_sum = float(mask.sum().item())   # TP + FN
                            fp = float((pred * (1 - mask)).sum().item())
                            fn = float(((1 - pred) * mask).sum().item())
                            tn = float(((1 - pred) * (1 - mask)).sum().item())
                            
                            dice_den = 2.0 * tp + fp + fn
                            if dice_den < 1e-7:
                                dice = 1.0 if (mask_sum < 1e-7 and pred_sum < 1e-7) else 0.0
                            else:
                                dice = (2.0 * tp) / dice_den
                            
                            union = tp + fp + fn
                            iou = 1.0 if union < 1e-7 else tp / union
                            
                            if (tp + fp) < 1e-7:
                                precision = 1.0 if mask_sum < 1e-7 else 0.0
                            else:
                                precision = tp / (tp + fp)
                            
                            if (tp + fn) < 1e-7:
                                recall = 1.0 if pred_sum < 1e-7 else 0.0
                            else:
                                recall = tp / (tp + fn)
                            
                            specificity = 1.0 if (tn + fp) < 1e-7 else tn / (tn + fp)
                            
                            f1 = dice  # äºŒåˆ†ç±»ä¸‹F1=Dice
                            hd95 = calculate_hd95(
                                pred.cpu().numpy(),
                                mask.cpu().numpy()
                            )
                            
                            epoch_metrics['dice'].append(float(dice))
                            epoch_metrics['iou'].append(float(iou))
                            epoch_metrics['precision'].append(float(precision))
                            epoch_metrics['recall'].append(float(recall))
                            epoch_metrics['sensitivity'].append(float(recall))
                            epoch_metrics['specificity'].append(float(specificity))
                            epoch_metrics['f1'].append(float(f1))
                            epoch_metrics['hd95'].append(hd95)
                            
                            eval_count += 1
                        
                        if eval_count >= eval_samples:
                            break
                
                # è®¡ç®—å¹³å‡æŒ‡æ ‡
                avg_epoch_metrics = {}
                for k, values in epoch_metrics.items():
                    arr = np.array(values, dtype=float)
                    if arr.size == 0 or np.all(np.isnan(arr)):
                        avg_epoch_metrics[k] = float('nan')
                    else:
                        avg_epoch_metrics[k] = float(np.nanmean(arr))

                # åŸºäºå½“å‰é˜ˆå€¼çš„å¹³å‡æŒ‡æ ‡ï¼Œè®¡ç®—ç»¼åˆè¯„åˆ†
                hd95_mean = avg_epoch_metrics.get('hd95', float('inf'))
                total_score = calculate_custom_score(
                    dice=avg_epoch_metrics.get('dice', 0.0),
                    iou=avg_epoch_metrics.get('iou', 0.0),
                    precision=avg_epoch_metrics.get('precision', 0.0),
                    recall=avg_epoch_metrics.get('recall', 0.0),
                    specificity=avg_epoch_metrics.get('specificity', 0.0),
                    hd95=hd95_mean,
                )
                avg_epoch_metrics['score'] = float(total_score)

                # æ ¼å¼åŒ– HD95ï¼ˆå¤„ç† NaN/Inf æƒ…å†µï¼‰
                hd95_str = f"{hd95_mean:.4f}" if np.isfinite(hd95_mean) else "nan"
                print(
                    f"[éªŒè¯è¯„åˆ†] Epoch {epoch+1}: threshold={val_threshold:.3f}, "
                    f"TotalScore={total_score:.4f}, "
                    f"Dice={avg_epoch_metrics.get('dice', float('nan')):.4f}, "
                    f"IoU={avg_epoch_metrics.get('iou', float('nan')):.4f}, "
                    f"Precision={avg_epoch_metrics.get('precision', float('nan')):.4f}, "
                    f"Recall={avg_epoch_metrics.get('recall', float('nan')):.4f}, "
                    f"Specificity={avg_epoch_metrics.get('specificity', float('nan')):.4f}, "
                    f"HD95={hd95_str}"
                )
                
                # å‘é€epochåˆ†æç»“æœä¿¡å·ï¼ˆåŒ…å«ç»¼åˆè¯„åˆ†ï¼‰
                self.epoch_analysis_ready.emit(epoch + 1, test_viz_path, avg_epoch_metrics)
                
                # Save best model
                if val_dice > self.best_dice:
                    self.best_dice = val_dice
                    if self.save_best:
                        os.makedirs(self.best_model_cache_dir, exist_ok=True)
                        self.best_model_path = os.path.join(
                            self.best_model_cache_dir, f"best_model_dice_{val_dice:.4f}.pth"
                        )
                        self._save_checkpoint(eval_model_for_epoch, self.best_model_path)
                        self.model_saved.emit(f"å·²ä¿å­˜æœ€ä½³æ¨¡å‹ (Dice: {val_dice:.4f})")

                # æ¢å¤EMAæ¨¡å‹ä¸ºtrainæ¨¡å¼ï¼ˆå¦‚æœä½¿ç”¨äº†EMAï¼‰
                if self.use_ema and ema_model is not None and epoch >= self.ema_eval_start_epoch:
                    ema_model.train()
                
                # è§¦å‘æ—©åœ
                if early_stopping.step(val_dice):
                    early_stop_triggered = True
                    self.update_progress.emit(
                        min(90, int(70 + 20 * (epoch + 1) / max(1, self.epochs))),
                        "éªŒè¯Diceé•¿æœŸæœªæå‡ï¼Œè§¦å‘æ—©åœ..."
                    )
                    break
            
            # ç¡®å®šæœ€ç»ˆç”¨äºè¯„ä¼°çš„æ¨¡å‹ï¼ˆä¼˜å…ˆä½¿ç”¨EMAï¼Œå…¶æ¬¡SWAï¼Œæœ€åæ™®é€šæ¨¡å‹ï¼‰
            eval_model = model
            if self.use_ema and ema_model is not None and self.epochs >= self.ema_eval_start_epoch:
                self.update_progress.emit(87, "ä½¿ç”¨EMAæ¨¡å‹è¿›è¡Œæœ€ç»ˆè¯„ä¼°...")
                ema_model.eval()
                eval_model = ema_model
                if isinstance(model, nn.DataParallel):
                    eval_model = nn.DataParallel(ema_model)
            elif swa_enabled and swa_active_epochs > 0 and swa_model is not None:
                self.update_progress.emit(88, "åº”ç”¨SWAæƒé‡å¹¶æ›´æ–°BNç»Ÿè®¡...")
                # ä½¿ç”¨å®‰å…¨çš„BNæ›´æ–°å‡½æ•°ï¼Œå¤„ç†å¯èƒ½åŒ…å«åˆ†ç±»æ ‡ç­¾çš„æ•°æ®
                self._safe_update_bn(swa_model, train_loader, device)
                eval_model = swa_model
                if self.save_best:
                    swa_model_path = os.path.join(self.temp_dir, f"swa_model_epoch_{epoch+1}.pth")
                    self._save_checkpoint(eval_model, swa_model_path)
                    self.model_saved.emit(f"SWAå¹³æ»‘æ¨¡å‹å·²ä¿å­˜: {os.path.basename(swa_model_path)}")

            # æœ€ç»ˆè¯„ä¼°å’Œå¯è§†åŒ–
            self.update_progress.emit(90, "æ­£åœ¨æ‰§è¡Œæœ€ç»ˆè¯„ä¼°...")
            
            # ç”Ÿæˆè®­ç»ƒå†å²å›¾è¡¨
            history_path = self.plot_training_history()
            self.visualization_ready.emit(history_path)
            
            # æ‰§è¡Œç»¼åˆè¯„ä¼°ï¼ˆå•é˜¶æ®µï¼šä»…åˆ†å‰²æ¨¡å‹ï¼‰- ä½¿ç”¨TTAæå‡æ€§èƒ½
            self.update_progress.emit(92, "è®¡ç®—æ€§èƒ½æŒ‡æ ‡ï¼ˆå•é˜¶æ®µåˆ†å‰²æ¨¡å‹ï¼Œä½¿ç”¨TTAï¼‰...")
            detailed_metrics, metrics_path = self.evaluate_model(eval_model, val_loader, device, use_tta=True, adaptive_threshold=True)
            self.metrics_ready.emit(detailed_metrics)
            
            # ä¿å­˜å•é˜¶æ®µè¯„ä¼°ç»“æœç”¨äºå¯¹æ¯”
            single_stage_results = {
                'segmentation_dice': detailed_metrics['average']['dice'],
                'segmentation_iou': detailed_metrics['average']['iou'],
                'segmentation_precision': detailed_metrics['average']['precision'],
                'segmentation_recall': detailed_metrics['average']['recall'],
                'segmentation_f1': detailed_metrics['average']['f1']
            }
            
            # åˆ†ç±»æ¨¡å‹ç›¸å…³è¯„ä¼°å·²åˆ é™¤
            if False:  # å·²ç¦ç”¨ä¸¤é˜¶æ®µè¯„ä¼°
                self.update_progress.emit(93, "è¯„ä¼°ä¸¤é˜¶æ®µç³»ç»Ÿï¼ˆåˆ†ç±»+åˆ†å‰²ï¼‰...")
                try:
                    # é‡æ–°åˆ›å»ºéªŒè¯æ•°æ®åŠ è½½å™¨ï¼ˆå› ä¸ºval_loader_clså¯èƒ½ä¸åœ¨ä½œç”¨åŸŸå†…ï¼‰
                    val_dataset_cls = self.load_dataset(val_ids, val_transform, split_name="val", return_classification=True)
                    cpu_count = os.cpu_count() or 1
                    num_workers = max(0, min(4, cpu_count - 1))
                    val_loader_cls = DataLoader(
                        val_dataset_cls,
                        batch_size=self.batch_size,
                        shuffle=False,
                        num_workers=num_workers,
                        pin_memory=True,
                        persistent_workers=num_workers > 0
                    )
                    
                    # åŠ è½½åˆ†ç±»æ¨¡å‹
                    classification_model = self._build_classification_model(device)
                    cls_checkpoint = torch.load(self.classification_model_path, map_location=device)
                    actual_cls_model = self._unwrap_model(classification_model)
                    actual_cls_model.load_state_dict(cls_checkpoint['state_dict'])
                    classification_model.eval()
                    
                    # è¯„ä¼°åˆ†ç±»æ¨¡å‹ï¼ˆè‡ªåŠ¨å¯»æ‰¾æœ€ä¼˜é˜ˆå€¼ï¼‰
                    cls_metrics = self.evaluate_classification_model(classification_model, val_loader_cls, device)
                    
                    # ä½¿ç”¨è‡ªåŠ¨æ‰¾åˆ°çš„æœ€ä¼˜åˆ†ç±»é˜ˆå€¼
                    optimal_cls_threshold = cls_metrics.get('optimal_threshold', 0.5)
                    if optimal_cls_threshold != 0.5:
                        print(f"\n[ä¼˜åŒ–] è‡ªåŠ¨æ‰¾åˆ°æœ€ä¼˜åˆ†ç±»é˜ˆå€¼: {optimal_cls_threshold:.3f} (åŸé˜ˆå€¼: 0.5)")
                        print(f"[ä¼˜åŒ–] åœ¨æœ€ä¼˜é˜ˆå€¼ä¸‹çš„F1åˆ†æ•°: {cls_metrics.get('best_f1_at_threshold', 0.0):.4f}")
                    
                    # è¯„ä¼°ä¸¤é˜¶æ®µç³»ç»Ÿï¼ˆä½¿ç”¨æ”¹è¿›çš„çº§è”ç­–ç•¥ï¼‰
                    # ç­–ç•¥1ï¼šè‡ªé€‚åº”ç­–ç•¥ï¼ˆåªå¯¹é«˜ç½®ä¿¡åº¦çš„æ— ç—…å˜æ ·æœ¬è·³è¿‡åˆ†å‰²ï¼‰
                    two_stage_results_adaptive = self.evaluate_two_stage_system(
                        classification_model, eval_model, val_loader_cls, device,
                        classification_threshold=optimal_cls_threshold, 
                        segmentation_threshold=self.last_optimal_threshold,
                        use_adaptive_strategy=True,
                        confidence_threshold=0.85  # åªæœ‰æ— ç—…å˜æ¦‚ç‡>85%æ‰è·³è¿‡
                    )
                    
                    # ç­–ç•¥2ï¼šä¿å®ˆç­–ç•¥ï¼ˆæ‰€æœ‰æ ·æœ¬éƒ½è¿›è¡Œåˆ†å‰²ï¼Œåˆ†ç±»æ¨¡å‹ä»…ç”¨äºå¼•å¯¼ï¼‰
                    two_stage_results_conservative = self.evaluate_two_stage_system(
                        classification_model, eval_model, val_loader_cls, device,
                        classification_threshold=optimal_cls_threshold, 
                        segmentation_threshold=self.last_optimal_threshold,
                        use_adaptive_strategy=False  # æ‰€æœ‰æ ·æœ¬éƒ½åˆ†å‰²
                    )
                    
                    # é€‰æ‹©æœ€ä½³ç­–ç•¥ï¼ˆé€‰æ‹©æœ€æ¥è¿‘å•é˜¶æ®µæ€§èƒ½çš„ç­–ç•¥ï¼‰
                    adaptive_dice = two_stage_results_adaptive['system'].get('dice', 0.0)
                    conservative_dice = two_stage_results_conservative['system'].get('dice', 0.0)
                    single_dice = single_stage_results['segmentation_dice']
                    
                    if abs(adaptive_dice - single_dice) < abs(conservative_dice - single_dice):
                        two_stage_results = two_stage_results_adaptive
                        strategy_name = "è‡ªé€‚åº”ç­–ç•¥ï¼ˆé«˜ç½®ä¿¡åº¦è·³è¿‡ï¼‰"
                    else:
                        two_stage_results = two_stage_results_conservative
                        strategy_name = "ä¿å®ˆç­–ç•¥ï¼ˆå…¨éƒ¨åˆ†å‰²ï¼‰"
                    
                    print(f"\n[çº§è”ç­–ç•¥ä¼˜åŒ–] é€‰æ‹©ç­–ç•¥: {strategy_name}")
                    print(f"  - è‡ªé€‚åº”ç­–ç•¥Dice: {adaptive_dice:.4f} (è·³è¿‡ç‡: {two_stage_results_adaptive['system'].get('efficiency', {}).get('computation_saved', 0.0):.1f}%)")
                    print(f"  - ä¿å®ˆç­–ç•¥Dice: {conservative_dice:.4f} (è·³è¿‡ç‡: 0.0%)")
                    print(f"  - å•é˜¶æ®µDice: {single_dice:.4f}")
                    print(f"  - æœ€ç»ˆé€‰æ‹©: {strategy_name} (Dice: {two_stage_results['system'].get('dice', 0.0):.4f})")
                    
                    # ä¿å­˜å¯¹æ¯”è¯„ä¼°ç»“æœï¼ˆåŒ…å«ä¸¤ç§ç­–ç•¥ï¼‰
                    comparison_path = os.path.join(self.temp_dir, 'system_comparison.json')
                    import json
                    with open(comparison_path, 'w', encoding='utf-8') as f:
                        json.dump({
                            'single_stage': single_stage_results,
                            'two_stage': {
                                'adaptive_strategy': {
                                    'results': two_stage_results_adaptive,
                                    'dice': adaptive_dice
                                },
                                'conservative_strategy': {
                                    'results': two_stage_results_conservative,
                                    'dice': conservative_dice
                                },
                                'selected_strategy': strategy_name,
                                'final_results': two_stage_results
                            },
                            'classification_metrics': cls_metrics,
                            'comparison': {
                                'dice_improvement_adaptive': adaptive_dice - single_stage_results['segmentation_dice'],
                                'dice_improvement_conservative': conservative_dice - single_stage_results['segmentation_dice'],
                                'recommendation': 'two_stage_adaptive' if (adaptive_dice > single_stage_results['segmentation_dice'] + 0.01) else ('two_stage_conservative' if (conservative_dice > single_stage_results['segmentation_dice'] + 0.01) else 'single_stage')
                            }
                        }, f, ensure_ascii=False, indent=2)
                    
                    print("\n" + "="*60)
                    print("ã€æ€§èƒ½å¯¹æ¯”åˆ†æã€‘")
                    print("="*60)
                    
                    # å•é˜¶æ®µ vs ä¸¤é˜¶æ®µå¯¹æ¯”
                    print("\nã€å•é˜¶æ®µç³»ç»Ÿã€‘ï¼ˆä»…åˆ†å‰²æ¨¡å‹ï¼‰:")
                    print(f"  - Dice: {single_stage_results['segmentation_dice']:.4f}")
                    print(f"  - IoU: {single_stage_results['segmentation_iou']:.4f}")
                    print(f"  - Precision: {single_stage_results['segmentation_precision']:.4f}")
                    print(f"  - Recall: {single_stage_results['segmentation_recall']:.4f}")
                    print(f"  - F1: {single_stage_results['segmentation_f1']:.4f}")
                    
                    print("\nã€ä¸¤é˜¶æ®µç³»ç»Ÿã€‘ï¼ˆåˆ†ç±»+åˆ†å‰²ï¼‰:")
                    print(f"  åˆ†ç±»æ¨¡å‹å‡†ç¡®ç‡: {cls_metrics['accuracy']:.2f}%")
                    print(f"  åˆ†å‰²æ¨¡å‹æŒ‡æ ‡ï¼ˆä»…å¯¹åˆ†ç±»ä¸ºæœ‰ç—…å˜çš„æ ·æœ¬ï¼‰:")
                    print(f"    - Dice: {two_stage_results['segmentation']['dice']:.4f}")
                    print(f"    - IoU: {two_stage_results['segmentation']['iou']:.4f}")
                    print(f"  ç³»ç»Ÿæ•´ä½“æŒ‡æ ‡ï¼ˆæ‰€æœ‰æ ·æœ¬ï¼ŒåŒ…æ‹¬åˆ†ç±»é”™è¯¯ï¼‰:")
                    print(f"    - ç³»ç»ŸDice: {two_stage_results['system'].get('dice', 0.0):.4f} â­")
                    print(f"    - ç³»ç»ŸIoU: {two_stage_results['system'].get('iou', 0.0):.4f}")
                    print(f"    - ç³»ç»ŸF1: {two_stage_results['system']['f1']:.4f}")
                    print(f"    - ç³»ç»ŸPrecision: {two_stage_results['system']['precision']:.4f}")
                    print(f"    - ç³»ç»ŸRecall: {two_stage_results['system']['recall']:.4f}")
                    
                    # æ€§èƒ½å¯¹æ¯”åˆ†æ
                    print("\nã€æ€§èƒ½å¯¹æ¯”ã€‘:")
                    dice_diff = two_stage_results['system'].get('dice', 0.0) - single_stage_results['segmentation_dice']
                    if dice_diff > 0.01:
                        print(f"  âœ… ä¸¤é˜¶æ®µç³»ç»ŸDiceæå‡: +{dice_diff:.4f} ({(dice_diff/single_stage_results['segmentation_dice']*100):.1f}%)")
                        print(f"  ğŸ’¡ å»ºè®®ï¼šä½¿ç”¨ä¸¤é˜¶æ®µç³»ç»Ÿ")
                    elif dice_diff < -0.01:
                        print(f"  âš ï¸  ä¸¤é˜¶æ®µç³»ç»ŸDiceä¸‹é™: {dice_diff:.4f} ({(dice_diff/single_stage_results['segmentation_dice']*100):.1f}%)")
                        print(f"  ğŸ’¡ å»ºè®®ï¼šä»…ä½¿ç”¨åˆ†å‰²æ¨¡å‹ï¼ˆå•é˜¶æ®µï¼‰")
                    else:
                        print(f"  â¡ï¸  ä¸¤é˜¶æ®µç³»ç»ŸDiceå˜åŒ–: {dice_diff:+.4f} (åŸºæœ¬æŒå¹³)")
                        print(f"  ğŸ’¡ å»ºè®®ï¼šæ ¹æ®å®é™…éœ€æ±‚é€‰æ‹©ï¼ˆä¸¤é˜¶æ®µå¯èŠ‚çœè®¡ç®—ï¼Œå•é˜¶æ®µæ›´ç®€å•ï¼‰")
                    
                    # æ•ˆç‡åˆ†æ
                    if cls_metrics['accuracy'] > 0.7:
                        efficiency_gain = (1 - cls_metrics.get('false_positive_rate', 0.3)) * 100
                        print(f"\nã€æ•ˆç‡åˆ†æã€‘:")
                        print(f"  - åˆ†ç±»å‡†ç¡®ç‡: {cls_metrics['accuracy']:.2f}%")
                        print(f"  - é¢„è®¡å¯è·³è¿‡çº¦ {(1-cls_metrics.get('false_positive_rate', 0.3))*100:.1f}% çš„æ— ç—…å˜å›¾åƒåˆ†å‰²")
                        print(f"  - ä¸¤é˜¶æ®µç³»ç»Ÿå¯æ˜¾è‘—æå‡æ¨ç†æ•ˆç‡")
                    else:
                        print(f"\nã€æ•ˆç‡åˆ†æã€‘:")
                        print(f"  âš ï¸  åˆ†ç±»å‡†ç¡®ç‡è¾ƒä½ ({cls_metrics['accuracy']:.2f}%)ï¼Œå¯èƒ½å½±å“ç³»ç»Ÿæ•ˆç‡")
                        print(f"  ğŸ’¡ å»ºè®®ï¼šä¼˜åŒ–åˆ†ç±»æ¨¡å‹æˆ–ä»…ä½¿ç”¨åˆ†å‰²æ¨¡å‹")
                    
                    print("="*60 + "\n")
                    
                except Exception as e:
                    print(f"ä¸¤é˜¶æ®µè¯„ä¼°å‡ºé”™: {e}")
                    import traceback
                    traceback.print_exc()
            
            # ç”Ÿæˆæµ‹è¯•ç»“æœå¯è§†åŒ– - ä½¿ç”¨TTAæå‡æ€§èƒ½
            self.update_progress.emit(95, "ç”Ÿæˆæµ‹è¯•é›†åˆ†å‰²ç»“æœå¯è§†åŒ–ï¼ˆTTAï¼‰...")
            test_viz_path = self.visualize_test_results(eval_model, val_loader, device, num_samples=8, use_tta=True)
            
            # ç”Ÿæˆæ€§èƒ½åˆ†æ
            self.update_progress.emit(98, "ç”Ÿæˆæ€§èƒ½åˆ†ææŠ¥å‘Š...")
            perf_analysis_path = self.generate_performance_analysis(detailed_metrics)
            
            # ç”Ÿæˆæ³¨æ„åŠ›å¯è§†åŒ–ç”¨äºå¯è§£é‡Šæ€§åˆ†æï¼ˆè‹¥æ¨¡å‹æ”¯æŒï¼‰- ä½¿ç”¨TTA
            if self._supports_attention_maps(eval_model):
                self.update_progress.emit(99, "ç”Ÿæˆæ³¨æ„åŠ›å¯è§£é‡Šæ€§åˆ†æï¼ˆTTAï¼‰...")
                # æ³¨æ„ï¼švisualize_attention_maps å†…éƒ¨ä¼šä½¿ç”¨ return_attentionï¼ŒTTAå¯èƒ½ä¸æ”¯æŒï¼Œä¿æŒåŸæ ·
                attention_viz_path = self.visualize_attention_maps(eval_model, val_loader, device, num_samples=4)
                attention_stats = self.analyze_attention_statistics(eval_model, val_loader, device, num_samples=20)
            else:
                self.update_progress.emit(99, "å½“å‰æ¨¡å‹ä¸æ”¯æŒæ³¨æ„åŠ›å¯è§†åŒ–ï¼Œè·³è¿‡è¯¥æ­¥éª¤ã€‚")
                attention_viz_path = ""
                attention_stats = {}
            
            # å‘é€æµ‹è¯•ç»“æœä¿¡å·ï¼ŒåŒ…å«æ€§èƒ½åˆ†æè·¯å¾„
            self.test_results_ready.emit(test_viz_path, detailed_metrics)
            self.visualization_ready.emit(perf_analysis_path)  # åŒæ—¶å‘é€æ€§èƒ½åˆ†æ
            self.attention_analysis_ready.emit(attention_viz_path, attention_stats)  # å‘é€æ³¨æ„åŠ›åˆ†æ
            
            # è®­ç»ƒå®Œæˆ
            fallback_dice = self.val_dice_history[-1] if self.val_dice_history else 0.0
            final_best = self.best_dice if self.best_dice >= 0 else fallback_dice
            if early_stop_triggered:
                finish_msg = f"è®­ç»ƒæå‰ç»“æŸï¼ˆæ—©åœï¼‰ï¼Œæœ€ä½³Diceåˆ†æ•°: {final_best:.4f}"
            else:
                finish_msg = f"è®­ç»ƒå®Œæˆï¼æœ€ä½³Diceåˆ†æ•°: {final_best:.4f}"
            self.update_progress.emit(100, finish_msg)
            self.training_finished.emit(finish_msg, self.best_model_path if self.save_best else None)
            
        except KeyboardInterrupt:
            # ç”¨æˆ·æ‰‹åŠ¨ä¸­æ–­è®­ç»ƒï¼ˆCtrl+Cï¼‰
            print("\n[ç”¨æˆ·ä¸­æ–­] è®­ç»ƒå·²è¢«ç”¨æˆ·æ‰‹åŠ¨åœæ­¢")
            self.update_progress.emit(0, "è®­ç»ƒå·²è¢«ç”¨æˆ·ä¸­æ–­")
            self.training_finished.emit("è®­ç»ƒå·²è¢«ç”¨æˆ·ä¸­æ–­", None)
        except Exception as e:
            import traceback
            error_trace = traceback.format_exc()
            error_msg = f"è®­ç»ƒé”™è¯¯: {str(e)}"
            # æ‰“å°è¯¦ç»†é”™è¯¯ä¿¡æ¯ä»¥ä¾¿è°ƒè¯•
            print(f"\n{'='*60}")
            print("è®­ç»ƒé”™è¯¯è¯¦æƒ…:")
            print(f"{'='*60}")
            print(error_trace)
            print(f"{'='*60}\n")
            self.update_progress.emit(0, error_msg)
            self.training_finished.emit(error_msg, None)
        finally:
            # ç¡®ä¿é‡Šæ”¾GPUå†…å­˜
            torch.cuda.empty_cache()
    
    def stop(self):
        """å®‰å…¨åœæ­¢è®­ç»ƒ"""
        self.stop_requested = True     
    def __del__(self):
        """è‡ªåŠ¨æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        if os.path.exists(self.temp_dir):
            shutil.rmtree(self.temp_dir)
    
    def _collect_image_mask_paths(self, patient_ids: List[str]) -> Tuple[List[str], List[str]]:
        image_paths = []
        mask_paths = []
        
        for pid in patient_ids:
            patient_dir = os.path.join(self.data_dir, pid)
            if not os.path.exists(patient_dir):
                continue
                
            files = [f for f in os.listdir(patient_dir) 
                    if f.lower().endswith(('.tif', '.tiff', '.png', '.jpg', '.jpeg'))]
            
            for img_file in [f for f in files if 'mask' not in f.lower()]:
                base_name = os.path.splitext(img_file)[0]
                mask_file = self._find_matching_mask(files, base_name)
                if mask_file:
                    image_paths.append(os.path.join(patient_dir, img_file))
                    mask_paths.append(os.path.join(patient_dir, mask_file))
        return image_paths, mask_paths

    def _find_matching_mask(self, files: List[str], base_name: str) -> Optional[str]:
        """ä¸¥æ ¼åŒ¹é…å›¾åƒå¯¹åº”çš„maskï¼Œé¿å… base_name å­ä¸²é€ æˆä¸²å·ã€‚"""
        base_lower = base_name.lower()
        preferred_suffixes = ['_mask', '-mask', ' mask', '_seg', '-seg']

        def normalize(name: str) -> str:
            name_no_ext = os.path.splitext(name)[0].lower()
            for suffix in preferred_suffixes:
                if name_no_ext.endswith(suffix):
                    return name_no_ext[:-len(suffix)]
            return name_no_ext.replace('mask', '').strip('_- ')

        exact_match = None
        fuzzy_candidates = []
        for f in files:
            if 'mask' not in f.lower():
                continue
            normalized = normalize(f)
            if normalized == base_lower:
                exact_match = f
                break
            if base_lower in os.path.splitext(f)[0].lower():
                fuzzy_candidates.append(f)

        if exact_match:
            return exact_match
        if fuzzy_candidates:
            return sorted(fuzzy_candidates, key=lambda x: len(x))[0]
        return None

    def load_dataset(self, patient_ids, transform, split_name="train", return_classification=False, 
                     use_percentile_normalization=True, use_weighted_sampling=None):
        """
        åŠ è½½åŒ»å­¦å›¾åƒæ•°æ®é›†ï¼Œä¼˜å…ˆä½¿ç”¨MATLABç¼“å­˜
        
        Args:
            patient_ids: ç—…äººIDåˆ—è¡¨
            transform: æ•°æ®å¢å¼ºå˜æ¢
            split_name: æ•°æ®é›†åˆ†å‰²åç§°
            return_classification: æ˜¯å¦è¿”å›åˆ†ç±»æ ‡ç­¾
            use_percentile_normalization: æ˜¯å¦ä½¿ç”¨ç™¾åˆ†ä½æ•°å½’ä¸€åŒ–ï¼ˆp10-p99ï¼Œæ›´é²æ£’ï¼‰
            use_weighted_sampling: æ˜¯å¦ä½¿ç”¨åŸºäºmaskçš„æƒé‡é‡‡æ ·ï¼ˆNoneæ—¶è‡ªåŠ¨ï¼šè®­ç»ƒé›†å¯ç”¨ï¼ŒéªŒè¯é›†ç¦ç”¨ï¼‰
        """
        image_paths, mask_paths = self._collect_image_mask_paths(patient_ids)
        self.split_metadata[split_name] = {
            'image_paths': image_paths,
            'mask_paths': mask_paths
        }
        extra_modalities = self._prepare_extra_modalities(image_paths)
        
        # è‡ªåŠ¨å†³å®šæ˜¯å¦ä½¿ç”¨æƒé‡é‡‡æ ·
        if use_weighted_sampling is None:
            use_weighted_sampling = (split_name == "train")
        
        base_dataset = MedicalImageDataset(
            image_paths,
            mask_paths,
            transform,
            training=(split_name == "train"),
            return_classification=return_classification,
            extra_modalities=extra_modalities,
            context_slices=self.context_slices,
            context_gap=self.context_gap,
            use_percentile_normalization=use_percentile_normalization,
            use_weighted_sampling=use_weighted_sampling
        )

        return base_dataset

    def _prepare_extra_modalities(self, image_paths: List[str]) -> Optional[Dict[str, List[Optional[str]]]]:
        if not self.extra_modalities_dirs:
            return None
        return build_extra_modalities_lists(image_paths, self.extra_modalities_dirs)


    def _estimate_pos_weight(self, mask_paths: List[str], sample_size: int = 100) -> float:
        """ä¼°ç®—æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹ï¼Œè‡ªé€‚åº”è°ƒèŠ‚BCEçš„pos_weightã€‚"""
        if not mask_paths:
            return 1.0

        sample_paths = random.sample(mask_paths, min(sample_size, len(mask_paths)))
        total_pos = 0
        total_neg = 0

        for path in sample_paths:
            mask = cv2.imread(path, cv2.IMREAD_GRAYSCALE)
            if mask is None:
                continue
            pos = int(np.count_nonzero(mask))
            neg = int(mask.size - pos)
            total_pos += pos
            total_neg += neg

        if total_pos == 0:
            return 1.0
        ratio = total_neg / max(total_pos, 1)
        return float(max(ratio, 1.0))

    def _build_model(self, device, swin_params=None, dstrans_params: Optional[dict] = None):
        """
        æ ¹æ®é…ç½®æ„å»ºæ¨¡å‹ï¼Œæ”¯æŒResNet34ç¼–ç å™¨UNetã€æ”¹è¿›UNetã€TransUNetã€SwinUNetã€Swin-U Mambaã€‚
        
        Args:
            device: è®¾å¤‡
            swin_params: SwinUNetçš„è¶…å‚æ•°ï¼ˆå¦‚æœä½¿ç”¨GWOä¼˜åŒ–ï¼‰
            dstrans_params: DS-TransUNetçš„è¶…å‚æ•°ï¼ˆå¦‚æœä½¿ç”¨GWOä¼˜åŒ–ï¼‰
        """
        if self.model_type == "resnet_unet":
            # é»˜è®¤å†»ç»“ç¼–ç å™¨ï¼Œå‰50% epochåªè®­ç»ƒè§£ç å™¨ï¼Œå50%è§£å†»è¿›è¡Œå¾®è°ƒ
            freeze_encoder = True  # å¯ä»¥é€šè¿‡é…ç½®æ§åˆ¶
            model = ResNetUNet(freeze_encoder=freeze_encoder).to(device)
        elif self.model_type == "trans_unet" or self.model_type == "transunet":
            model = TransUNet().to(device)
            self.update_progress.emit(15, "ä½¿ç”¨Transformer+UNetæ··åˆæ¶æ„ï¼ˆå¯æé«˜DiceæŒ‡æ ‡ï¼‰")
        elif self.model_type in ("ds_trans_unet", "dstransunet", "ds-transunet"):
            dstrans_kwargs = {
                "in_channels": 3,
                "out_channels": 1,
                "embed_dim": 256,
                "num_heads": 8,
                "num_layers": 2,
                "mlp_ratio": 4.0,
                "dropout": 0.1,
            }
            if dstrans_params:
                dstrans_kwargs.update(copy.deepcopy(dstrans_params))
            # ç§»é™¤DSTransUNetä¸æ¥å—çš„å†…ç½®å‚æ•°
            dstrans_kwargs.pop('_from_checkpoint', None)
            if dstrans_kwargs["embed_dim"] % dstrans_kwargs["num_heads"] != 0:
                dstrans_kwargs["embed_dim"] = dstrans_kwargs["num_heads"] * max(1, dstrans_kwargs["embed_dim"] // dstrans_kwargs["num_heads"])
            model = DSTransUNet(**dstrans_kwargs).to(device)
            self.update_progress.emit(15, "ä½¿ç”¨DS-TransUNetï¼ˆåŒå°ºåº¦Transformer+UNetï¼Œå¢å¼ºå¤šå°ºåº¦ç‰¹å¾æå–ï¼‰")
        elif self.model_type == "swin_unet" or self.model_type == "swinunet":
            swin_kwargs = {
                "in_channels": 3,
                "out_channels": 1
            }
            if swin_params:
                swin_kwargs.update(copy.deepcopy(swin_params))
            # å¦‚æœå‚æ•°æ¥è‡ªcheckpointæ¨æ–­ï¼Œè·³è¿‡å½’ä¸€åŒ–ä»¥ä¿æŒå…¼å®¹
            from_checkpoint = swin_params and swin_params.get('_from_checkpoint', False)
            if not from_checkpoint:
                normalized_embed = SwinUNet._normalize_embed_dim(swin_kwargs.get('embed_dim', 96))
                swin_kwargs['embed_dim'] = normalized_embed
            img_size = swin_kwargs.get('img_size', (224, 224))
            if isinstance(img_size, int):
                img_size = (img_size, img_size)
            patch_size = swin_kwargs.get('patch_size', (4, 4))
            if isinstance(patch_size, int):
                patch_size = (patch_size, patch_size)
            grid_h = max(2, img_size[0] // max(1, patch_size[0]))
            if not from_checkpoint:
                normalized_window = SwinUNet._normalize_window_size(swin_kwargs.get('window_size', 8), max_grid=grid_h)
                swin_kwargs['window_size'] = normalized_window
            if 'drop_path_rate' not in swin_kwargs:
                swin_kwargs['drop_path_rate'] = 0.1 if not from_checkpoint else 0.0
            swin_kwargs['img_size'] = img_size
            swin_kwargs['patch_size'] = patch_size
            # ä¿ç•™_from_checkpointå’Œ_mlp_hidden_dimsä¼ ç»™SwinUNet
            model = SwinUNet(**swin_kwargs).to(device)
            final_embed = swin_kwargs.get('embed_dim', 96)
            final_window = swin_kwargs.get('window_size', 8)
            self.update_progress.emit(
                15,
                f"ä½¿ç”¨SwinUNetï¼ˆå‚æ•°ï¼šembed_dim={int(final_embed)}, window_size={int(final_window)}ï¼‰"
            )
        elif self.model_type in ("swin_u_mamba", "swin-u-mamba", "swinumamba"):
            mamba_kwargs = {
                "in_channels": 3,
                "out_channels": 1,
                "base_channels": 64,
                "num_blocks": (2, 2, 2, 2),
                "dropout": 0.05,
            }
            if swin_params:
                mamba_kwargs.update(copy.deepcopy(swin_params))
            model = SwinUMamba(**mamba_kwargs).to(device)
            self.update_progress.emit(
                15,
                f"ä½¿ç”¨Swin-U Mambaï¼ˆbase_channels={mamba_kwargs.get('base_channels',64)}, blocks={mamba_kwargs.get('num_blocks',(2,2,2,2))}ï¼‰"
            )
        else:
            model = ImprovedUNet().to(device)

        if torch.cuda.device_count() > 1:
            model = nn.DataParallel(model)
            self.update_progress.emit(20, f"ä½¿ç”¨ {torch.cuda.device_count()} ä¸ªGPUè¿›è¡Œè®­ç»ƒ")
        # åˆå§‹åŒ–SkullStripper
        if self.use_skull_stripper and self.skull_stripper is None:
            self.skull_stripper = SkullStripper(self.skull_stripper_path, device, self.skull_stripper_threshold)
            if not self.skull_stripper.is_available():
                self.use_skull_stripper = False
                print("[è­¦å‘Š] SkullStripperæœªå‡†å¤‡å¥½ï¼Œå°†è·³è¿‡å‰¥é™¤é¢…éª¨æ­¥éª¤ã€‚")
        return model

    def _apply_skull_strip(self, images: torch.Tensor) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """
        å¦‚æœå¯ç”¨SkullStripperï¼Œåˆ™å¯¹è¾“å…¥è¿›è¡Œå‰¥é™¤é¢…éª¨å¤„ç†ã€‚
        Returns:
            processed_images, brain_mask
        """
        if not self.use_skull_stripper or not self.skull_stripper or not self.skull_stripper.is_available():
            return images, None
        return self.skull_stripper.strip(images)

    # åˆ†ç±»æ¨¡å‹ç›¸å…³å‡½æ•°å·²åˆ é™¤

    def _safe_update_bn(self, model, dataloader, device):
        """å®‰å…¨åœ°æ›´æ–°BNç»Ÿè®¡é‡ï¼Œå¤„ç†å¯èƒ½åŒ…å«åˆ†ç±»æ ‡ç­¾çš„æ•°æ®åŠ è½½å™¨"""
        model.train()
        with torch.no_grad():
            for batch_data in dataloader:
                # å¤„ç†æ•°æ®ï¼šå¯èƒ½åŒ…å«åˆ†ç±»æ ‡ç­¾
                if len(batch_data) == 3:
                    images, masks, _ = batch_data
                else:
                    images, masks = batch_data
                images = images.to(device)
                _ = model(images)  # åªä½¿ç”¨imagesæ¥æ›´æ–°BNç»Ÿè®¡é‡
    
    def _unwrap_model(self, model):
        """è§£åŒ…DataParallelï¼Œè¿”å›å®é™…æ¨¡å‹"""
        actual = model
        if isinstance(actual, nn.DataParallel):
            actual = actual.module
        if isinstance(actual, AveragedModel):
            # AveragedModelåŒ…è£…äº†åŸå§‹æ¨¡å‹ï¼Œä½äºmoduleå±æ€§
            actual = actual.module
        return actual

    def _supports_aux_outputs(self, model):
        """æ¨¡å‹æ˜¯å¦æ”¯æŒè¾…åŠ©è¾“å‡º"""
        actual = self._unwrap_model(model)
        return isinstance(actual, (ImprovedUNet, TransUNet, DSTransUNet, SwinUNet))

    def _supports_attention_maps(self, model):
        """æ¨¡å‹æ˜¯å¦æä¾›æ³¨æ„åŠ›å›¾"""
        actual = self._unwrap_model(model)
        return isinstance(actual, (ImprovedUNet, TransUNet, DSTransUNet, SwinUNet, ResNetUNet))
    
    def _create_optimizer(self, parameters, lr):
        # å¾®è°ƒé˜¶æ®µç»Ÿä¸€æ”¶ç´§å­¦ä¹ ç‡ï¼šå¤§äº1e-4çš„å¼ºåˆ¶å‹åˆ°1e-4ï¼Œè‹¥æ°å¥½ç­‰äº1e-4åˆ™è¿›ä¸€æ­¥é™ä¸º1e-5
        effective_lr = float(lr)
        if effective_lr > 1e-4:
            effective_lr = 1e-4
        elif abs(effective_lr - 1e-4) < 1e-9:
            effective_lr = 1e-5
        # è‹¥å¤–éƒ¨å·²ä¼ å…¥æ›´å°çš„å­¦ä¹ ç‡ï¼ˆå¦‚2e-5ï¼‰ï¼Œåˆ™ä¿æŒä¸å˜
        if self.optimizer_type == "adam":
            return optim.Adam(parameters, lr=effective_lr, betas=(0.9, 0.999), weight_decay=5e-4)
        if self.optimizer_type == "sgd":
            # ä½¿ç”¨ SGD + Nesterov åŠ¨é‡
            return optim.SGD(parameters, lr=effective_lr, momentum=0.99, nesterov=True, weight_decay=5e-4)
        # é»˜è®¤ä½¿ç”¨AdamW - å°æ•°æ®é›†å¢å¼ºæ­£åˆ™åŒ–
        return optim.AdamW(parameters, lr=effective_lr, weight_decay=5e-4)
    
    def _get_loss_weights(self, epoch: int, total_epochs: int) -> Dict[str, float]:
        """ä¼˜åŒ–çš„æŸå¤±æƒé‡ç­–ç•¥ - æ›´å¼ºè°ƒDiceå’ŒTversky"""
        progress = epoch / max(1, total_epochs - 1)
        # æ—©æœŸï¼šBCEä¸»å¯¼å¸®åŠ©æ”¶æ•›ï¼›åæœŸï¼šDice+Tverskyä¸»å¯¼æå‡åˆ†å‰²è´¨é‡
        weights = {
            # BCE åªè´Ÿè´£å‰æœŸæ”¶æ•›, åæœŸæƒé‡ä¸‹é™åˆ°è¾ƒä½æ°´å¹³
            'bce': max(0.10, 0.30 - 0.18 * progress),
            # Dice ä»ä¸€å¼€å§‹å°±å æ¯”è¾ƒé«˜, éšepochè¿›ä¸€æ­¥æå‡
            'dice': 0.45 + 0.30 * progress,          # 0.45 -> 0.75
            # Tversky åœ¨åæœŸé…åˆDice, æ›´å…³æ³¨ FN
            'tversky': 0.25 + 0.15 * progress,       # 0.25 -> 0.40
            # Focal Tversky é’ˆå¯¹éš¾æ¡ˆä¾‹ï¼Œé€æ­¥åŠ æƒ
            'tversky_focal': 0.05 + 0.10 * progress,  # 0.05 -> 0.15
            # è¾¹ç•ŒæŸå¤±ç¨å¾®é™ä½, é˜²æ­¢è¿‡åº¦å…³æ³¨ç»†å°å™ªå£°
            'boundary': 0.08,
            # Hausdorff è·ç¦»æŸå¤±ï¼šè®­ç»ƒå‰30%å…³é—­ï¼Œä¹‹åæ¸è¿›å¼€å¯ï¼ˆä¸“æ³¨è¾¹ç•Œï¼‰
            'hausdorff': 0.08 * max((progress - 0.3) / 0.7, 0.0),
            # Focal ä¸»è¦åœ¨å‰æœŸèµ·ä½œç”¨, åæœŸæƒé‡å¾ˆå°
            'focal': max(0.03, 0.10 * (1.0 - progress)),
            # Lovasz åœ¨å…¨ç¨‹å‚ä¸, ä½†åæœŸæ¯”é‡æ›´é«˜, å¯¹é½ IoU/Dice
            'lovasz': 0.05 + 0.10 * progress,        # 0.05 -> 0.15
            # å‡é˜´æ€§æƒ©ç½šé€æ¸å¢åŠ , æé«˜å¬å›ç‡, ä¸€èˆ¬èƒ½æ‹‰é«˜Dice
            'fn_penalty': 0.06 + 0.09 * progress,    # 0.06 -> 0.15
            # å‡é˜³æ€§æƒ©ç½šéšepochç•¥å¾®ä¸‹é™, è®©æ¨¡å‹åœ¨åæœŸæ›´æ•¢é¢„æµ‹å‰æ™¯
            'fp_penalty': 0.18 + 0.12 * (1.0 - progress),  # 0.30 -> 0.18
        }
        total = sum(weights.values())
        for k in weights:
            weights[k] /= total
        return weights
    
    def _init_ema_model(self, model, device):
        """
        åˆå§‹åŒ–EMAæ¨¡å‹å‰¯æœ¬
        æ³¨æ„ï¼šEMAæ¨¡å‹ä¿æŒtrain()æ¨¡å¼ï¼Œä»¥ä¾¿BNç»Ÿè®¡é‡ä¹Ÿèƒ½æ­£ç¡®æ›´æ–°
        """
        actual_model = self._unwrap_model(model)
        ema_model = copy.deepcopy(actual_model).to(device)
        # ä¿æŒtrainæ¨¡å¼ï¼Œè¿™æ ·BNçš„runningç»Ÿè®¡é‡ä¹Ÿèƒ½è¢«EMAæ›´æ–°
        ema_model.train()
        # ç¦ç”¨æ¢¯åº¦è®¡ç®—
        for param in ema_model.parameters():
            param.requires_grad = False
        # ç¡®ä¿åˆå§‹æƒé‡å®Œå…¨åŒæ­¥ï¼ˆä½¿ç”¨decay=0è¿›è¡Œä¸€æ¬¡æ›´æ–°ï¼Œç¡®ä¿å®Œå…¨å¤åˆ¶ï¼‰
        # è¿™æ ·EMAæ¨¡å‹ä»ä¸€å¼€å§‹å°±å’ŒåŸæ¨¡å‹å®Œå…¨ä¸€è‡´
        with torch.no_grad():
            ema_state = ema_model.state_dict()
            model_state = actual_model.state_dict()
            for key in ema_state.keys():
                if key in model_state:
                    ema_state[key].copy_(model_state[key])
        return ema_model
    
    def _update_ema_model(self, ema_model, model, decay=None):
        """
        ä½¿ç”¨å½“å‰æ¨¡å‹å‚æ•°æ›´æ–°EMAæ¨¡å‹
        åŒæ—¶æ›´æ–°BNçš„running_meanå’Œrunning_var
        """
        if ema_model is None:
            return
        if decay is None:
            decay = self.ema_decay
        if not 0.0 < decay < 1.0:
            decay = 0.995
        
        actual_model = self._unwrap_model(model)
        
        with torch.no_grad():
            # æ›´æ–°æ™®é€šå‚æ•°ï¼ˆåªæ›´æ–°requires_grad=Trueçš„å‚æ•°ï¼‰
            for ema_param, model_param in zip(ema_model.parameters(), actual_model.parameters()):
                if model_param.requires_grad:
                    ema_param.data.mul_(decay).add_(model_param.data, alpha=1.0 - decay)
            
            # æ›´æ–°BNå±‚çš„runningç»Ÿè®¡é‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            # ä½¿ç”¨state_dictæ¥ç¡®ä¿æ­£ç¡®åŒ¹é…æ¨¡å—
            ema_state = ema_model.state_dict()
            model_state = actual_model.state_dict()
            
            for key in ema_state.keys():
                if 'running_mean' in key or 'running_var' in key:
                    if key in model_state:
                        ema_state[key].mul_(decay).add_(model_state[key], alpha=1.0 - decay)
                elif 'num_batches_tracked' in key:
                    if key in model_state:
                        ema_state[key] = model_state[key]

    def _sanitize_gradients(self, model, clamp_value=5.0):
        """
        æ¸…ç†æ¢¯åº¦ä¸­çš„NaN/Infï¼Œé¿å…ä¼ æ’­åˆ°åç»­æ­¥éª¤ã€‚
        Returns:
            bool: æ˜¯å¦å‘ç°å¹¶ä¿®å¤äº†å¼‚å¸¸æ¢¯åº¦
        """
        had_issue = False
        actual_model = self._unwrap_model(model)
        for name, param in actual_model.named_parameters():
            if param.grad is None:
                continue
            if not torch.isfinite(param.grad).all():
                had_issue = True
                param.grad = torch.nan_to_num(param.grad, nan=0.0, posinf=clamp_value, neginf=-clamp_value)
                param.grad.clamp_(min=-clamp_value, max=clamp_value)
        return had_issue
    
    def _extract_model_config(self, model):
        actual = self._unwrap_model(model)
        config = {"model_type": self.model_type}
        if isinstance(actual, SwinUNet):
            config["swin_params"] = copy.deepcopy(actual.get_config())
        if isinstance(actual, DSTransUNet):
            config["dstrans_params"] = copy.deepcopy(actual.get_config())
        if isinstance(actual, ResNetUNet):
            # ä»æ¨¡å‹ç»“æ„ä¸­æ¨æ–­backbone_name
            # æ£€æŸ¥enc4çš„è¾“å‡ºé€šé“æ•°æ¥åˆ¤æ–­æ˜¯ResNet50è¿˜æ˜¯ResNet101
            if hasattr(actual, 'enc4'):
                # ResNet101çš„layer4è¾“å‡º2048é€šé“ï¼ŒResNet50ä¹Ÿæ˜¯2048ï¼Œä½†å¯ä»¥é€šè¿‡layeræ•°é‡åˆ¤æ–­
                # æ›´ç®€å•çš„æ–¹æ³•ï¼šæ£€æŸ¥æ˜¯å¦æœ‰backbone_nameå±æ€§ï¼Œæˆ–è€…ä»state_dictæ¨æ–­
                backbone_name = getattr(actual, 'backbone_name', 'resnet101')
                config["resnet_params"] = {
                    "in_channels": getattr(actual, 'in_channels', 3),
                    "out_channels": getattr(actual, 'out_channels', 1),
                    "pretrained": False,  # æµ‹è¯•æ—¶ä¸éœ€è¦pretrained
                    "backbone_name": backbone_name
                }
        config["best_threshold"] = float(getattr(self, "last_optimal_threshold", 0.5))
        config["skull_stripping"] = {
            "enabled": self.use_skull_stripper,
            "model_path": self.skull_stripper_path,
            "threshold": self.skull_stripper_threshold
        }
        config["context"] = {
            "slices": self.context_slices,
            "gap": self.context_gap
        }
        config["extra_modalities"] = list(self.extra_modalities_dirs.keys())
        return config
    
    def _save_checkpoint(self, model, path):
        actual = self._unwrap_model(model)
        state_dict = actual.state_dict()
        config = self._extract_model_config(model)
        torch.save({"state_dict": state_dict, "config": config}, path)
    
    def _gwo_optimize_swin_params(self, train_loader, val_loader, device, n_wolves=10, max_iter=5):
        """
        ä½¿ç”¨GWOä¼˜åŒ–SwinUNetçš„è¶…å‚æ•°
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            device: è®¾å¤‡
            n_wolves: ç°ç‹¼æ•°é‡ï¼ˆå‡å°‘ä»¥åŠ å¿«ä¼˜åŒ–é€Ÿåº¦ï¼‰
            max_iter: æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼ˆå‡å°‘ä»¥åŠ å¿«ä¼˜åŒ–é€Ÿåº¦ï¼‰
        
        Returns:
            æœ€ä½³å‚æ•°å­—å…¸
        """
        def objective_func(params):
            """ç›®æ ‡å‡½æ•°ï¼šè®­ç»ƒæ¨¡å‹å¹¶è¿”å›éªŒè¯Diceåˆ†æ•°"""
            try:
                params = params.copy()
                params['embed_dim'] = SwinUNet._normalize_embed_dim(params.get('embed_dim', 96))
                params['window_size'] = SwinUNet._normalize_window_size(params.get('window_size', 8), max_grid=64)
                # åˆ›å»ºä¸´æ—¶æ¨¡å‹ - å°æ•°æ®é›†é»˜è®¤æ›´é«˜dropout
                temp_model = SwinUNet(
                    embed_dim=int(params['embed_dim']),
                    window_size=int(params['window_size']),
                    mlp_ratio=params.get('mlp_ratio', 4.0),
                    drop_rate=params.get('drop_rate', 0.2),
                    attn_drop_rate=params.get('attn_drop_rate', 0.2)
                ).to(device)
                
                # å¿«é€Ÿè®­ç»ƒå‡ ä¸ªæ‰¹æ¬¡æ¥è¯„ä¼°å‚æ•°
                temp_model.train()
                optimizer = self._create_optimizer(temp_model.parameters(), lr=1e-4)
                bce_criterion = nn.BCEWithLogitsLoss()
                
                # å¿«é€Ÿè®­ç»ƒï¼ˆä»…å‡ ä¸ªæ‰¹æ¬¡ï¼‰
                max_batches = 5
                for batch_idx, batch_data in enumerate(train_loader):
                    if batch_idx >= max_batches:
                        break
                    # å¤„ç†æ•°æ®ï¼šå¯èƒ½åŒ…å«åˆ†ç±»æ ‡ç­¾
                    if len(batch_data) == 3:
                        images, masks, _ = batch_data
                    else:
                        images, masks = batch_data
                    images, masks = images.to(device), masks.to(device)
                    optimizer.zero_grad()
                    outputs = temp_model(images)
                    loss = bce_criterion(outputs, masks)
                    loss.backward()
                    optimizer.step()
                
                # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°ï¼ˆæ”¹è¿›ï¼šåŠ å…¥Hausdorff Distanceä½œä¸ºä¼˜åŒ–ç›®æ ‡ï¼‰
                temp_model.eval()
                dice_scores = []
                hd95_scores = []
                # ä½¿ç”¨ä¸ä¸»éªŒè¯é˜¶æ®µä¸€è‡´çš„é˜ˆå€¼ï¼Œé¿å…Diceä¸ä¸€è‡´
                eval_threshold = float(getattr(self, "last_optimal_threshold", 0.5))
                with torch.no_grad():
                    for batch_idx, batch_data in enumerate(val_loader):
                        if batch_idx >= 3:  # ä»…è¯„ä¼°å‡ ä¸ªæ‰¹æ¬¡
                            break
                        # å¤„ç†æ•°æ®ï¼šå¯èƒ½åŒ…å«åˆ†ç±»æ ‡ç­¾
                        if len(batch_data) == 3:
                            images, masks, _ = batch_data
                        else:
                            images, masks = batch_data
                        images, masks = images.to(device), masks.to(device)
                        outputs = temp_model(images)
                        preds = torch.sigmoid(outputs)
                        # ç¡®ä¿ preds å’Œ masks çš„ç©ºé—´å°ºå¯¸åŒ¹é…
                        if preds.shape[2:] != masks.shape[2:]:
                            preds = F.interpolate(preds, size=masks.shape[2:], mode='bilinear', align_corners=False)
                        preds = preds > eval_threshold
                        batch_dice = self.calculate_batch_dice(preds.float(), masks)
                        dice_scores.extend(batch_dice.cpu().numpy())
                        
                        # è®¡ç®—Hausdorff Distance 95
                        try:
                            for i in range(preds.shape[0]):
                                pred_mask = preds[i, 0].cpu().numpy()
                                target_mask = masks[i, 0].cpu().numpy()
                                hd95 = calculate_hd95(pred_mask, target_mask)
                                if not np.isnan(hd95):
                                    hd95_scores.append(hd95)
                        except Exception:
                            pass  # å¦‚æœHD95è®¡ç®—å¤±è´¥ï¼Œè·³è¿‡
                
                avg_dice = np.mean(dice_scores) if dice_scores else 0.0
                avg_hd95 = np.mean(hd95_scores) if hd95_scores else 0.0
                
                # ç»„åˆä¼˜åŒ–ç›®æ ‡ï¼šDiceè¶Šé«˜è¶Šå¥½ï¼ŒHD95è¶Šä½è¶Šå¥½
                # å½’ä¸€åŒ–HD95ï¼ˆå‡è®¾æœ€å¤§HD95ä¸º100åƒç´ ï¼‰ï¼Œç„¶åä¸Diceç»„åˆ
                normalized_hd95 = 1.0 - min(avg_hd95 / 100.0, 1.0)  # å½’ä¸€åŒ–åˆ°[0, 1]ï¼Œè¶Šé«˜è¶Šå¥½
                combined_score = 0.7 * avg_dice + 0.3 * normalized_hd95  # Diceæƒé‡70%ï¼ŒHD95æƒé‡30%
                
                del temp_model
                torch.cuda.empty_cache() if torch.cuda.is_available() else None
                return combined_score
            except Exception as e:
                print(f"GWOè¯„ä¼°é”™è¯¯: {e}")
                return 0.0
        
        # å®šä¹‰å‚æ•°è¾¹ç•Œ
        bounds = {
            'embed_dim': (64, 128),
            'window_size': (4, 12),
            'mlp_ratio': (2.0, 6.0),
            'drop_rate': (0.15, 0.35),  # å°æ•°æ®é›†æ›´é«˜dropout
            'attn_drop_rate': (0.15, 0.35),
        }
        
        # åˆ›å»ºGWOä¼˜åŒ–å™¨
        gwo = GWOOptimizer(
            n_wolves=n_wolves,
            max_iter=max_iter,
            bounds=bounds,
            objective_func=objective_func
        )
        
        # æ‰§è¡Œä¼˜åŒ–
        def callback(iter, score, params):
            self.update_progress.emit(13, f"GWOè¿­ä»£ {iter}/{max_iter}, å½“å‰æœ€ä½³ç»¼åˆåˆ†æ•°: {score:.4f} (Dice+HD95)")
        
        best_params, best_score, history = gwo.optimize(callback=callback)
        if best_params:
            best_params['embed_dim'] = SwinUNet._normalize_embed_dim(best_params.get('embed_dim', 96))
            best_params['window_size'] = SwinUNet._normalize_window_size(best_params.get('window_size', 8), max_grid=64)
        
        return best_params
    
    def _gwo_optimize_nnformer_params(self, train_loader, val_loader, device, n_wolves=5, max_iter=2):
        """
        ä½¿ç”¨GWOä¼˜åŒ–nnFormerçš„è¶…å‚æ•°
        
        æ³¨æ„ï¼šä¸ºäº†å‡å°‘å†…å­˜å ç”¨ï¼Œé»˜è®¤ä½¿ç”¨è¾ƒå°‘çš„wolveså’Œè¿­ä»£æ¬¡æ•°
        å¦‚æœå†…å­˜å……è¶³ï¼Œå¯ä»¥å¢åŠ è¿™äº›å‚æ•°ä»¥æé«˜ä¼˜åŒ–æ•ˆæœ
        """
        # è·Ÿè¸ªè¯„ä¼°è®¡æ•°å’Œå†…å­˜ä½¿ç”¨
        eval_count = [0]  # ä½¿ç”¨åˆ—è¡¨ä»¥ä¾¿åœ¨é—­åŒ…ä¸­ä¿®æ”¹
        total_evals = n_wolves * (max_iter + 1)  # åˆå§‹è¯„ä¼° + æ¯æ¬¡è¿­ä»£
        
        def objective_func(params):
            temp_model = None
            optimizer = None
            scaler = None
            try:
                eval_count[0] += 1
                current_eval = eval_count[0]
                
                # è·å–è¯„ä¼°å‰çš„å†…å­˜
                mem_before = self._get_gpu_memory_info()
                
                params = params.copy()
                embed_dim = int(params.get('embed_dim', 96))
                window_size = int(params.get('window_size', 7))
                mlp_ratio = float(params.get('mlp_ratio', 4.0))
                drop_rate = float(params.get('drop_rate', 0.0))
                attn_drop_rate = float(params.get('attn_drop_rate', 0.0))
                drop_path_rate = float(params.get('drop_path_rate', 0.1))
                global_attn_ratio = float(params.get('global_attn_ratio', 0.5))
                
                # ç¡®ä¿embed_dimèƒ½è¢«num_headsæ•´é™¤
                # æ ¹æ®embed_dimè‡ªåŠ¨è®¡ç®—åˆé€‚çš„num_heads
                if embed_dim >= 96:
                    num_heads = [3, 6, 12, 24]
                elif embed_dim >= 64:
                    num_heads = [2, 4, 8, 16]
                else:
                    num_heads = [2, 4, 6, 12]
                
                # å¯è§†åŒ–ï¼šæ˜¾ç¤ºå½“å‰è¯„ä¼°ä¿¡æ¯
                param_str = f"embed_dim={embed_dim}, window={window_size}, mlp={mlp_ratio:.2f}, drop={drop_rate:.2f}, global_attn={global_attn_ratio:.2f}"
                mem_str = f"å†…å­˜: {mem_before[0]:.2f}GB / {mem_before[1]:.2f}GB"
                progress_msg = f"GWOè¯„ä¼° [{current_eval}/{total_evals}] | {param_str} | {mem_str}"
                self.update_progress.emit(10 + int(80 * current_eval / total_evals), progress_msg)
                
                # æ£€æŸ¥å†…å­˜ä½¿ç”¨ï¼Œå¦‚æœè¿‡é«˜åˆ™æå‰è¿”å›
                if mem_before[0] > 13.0:  # å¦‚æœå·²ä½¿ç”¨è¶…è¿‡13GBï¼Œç›´æ¥è·³è¿‡
                    print(f"è­¦å‘Š: GPUå†…å­˜ä½¿ç”¨è¿‡é«˜ ({mem_before[0]:.2f}GB)ï¼Œè·³è¿‡æ­¤è¯„ä¼°ä»¥é¿å…å´©æºƒ")
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        torch.cuda.synchronize()
                    import gc
                    gc.collect()
                    return 0.0
                
                # åˆ›å»ºä¸´æ—¶æ¨¡å‹
                temp_model = nnFormer(
                    in_channels=3,
                    out_channels=1,
                    img_size=224,
                    patch_size=4,
                    embed_dim=embed_dim,
                    depths=[2, 2, 2, 2],
                    num_heads=num_heads,
                    window_size=window_size,
                    mlp_ratio=mlp_ratio,
                    drop_rate=drop_rate,
                    attn_drop_rate=attn_drop_rate,
                    drop_path_rate=drop_path_rate,
                    use_skip_attention=True,
                    global_attn_ratio=global_attn_ratio
                ).to(device)
                
                # å¿«é€Ÿè®­ç»ƒå‡ ä¸ªæ‰¹æ¬¡æ¥è¯„ä¼°å‚æ•°
                temp_model.train()
                optimizer = self._create_optimizer(temp_model.parameters(), lr=1e-4)
                bce_criterion = nn.BCEWithLogitsLoss()
                
                # æ··åˆç²¾åº¦è®­ç»ƒ
                amp_enabled = (device.type == 'cuda')
                scaler = GradScaler('cuda', enabled=amp_enabled) if amp_enabled else None
                
                # å¿«é€Ÿè®­ç»ƒï¼ˆä»…å‡ ä¸ªæ‰¹æ¬¡ï¼‰
                max_batches = 3  # å‡å°‘æ‰¹æ¬¡ä»¥èŠ‚çœå†…å­˜
                for batch_idx, batch_data in enumerate(train_loader):
                    if batch_idx >= max_batches:
                        break
                    
                    if len(batch_data) == 3:
                        images, masks, _ = batch_data
                    else:
                        images, masks = batch_data
                    images, masks = images.to(device), masks.to(device)
                    
                    optimizer.zero_grad(set_to_none=True)
                    if scaler is not None:
                        with torch.amp.autocast('cuda'):
                            outputs = temp_model(images)
                            if outputs.shape[2:] != masks.shape[2:]:
                                outputs = F.interpolate(outputs, size=masks.shape[2:], mode='bilinear', align_corners=False)
                            loss = bce_criterion(outputs, masks)
                        scaler.scale(loss).backward()
                        scaler.step(optimizer)
                        scaler.update()
                    else:
                        outputs = temp_model(images)
                        if outputs.shape[2:] != masks.shape[2:]:
                            outputs = F.interpolate(outputs, size=masks.shape[2:], mode='bilinear', align_corners=False)
                        loss = bce_criterion(outputs, masks)
                        loss.backward()
                        optimizer.step()
                    
                    # æ¸…ç†
                    del outputs, loss, images, masks
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        torch.cuda.synchronize()
                
                # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
                temp_model.eval()
                dice_scores = []
                eval_threshold = float(getattr(self, "last_optimal_threshold", 0.5))
                with torch.no_grad():
                    for batch_idx, batch_data in enumerate(val_loader):
                        if batch_idx >= 2:  # ä»…è¯„ä¼°2ä¸ªæ‰¹æ¬¡
                            break
                        
                        if len(batch_data) == 3:
                            images, masks, _ = batch_data
                        else:
                            images, masks = batch_data
                        images, masks = images.to(device), masks.to(device)
                        
                        if scaler is not None:
                            with torch.amp.autocast('cuda'):
                                outputs = temp_model(images)
                        else:
                            outputs = temp_model(images)
                        
                        if outputs.shape[2:] != masks.shape[2:]:
                            outputs = F.interpolate(outputs, size=masks.shape[2:], mode='bilinear', align_corners=False)
                        
                        preds = torch.sigmoid(outputs)
                        preds = (preds > eval_threshold).float()
                        batch_dice = self.calculate_batch_dice(preds, masks)
                        dice_scores.extend(batch_dice.cpu().numpy())
                        
                        del images, masks, outputs, preds, batch_dice
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()
                            torch.cuda.synchronize()
                
                if not dice_scores:
                    return 0.0
                dice_mean = float(np.mean(dice_scores))
                
                # è·å–è¯„ä¼°åçš„å†…å­˜
                mem_after = self._get_gpu_memory_info()
                mem_diff = mem_after[0] - mem_before[0]
                
                # å¯è§†åŒ–ï¼šæ˜¾ç¤ºè¯„ä¼°ç»“æœ
                result_msg = f"è¯„ä¼°å®Œæˆ | Dice: {dice_mean:.4f} | å†…å­˜å˜åŒ–: {mem_diff:+.2f}GB"
                self.update_progress.emit(10 + int(80 * current_eval / total_evals), result_msg)
                
                return dice_mean
            except Exception as e:
                print(f"GWOè¯„ä¼°é”™è¯¯: {e}")
                import traceback
                traceback.print_exc()
                return 0.0
            finally:
                # å…³é”®ï¼šæ˜¾å¼é‡Šæ”¾èµ„æº
                if temp_model is not None:
                    temp_model.cpu()
                    del temp_model
                if optimizer is not None:
                    optimizer.state.clear()
                    del optimizer
                if scaler is not None:
                    del scaler
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                    torch.cuda.reset_peak_memory_stats()
                import gc
                gc.collect()

        # å®šä¹‰å‚æ•°è¾¹ç•Œ
        bounds = {
            'embed_dim': (64, 128),  # æ•´æ•°èŒƒå›´
            'window_size': (4, 10),  # æ•´æ•°èŒƒå›´
            'mlp_ratio': (3.0, 5.0),  # æµ®ç‚¹æ•°èŒƒå›´
            'drop_rate': (0.0, 0.2),  # æµ®ç‚¹æ•°èŒƒå›´
            'attn_drop_rate': (0.0, 0.2),  # æµ®ç‚¹æ•°èŒƒå›´
            'drop_path_rate': (0.05, 0.15),  # æµ®ç‚¹æ•°èŒƒå›´
            'global_attn_ratio': (0.3, 0.7),  # æµ®ç‚¹æ•°èŒƒå›´ï¼Œæ§åˆ¶å…¨å±€æ³¨æ„åŠ›çš„æ¯”ä¾‹
        }

        # æ£€æŸ¥åˆå§‹å†…å­˜ä½¿ç”¨
        initial_mem = self._get_gpu_memory_info()
        if initial_mem[0] > 12.0:
            warning_msg = f"è­¦å‘Š: GPUå†…å­˜ä½¿ç”¨å·²è¾ƒé«˜ ({initial_mem[0]:.2f}GB)ï¼Œå»ºè®®å…³é—­å…¶ä»–ç¨‹åºåå†è¿è¡ŒGWOä¼˜åŒ–"
            self.update_progress.emit(10, warning_msg)
            print(warning_msg)
        
        gwo = GWOOptimizer(
            n_wolves=n_wolves,
            max_iter=max_iter,
            bounds=bounds,
            objective_func=objective_func,
        )

        def callback(iter, score, params):
            mem_allocated, mem_reserved, mem_max = self._get_gpu_memory_info()
            mem_percent = (mem_allocated / 16.0) * 100 if torch.cuda.is_available() else 0
            
            param_info = f"embed_dim={int(params.get('embed_dim', 96))}, "
            param_info += f"window={int(params.get('window_size', 7))}, "
            param_info += f"mlp={params.get('mlp_ratio', 4.0):.2f}, "
            param_info += f"global_attn={params.get('global_attn_ratio', 0.5):.2f}"
            
            status_msg = f"GWOè¿­ä»£ {iter}/{max_iter} | æœ€ä½³Dice: {score:.4f} | {param_info} | GPUå†…å­˜: {mem_allocated:.2f}GB ({mem_percent:.1f}%)"
            
            if mem_percent > 90:
                status_msg += " âš ï¸âš ï¸ å†…å­˜ä¸¥é‡ä¸è¶³ï¼"
            elif mem_percent > 85:
                status_msg += " âš ï¸ å†…å­˜ä½¿ç”¨è¿‡é«˜ï¼"
            elif mem_percent > 70:
                status_msg += " âš¡ å†…å­˜ä½¿ç”¨è¾ƒé«˜"
            
            self.update_progress.emit(10 + int(80 * iter / max_iter), status_msg)

        # æ˜¾ç¤ºå¼€å§‹ä¿¡æ¯
        total_evals = n_wolves * (max_iter + 1)
        start_msg = f"å¼€å§‹GWOä¼˜åŒ–nnFormer: {n_wolves}ä¸ªwolves, {max_iter}æ¬¡è¿­ä»£, å…±{total_evals}æ¬¡è¯„ä¼° | åˆå§‹å†…å­˜: {initial_mem[0]:.2f}GB"
        self.update_progress.emit(10, start_msg)
        print(start_msg)
        
        best_params, best_score, history = gwo.optimize(callback=callback)
        
        if best_params:
            # ç¡®ä¿å‚æ•°ç±»å‹æ­£ç¡®
            best_params['embed_dim'] = int(best_params.get('embed_dim', 96))
            best_params['window_size'] = int(best_params.get('window_size', 7))
            best_params['mlp_ratio'] = float(best_params.get('mlp_ratio', 4.0))
            best_params['drop_rate'] = float(best_params.get('drop_rate', 0.0))
            best_params['attn_drop_rate'] = float(best_params.get('attn_drop_rate', 0.0))
            best_params['drop_path_rate'] = float(best_params.get('drop_path_rate', 0.1))
            best_params['global_attn_ratio'] = float(best_params.get('global_attn_ratio', 0.5))
        
        final_msg = f"GWOä¼˜åŒ–å®Œæˆ | æœ€ä½³Dice: {best_score:.4f} | æœ€ä½³å‚æ•°: {best_params}"
        self.update_progress.emit(14, final_msg)
        print(final_msg)
        
        return best_params
    
    def _get_gpu_memory_info(self):
        """è·å–GPUå†…å­˜ä½¿ç”¨ä¿¡æ¯"""
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**3  # GB
            reserved = torch.cuda.memory_reserved() / 1024**3  # GB
            max_allocated = torch.cuda.max_memory_allocated() / 1024**3  # GB
            return allocated, reserved, max_allocated
        return 0.0, 0.0, 0.0
    
    def _gwo_optimize_dstrans_params(self, train_loader, val_loader, device, n_wolves=5, max_iter=2):
        """
        ä½¿ç”¨GWOä¼˜åŒ–DS-TransUNetçš„è¶…å‚æ•°
        
        æ³¨æ„ï¼šä¸ºäº†å‡å°‘å†…å­˜å ç”¨ï¼Œé»˜è®¤ä½¿ç”¨è¾ƒå°‘çš„wolveså’Œè¿­ä»£æ¬¡æ•°
        å¦‚æœå†…å­˜å……è¶³ï¼Œå¯ä»¥å¢åŠ è¿™äº›å‚æ•°ä»¥æé«˜ä¼˜åŒ–æ•ˆæœ
        """
        # è·Ÿè¸ªè¯„ä¼°è®¡æ•°å’Œå†…å­˜ä½¿ç”¨
        eval_count = [0]  # ä½¿ç”¨åˆ—è¡¨ä»¥ä¾¿åœ¨é—­åŒ…ä¸­ä¿®æ”¹
        total_evals = n_wolves * (max_iter + 1)  # åˆå§‹è¯„ä¼° + æ¯æ¬¡è¿­ä»£
        
        def objective_func(params):
            temp_model = None
            optimizer = None
            try:
                eval_count[0] += 1
                current_eval = eval_count[0]
                
                # è·å–è¯„ä¼°å‰çš„å†…å­˜
                mem_before = self._get_gpu_memory_info()
                
                params = params.copy()
                embed_dim = int(params.get('embed_dim', 256))
                num_heads = int(params.get('num_heads', 8))
                num_layers = int(params.get('num_layers', 2))
                mlp_ratio = float(params.get('mlp_ratio', 4.0))
                dropout = float(params.get('dropout', 0.1))
                if embed_dim % num_heads != 0:
                    embed_dim = num_heads * max(1, embed_dim // num_heads)
                
                # å¯è§†åŒ–ï¼šæ˜¾ç¤ºå½“å‰è¯„ä¼°ä¿¡æ¯
                param_str = f"embed_dim={embed_dim}, heads={num_heads}, layers={num_layers}, mlp={mlp_ratio:.2f}, drop={dropout:.2f}"
                mem_str = f"å†…å­˜: {mem_before[0]:.2f}GB / {mem_before[1]:.2f}GB"
                progress_msg = f"GWOè¯„ä¼° [{current_eval}/{total_evals}] | {param_str} | {mem_str}"
                self.update_progress.emit(10 + int(80 * current_eval / total_evals), progress_msg)
                
                # æ£€æŸ¥å†…å­˜ä½¿ç”¨ï¼Œå¦‚æœè¿‡é«˜åˆ™æå‰è¿”å›ï¼ˆæ›´ä¸¥æ ¼çš„é™åˆ¶ï¼‰
                if mem_before[0] > 13.0:  # å¦‚æœå·²ä½¿ç”¨è¶…è¿‡13GBï¼Œç›´æ¥è·³è¿‡ï¼ˆä»14GBé™ä½åˆ°13GBï¼‰
                    print(f"è­¦å‘Š: GPUå†…å­˜ä½¿ç”¨è¿‡é«˜ ({mem_before[0]:.2f}GB)ï¼Œè·³è¿‡æ­¤è¯„ä¼°ä»¥é¿å…å´©æºƒ")
                    # å¼ºåˆ¶æ¸…ç†
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        torch.cuda.synchronize()
                    import gc
                    gc.collect()
                    return 0.0
                
                # åˆ›å»ºä¸´æ—¶æ¨¡å‹ï¼ˆä½¿ç”¨æ›´å°çš„embed_dimèŒƒå›´ä»¥å‡å°‘å†…å­˜ï¼‰
                temp_model = DSTransUNet(
                    embed_dim=embed_dim,
                    num_heads=num_heads,
                    num_layers=num_layers,
                    mlp_ratio=mlp_ratio,
                    dropout=dropout
                ).to(device)
                
                # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒä»¥å‡å°‘å†…å­˜
                temp_model.train()
                optimizer = self._create_optimizer(temp_model.parameters(), lr=1e-4)
                bce_criterion = nn.BCEWithLogitsLoss()
                # ä½¿ç”¨ torch.amp.GradScaler ä»¥é¿å…å¼ƒç”¨è­¦å‘Š
                scaler = GradScaler('cuda', enabled=torch.cuda.is_available())
                
                # é™åˆ¶è®­ç»ƒæ‰¹æ¬¡ä»¥å‡å°‘å†…å­˜ä½¿ç”¨
                max_batches = 1  # è®­ç»ƒ1ä¸ªbatch
                for batch_idx, batch_data in enumerate(train_loader):
                    if batch_idx >= max_batches:
                        break
                    
                    # ç«‹å³é‡Šæ”¾batch_dataå¼•ç”¨
                    if len(batch_data) == 3:
                        images, masks, _ = batch_data
                        del batch_data
                    else:
                        images, masks = batch_data
                        del batch_data
                    
                    images, masks = images.to(device), masks.to(device)
                    
                    # ä½¿ç”¨æ··åˆç²¾åº¦
                    if scaler is not None:
                        optimizer.zero_grad(set_to_none=True)  # æ›´å½»åº•åœ°æ¸…é›¶æ¢¯åº¦
                        with torch.amp.autocast('cuda'):
                            outputs = temp_model(images)
                            # ç¡®ä¿è¾“å‡ºå°ºå¯¸ä¸maskå°ºå¯¸åŒ¹é…
                            if outputs.shape[2:] != masks.shape[2:]:
                                outputs = F.interpolate(outputs, size=masks.shape[2:], mode='bilinear', align_corners=False)
                            loss = bce_criterion(outputs, masks)
                        scaler.scale(loss).backward()
                        scaler.step(optimizer)
                        scaler.update()
                    else:
                        optimizer.zero_grad(set_to_none=True)  # æ›´å½»åº•åœ°æ¸…é›¶æ¢¯åº¦
                        outputs = temp_model(images)
                        # ç¡®ä¿è¾“å‡ºå°ºå¯¸ä¸maskå°ºå¯¸åŒ¹é…
                        if outputs.shape[2:] != masks.shape[2:]:
                            outputs = F.interpolate(outputs, size=masks.shape[2:], mode='bilinear', align_corners=False)
                        loss = bce_criterion(outputs, masks)
                        loss.backward()
                        optimizer.step()
                    
                    # å½»åº•æ¸…ç†æ‰€æœ‰ä¸­é—´å˜é‡
                    del outputs, loss, images, masks
                    # æ¯æ¬¡batchåéƒ½æ¸…ç†ç¼“å­˜
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        torch.cuda.synchronize()
                
                temp_model.eval()
                dice_scores = []
                eval_threshold = float(getattr(self, "last_optimal_threshold", 0.5))
                with torch.no_grad():
                    for batch_idx, batch_data in enumerate(val_loader):
                        if batch_idx >= 1:  # è¯„ä¼°1ä¸ªbatch
                            break
                        
                        # ç«‹å³é‡Šæ”¾batch_dataå¼•ç”¨
                        if len(batch_data) == 3:
                            images, masks, _ = batch_data
                            del batch_data
                        else:
                            images, masks = batch_data
                            del batch_data
                        
                        images, masks = images.to(device), masks.to(device)
                        
                        # ä½¿ç”¨æ··åˆç²¾åº¦æ¨ç†
                        if scaler is not None:
                            with torch.amp.autocast('cuda'):
                                outputs = temp_model(images)
                        else:
                            outputs = temp_model(images)
                        
                        # ç¡®ä¿è¾“å‡ºå°ºå¯¸ä¸maskå°ºå¯¸åŒ¹é…
                        if outputs.shape[2:] != masks.shape[2:]:
                            outputs = F.interpolate(outputs, size=masks.shape[2:], mode='bilinear', align_corners=False)
                        
                        preds = torch.sigmoid(outputs)
                        preds = (preds > eval_threshold).float()
                        dice_scores_batch = self.calculate_batch_dice(preds, masks)
                        # ç«‹å³è½¬ç§»åˆ°CPUå¹¶è½¬æ¢ä¸ºnumpyï¼Œé‡Šæ”¾GPUå†…å­˜
                        dice_scores_batch_cpu = dice_scores_batch.cpu().numpy()
                        dice_scores.extend(dice_scores_batch_cpu)
                        
                        # å½»åº•æ¸…ç†æ‰€æœ‰ä¸­é—´å˜é‡
                        del images, masks, outputs, preds, dice_scores_batch, dice_scores_batch_cpu
                        # æ¯æ¬¡batchåéƒ½æ¸…ç†ç¼“å­˜
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()
                            torch.cuda.synchronize()
                
                if not dice_scores:
                    return 0.0
                dice_mean = float(np.mean(dice_scores))
                
                # è·å–è¯„ä¼°åçš„å†…å­˜
                mem_after = self._get_gpu_memory_info()
                mem_diff = mem_after[0] - mem_before[0]
                
                # å¯è§†åŒ–ï¼šæ˜¾ç¤ºè¯„ä¼°ç»“æœ
                result_msg = f"è¯„ä¼°å®Œæˆ | Dice: {dice_mean:.4f} | å†…å­˜å˜åŒ–: {mem_diff:+.2f}GB"
                self.update_progress.emit(10 + int(80 * current_eval / total_evals), result_msg)
                
                return dice_mean
            except Exception as e:
                print(f"GWOè¯„ä¼°é”™è¯¯: {e}")
                import traceback
                traceback.print_exc()
                return 0.0
            finally:
                # å…³é”®ï¼šæ˜¾å¼é‡Šæ”¾èµ„æº
                if temp_model is not None:
                    # å…ˆæ¸…é™¤æ¨¡å‹çš„æ‰€æœ‰å‚æ•°å’Œç¼“å†²åŒº
                    temp_model.cpu()  # ç§»åˆ°CPU
                    del temp_model
                if optimizer is not None:
                    # æ¸…é™¤ä¼˜åŒ–å™¨çŠ¶æ€
                    optimizer.state.clear()
                    del optimizer
                if scaler is not None:
                    del scaler
                # æ¸…ç†GPUç¼“å­˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                    # é‡ç½®æœ€å¤§å†…å­˜ç»Ÿè®¡ï¼Œä»¥ä¾¿ç›‘æ§æ¯æ¬¡è¯„ä¼°çš„å†…å­˜å³°å€¼
                    torch.cuda.reset_peak_memory_stats()
                # å¼ºåˆ¶Pythonåƒåœ¾å›æ”¶
                import gc
                gc.collect()

        # æ³¨æ„ï¼šæ•´æ•°å‚æ•°ä½¿ç”¨æ•´æ•°è¾¹ç•Œï¼Œæµ®ç‚¹æ•°å‚æ•°ä½¿ç”¨æµ®ç‚¹æ•°è¾¹ç•Œ
        # ä¸ºäº†å‡å°‘å†…å­˜å ç”¨ï¼Œç¼©å°å‚æ•°æœç´¢èŒƒå›´
        bounds = {
            'embed_dim': (128, 192),  # æ•´æ•°èŒƒå›´ï¼ˆä»128-256ç¼©å°åˆ°128-192ï¼‰
            'num_heads': (4, 6),  # æ•´æ•°èŒƒå›´ï¼ˆä»4-8ç¼©å°åˆ°4-6ï¼‰
            'num_layers': (2, 2),  # æ•´æ•°èŒƒå›´ï¼ˆå›ºå®šä¸º2å±‚ï¼Œæœ€å°åŒ–å†…å­˜ï¼‰
            'mlp_ratio': (3.0, 4.0),  # æµ®ç‚¹æ•°èŒƒå›´ï¼ˆä»3.0-4.5ç¼©å°åˆ°3.0-4.0ï¼‰
            'dropout': (0.05, 0.1),  # æµ®ç‚¹æ•°èŒƒå›´ï¼ˆä»0.05-0.15ç¼©å°åˆ°0.05-0.1ï¼‰
        }

        # æ£€æŸ¥åˆå§‹å†…å­˜ä½¿ç”¨
        initial_mem = self._get_gpu_memory_info()
        if initial_mem[0] > 12.0:  # å¦‚æœåˆå§‹å†…å­˜å·²è¶…è¿‡12GB
            warning_msg = f"è­¦å‘Š: GPUå†…å­˜ä½¿ç”¨å·²è¾ƒé«˜ ({initial_mem[0]:.2f}GB)ï¼Œå»ºè®®å…³é—­å…¶ä»–ç¨‹åºåå†è¿è¡ŒGWOä¼˜åŒ–"
            self.update_progress.emit(10, warning_msg)
            print(warning_msg)
        
        gwo = GWOOptimizer(
            n_wolves=n_wolves,
            max_iter=max_iter,
            bounds=bounds,
            objective_func=objective_func,
        )

        def callback(iter, score, params):
            # è·å–å½“å‰å†…å­˜ä½¿ç”¨
            mem_allocated, mem_reserved, mem_max = self._get_gpu_memory_info()
            mem_percent = (mem_allocated / 16.0) * 100 if torch.cuda.is_available() else 0  # å‡è®¾16GB GPU
            
            # æ ¼å¼åŒ–å‚æ•°ä¿¡æ¯
            param_info = f"embed_dim={int(params.get('embed_dim', 256))}, "
            param_info += f"heads={int(params.get('num_heads', 8))}, "
            param_info += f"layers={int(params.get('num_layers', 2))}"
            
            # æ„å»ºè¯¦ç»†çš„çŠ¶æ€ä¿¡æ¯
            status_msg = f"GWOè¿­ä»£ {iter}/{max_iter} | æœ€ä½³Dice: {score:.4f} | {param_info} | GPUå†…å­˜: {mem_allocated:.2f}GB ({mem_percent:.1f}%)"
            
            # å†…å­˜è­¦å‘Š
            if mem_percent > 90:
                status_msg += " âš ï¸âš ï¸ å†…å­˜ä¸¥é‡ä¸è¶³ï¼"
            elif mem_percent > 85:
                status_msg += " âš ï¸ å†…å­˜ä½¿ç”¨è¿‡é«˜ï¼"
            elif mem_percent > 70:
                status_msg += " âš¡ å†…å­˜ä½¿ç”¨è¾ƒé«˜"
            
            self.update_progress.emit(10 + int(80 * iter / max_iter), status_msg)

        # æ˜¾ç¤ºå¼€å§‹ä¿¡æ¯
        total_evals = n_wolves * (max_iter + 1)
        start_msg = f"å¼€å§‹GWOä¼˜åŒ–: {n_wolves}ä¸ªwolves, {max_iter}æ¬¡è¿­ä»£, å…±{total_evals}æ¬¡è¯„ä¼° | åˆå§‹å†…å­˜: {initial_mem[0]:.2f}GB"
        self.update_progress.emit(10, start_msg)
        
        best_params, best_score, history = gwo.optimize(callback=callback)
        if best_params:
            best_params["embed_dim"] = int(best_params.get("embed_dim", 256))
            best_params["num_heads"] = int(best_params.get("num_heads", 8))
            best_params["num_layers"] = int(best_params.get("num_layers", 2))
            best_params["mlp_ratio"] = float(best_params.get("mlp_ratio", 4.0))
            best_params["dropout"] = float(best_params.get("dropout", 0.1))
            if best_params["embed_dim"] % best_params["num_heads"] != 0:
                best_params["embed_dim"] = best_params["num_heads"] * max(1, best_params["embed_dim"] // best_params["num_heads"])
            best_params["_from_checkpoint"] = True
        return best_params

    def _save_matlab_viz_payload(
        self,
        images_list: List[np.ndarray],
        masks_list: List[np.ndarray],
        preds_list: List[np.ndarray],
        save_name: str
    ) -> str:
        if not images_list:
            raise ValueError("æ²¡æœ‰æ ·æœ¬å¯ç”¨äºç”ŸæˆMATLABå¯è§†åŒ–")

        payload_path = os.path.join(self.temp_dir, f"{save_name}_payload.mat")
        images_arr = np.transpose(np.stack(images_list, axis=0), (1, 2, 3, 0)).astype(np.float32)
        masks_arr = np.transpose(np.stack(masks_list, axis=0), (1, 2, 0)).astype(np.float32)
        preds_arr = np.transpose(np.stack(preds_list, axis=0), (1, 2, 0)).astype(np.float32)
        savemat(payload_path, {'images': images_arr, 'masks': masks_arr, 'preds': preds_arr})
        return payload_path

    def _save_training_history_payload(self) -> Optional[str]:
        if not self.train_loss_history or not self.val_loss_history:
            return None
        payload_path = os.path.join(self.temp_dir, "training_history_payload.mat")
        epochs = np.arange(1, len(self.train_loss_history) + 1, dtype=np.float32)
        savemat(payload_path, {
            'epochs': epochs,
            'train_loss': np.array(self.train_loss_history, dtype=np.float32),
            'val_loss': np.array(self.val_loss_history, dtype=np.float32),
            'val_dice': np.array(self.val_dice_history or [0.0] * len(epochs), dtype=np.float32)
        })
        return payload_path

    def _save_performance_payload(self, detailed_metrics: dict) -> str:
        payload_path = os.path.join(self.temp_dir, "performance_metrics_payload.mat")
        def to_array_map(source: dict) -> dict:
            return {k: np.array(source.get(k, 0.0)).astype(np.float32) for k in source}

        metrics = {k: np.array(v, dtype=np.float32) for k, v in detailed_metrics.get('all_samples', {}).items()}
        avg = to_array_map(detailed_metrics.get('average', {}))
        std = to_array_map(detailed_metrics.get('std', {}))
        min_vals = to_array_map(detailed_metrics.get('min', {}))
        max_vals = to_array_map(detailed_metrics.get('max', {}))
        median_vals = to_array_map(detailed_metrics.get('median', {}))
        savemat(payload_path, {
            'metrics': metrics,
            'avg_metrics': avg,
            'std_metrics': std,
            'min_metrics': min_vals,
            'max_metrics': max_vals,
            'median_metrics': median_vals
        })
        return payload_path

    def _save_test_results_payload(self, images_np: List[np.ndarray], masks_np: List[np.ndarray],
                                   preds_np: List[np.ndarray], metrics_list: List[dict],
                                   save_name: str) -> str:
        if not images_np:
            raise ValueError("æ²¡æœ‰æ ·æœ¬å¯ç”¨äºç”Ÿæˆæµ‹è¯•å¯è§†åŒ–")
        payload_path = os.path.join(self.temp_dir, f"{save_name}_payload.mat")
        images_arr = np.transpose(np.stack(images_np, axis=0), (1, 2, 3, 0)).astype(np.float32)
        masks_arr = np.transpose(np.stack(masks_np, axis=0), (1, 2, 0)).astype(np.float32)
        preds_arr = np.transpose(np.stack(preds_np, axis=0), (1, 2, 0)).astype(np.float32)
        dice_vals = np.array([m.get('dice', 0.0) for m in metrics_list], dtype=np.float32)
        iou_vals = np.array([m.get('iou', 0.0) for m in metrics_list], dtype=np.float32)
        savemat(payload_path, {
            'images': images_arr,
            'masks': masks_arr,
            'preds': preds_arr,
            'dice': dice_vals,
            'iou': iou_vals
        })
        return payload_path

    def _save_attention_payload(self, images_np: List[np.ndarray], masks_np: List[np.ndarray],
                                preds_np: List[np.ndarray], attention_maps: dict,
                                save_name: str) -> str:
        if not images_np:
            raise ValueError("æ²¡æœ‰æ ·æœ¬å¯ç”¨äºç”Ÿæˆæ³¨æ„åŠ›å¯è§†åŒ–")
        payload_path = os.path.join(self.temp_dir, f"{save_name}_payload.mat")
        images_arr = np.transpose(np.stack(images_np, axis=0), (1, 2, 3, 0)).astype(np.float32)
        masks_arr = np.transpose(np.stack(masks_np, axis=0), (1, 2, 0)).astype(np.float32)
        preds_arr = np.transpose(np.stack(preds_np, axis=0), (1, 2, 0)).astype(np.float32)
        payload = {
            'images': images_arr,
            'masks': masks_arr,
            'preds': preds_arr
        }
        for key, maps in attention_maps.items():
            if not maps:
                continue
            payload[key] = np.transpose(np.stack(maps, axis=0), (1, 2, 0)).astype(np.float32)
        savemat(payload_path, payload)
        return payload_path

    def _safe_dice_score(self, pred, target, eps: float = 1e-7) -> float:
        """
        è®¡ç®—Diceç³»æ•°,å¯¹ç©ºé¢„æµ‹å’Œç©ºç›®æ ‡è¿›è¡Œå®‰å…¨å¤„ç†ã€‚
        
        å¤„ç†ç­–ç•¥:
        - å½“ç›®æ ‡ä¸ºç©ºä¸”é¢„æµ‹ä¹Ÿä¸ºç©º: Dice = 1.0 (å®Œç¾åŒ¹é…)
        - å½“ç›®æ ‡ä¸ºç©ºä½†é¢„æµ‹æœ‰è¯¯æ£€: ä½¿ç”¨ç›¸å¯¹è¯¯å·®å…¬å¼,é¿å…è¿‡åº¦æƒ©ç½š
        - å½“é¢„æµ‹ä¸ºç©ºä½†ç›®æ ‡æœ‰å‰æ™¯: Dice = 0.0 (å®Œå…¨æ¼æ£€)
        - æ­£å¸¸æƒ…å†µ: ä½¿ç”¨æ ‡å‡†Diceå…¬å¼
        """
        if isinstance(pred, torch.Tensor):
            # ç¡®ä¿ pred å’Œ target çš„ç©ºé—´å°ºå¯¸åŒ¹é…
            if pred.shape != target.shape:
                if pred.dim() >= 2 and target.dim() >= 2:
                    if pred.shape[-2:] != target.shape[-2:]:
                        # å°† pred è°ƒæ•´åˆ° target çš„å°ºå¯¸
                        if pred.dim() == 2:
                            pred = pred.unsqueeze(0).unsqueeze(0)
                        elif pred.dim() == 3:
                            pred = pred.unsqueeze(0)
                        if target.dim() == 2:
                            target = target.unsqueeze(0).unsqueeze(0)
                        elif target.dim() == 3:
                            target = target.unsqueeze(0)
                        pred = F.interpolate(pred, size=target.shape[2:], mode='bilinear', align_corners=False)
                        if pred.dim() == 4 and pred.size(0) == 1:
                            pred = pred.squeeze(0)
                        if target.dim() == 4 and target.size(0) == 1:
                            target = target.squeeze(0)
            inter = float((pred * target).sum().item())
            pred_sum = float(pred.sum().item())
            target_sum = float(target.sum().item())
            total_pixels = pred.numel()
        else:
            # NumPy æ•°ç»„å¤„ç†
            if pred.shape != target.shape:
                # ä½¿ç”¨ scipy æˆ– PIL è¿›è¡Œ resize
                from scipy.ndimage import zoom
                if len(pred.shape) == 2 and len(target.shape) == 2:
                    zoom_factors = (target.shape[0] / pred.shape[0], target.shape[1] / pred.shape[1])
                    pred = zoom(pred, zoom_factors, order=1)
            inter = float(np.sum(pred * target))
            pred_sum = float(np.sum(pred))
            target_sum = float(np.sum(target))
            total_pixels = pred.size if hasattr(pred, 'size') else np.prod(pred.shape)
        
        # Case 1: ç›®æ ‡ä¸ºç©º
        if target_sum <= eps:
            if pred_sum <= eps:
                return 1.0  # é¢„æµ‹ä¹Ÿä¸ºç©º,å®Œç¾åŒ¹é…
            else:
                # é¢„æµ‹æœ‰è¯¯æ£€,è®¡ç®—ç›¸å¯¹æƒ©ç½š
                # åŸºäºè¯¯æ£€åƒç´ å æ€»åƒç´ çš„æ¯”ä¾‹
                false_positive_ratio = pred_sum / total_pixels
                # ä½¿ç”¨çº¿æ€§æƒ©ç½š: Dice = 1 - 2Ã—è¯¯æ£€ç‡
                # ä¾‹å¦‚: 1%è¯¯æ£€ -> 0.98, 5%è¯¯æ£€ -> 0.90, 10%è¯¯æ£€ -> 0.80
                return max(0.0, 1.0 - 2.0 * false_positive_ratio)
        
        # Case 2: é¢„æµ‹ä¸ºç©ºä½†ç›®æ ‡æœ‰å‰æ™¯
        if pred_sum <= eps:
            return 0.0  # å®Œå…¨æ¼æ£€
        
        # Case 3: æ­£å¸¸æƒ…å†µ,ä½¿ç”¨æ ‡å‡†Dice
        denom = pred_sum + target_sum
        return (2.0 * inter + eps) / (denom + eps)
    def calculate_hd95(self, pred, gt):
        """
        è®¡ç®— Hausdorff Distance 95 (HD95)
        è¡¡é‡é¢„æµ‹è¾¹ç•Œä¸çœŸå®è¾¹ç•Œçš„é‡åˆåº¦ï¼Œå•ä½ï¼šåƒç´ 
        """
        import numpy as np
        from scipy.ndimage import binary_erosion, distance_transform_edt
        
        try:
            # ç¡®ä¿è¾“å…¥æ˜¯ bool ç±»å‹
            if pred.dtype != bool:
                pred = (pred > 0.5).astype(bool)
            if gt.dtype != bool:
                gt = (gt > 0.5).astype(bool)
            
            # å¦‚æœå…¨æ˜¯é»‘çš„ï¼ˆæ²¡æœ‰é¢„æµ‹æˆ–æ²¡æœ‰çœŸå€¼ï¼‰ï¼Œç›´æ¥è¿”å›é»˜è®¤å€¼
            if not pred.any() or not gt.any():
                # å¦‚æœéƒ½æ²¡ç—…ç¶ï¼Œè·ç¦»ä¸º0ï¼›å¦‚æœä¸€ä¸ªæœ‰ä¸€ä¸ªæ²¡ï¼Œè·ç¦»æ— ç©·å¤§(ç”¨99.9ä»£æ›¿)
                return 0.0 if (not pred.any() and not gt.any()) else 99.9
            
            # æå–è¾¹ç•Œ
            structure = np.ones((3, 3), dtype=bool)
            pred_border = np.logical_xor(pred, binary_erosion(pred, structure))
            gt_border = np.logical_xor(gt, binary_erosion(gt, structure))
            
            # å¦‚æœè¾¹ç•Œæå–å¤±è´¥ï¼ˆæ¯”å¦‚åªæœ‰ä¸€ä¸ªåƒç´ ï¼‰ï¼Œå›é€€åˆ°åŸå›¾
            if not pred_border.any(): pred_border = pred
            if not gt_border.any(): gt_border = gt
            
            # è®¡ç®—è·ç¦»å˜æ¢ (Distance Transform)
            # dt[i] è¡¨ç¤ºåƒç´  i åˆ°æœ€è¿‘èƒŒæ™¯åƒç´ çš„è·ç¦»
            # æˆ‘ä»¬éœ€è¦çš„æ˜¯ï¼šé¢„æµ‹è¾¹ç•Œä¸Šçš„ç‚¹ -> åˆ° -> çœŸå®è¾¹ç•Œ çš„æœ€è¿‘è·ç¦»
            gt_dt = distance_transform_edt(~gt_border)
            pred_dt = distance_transform_edt(~pred_border)
            
            # åŒå‘è·ç¦»
            d1 = gt_dt[pred_border] # é¢„æµ‹è¾¹ç•Œç‚¹ åˆ° çœŸå®è¾¹ç•Œ çš„è·ç¦»
            d2 = pred_dt[gt_border] # çœŸå®è¾¹ç•Œç‚¹ åˆ° é¢„æµ‹è¾¹ç•Œ çš„è·ç¦»
            
            all_distances = np.concatenate([d1, d2])
            
            if all_distances.size == 0:
                return 0.0
            
            # å–ç¬¬ 95 ç™¾åˆ†ä½è·ç¦»ï¼Œæ’é™¤ç¦»ç¾¤ç‚¹å¹²æ‰°
            hd95 = np.percentile(all_distances, 95)
            return float(hd95)
            
        except Exception as e:
            print(f"[Warning] HD95 è®¡ç®—å¤±è´¥: {e}")
            return 99.9 
    def calculate_dice(self, pred, target, smooth=1e-7):
        """è®¡ç®—å•ä¸ªæ ·æœ¬çš„Diceç³»æ•°"""
        if isinstance(pred, torch.Tensor):
            pred_tensor = pred.float()
            target_tensor = target.float()
        else:
            pred_tensor = torch.from_numpy(pred).float()
            target_tensor = torch.from_numpy(target).float()
        
        # ç¡®ä¿ pred å’Œ target çš„ç©ºé—´å°ºå¯¸åŒ¹é…
        if pred_tensor.dim() >= 2 and target_tensor.dim() >= 2:
            if pred_tensor.shape[-2:] != target_tensor.shape[-2:]:
                # å°† pred è°ƒæ•´åˆ° target çš„å°ºå¯¸
                if pred_tensor.dim() == 2:
                    pred_tensor = pred_tensor.unsqueeze(0).unsqueeze(0)
                elif pred_tensor.dim() == 3:
                    pred_tensor = pred_tensor.unsqueeze(0)
                if target_tensor.dim() == 2:
                    target_tensor = target_tensor.unsqueeze(0).unsqueeze(0)
                elif target_tensor.dim() == 3:
                    target_tensor = target_tensor.unsqueeze(0)
                pred_tensor = F.interpolate(pred_tensor, size=target_tensor.shape[2:], mode='bilinear', align_corners=False)
                if pred_tensor.dim() == 4 and pred_tensor.size(0) == 1:
                    pred_tensor = pred_tensor.squeeze(0)
                if target_tensor.dim() == 4 and target_tensor.size(0) == 1:
                    target_tensor = target_tensor.squeeze(0)
        
        if pred_tensor.dim() > 2:
            pred_tensor = pred_tensor.view(1, -1)
            target_tensor = target_tensor.view(1, -1)
        else:
            pred_tensor = pred_tensor.view(1, -1)
            target_tensor = target_tensor.view(1, -1)
        
        intersection = (pred_tensor * target_tensor).sum()
        return (2. * intersection + smooth) / (pred_tensor.sum() + target_tensor.sum() + smooth)

    def calculate_batch_dice(self, pred, target, smooth=1e-7):
        """
        è®¡ç®—ä¸€ä¸ªæ‰¹æ¬¡ä¸­æ¯ä¸ªæ ·æœ¬çš„Diceç³»æ•°ã€‚
        å¯¹ç©ºmaskæƒ…å†µè¿›è¡Œç‰¹æ®Šå¤„ç†,é¿å…è¿‡åº¦æƒ©ç½šå°‘é‡è¯¯æ£€ã€‚
        """
        # ç¡®ä¿ pred å’Œ target çš„ç©ºé—´å°ºå¯¸åŒ¹é…
        if pred.shape[2:] != target.shape[2:]:
            # å°† pred è°ƒæ•´åˆ° target çš„å°ºå¯¸ï¼ˆå› ä¸º target æ˜¯ ground truthï¼‰
            pred = F.interpolate(pred, size=target.shape[2:], mode='bilinear', align_corners=False)
        
        if pred.dim() == 3:
            pred = pred.unsqueeze(1)
        if target.dim() == 3:
            target = target.unsqueeze(1)
        
        pred_flat = pred.view(pred.size(0), -1).float()
        target_flat = target.view(target.size(0), -1).float()
        
        batch_size = pred.size(0)
        total_pixels = pred_flat.size(1)
        avg_fg_ratio = float(target_flat.sum() / max(1.0, batch_size * total_pixels))
        # ã€ä¿®å¤ã€‘é™ä½ç©ºmaské˜ˆå€¼ï¼Œä»0.015æ”¹ä¸º0.001ï¼Œé¿å…å°†å°‘é‡å‰æ™¯åƒç´ è¯¯åˆ¤ä¸ºç©ºmask
        # å¯¹äº256x256å›¾åƒï¼Œé˜ˆå€¼ä»9.8åƒç´ é™ä½åˆ°0.65åƒç´ ï¼Œæ›´ä¸¥æ ¼
        adaptive_empty_threshold = max(smooth, avg_fg_ratio * 0.001)
        dice_scores = []
        
        for i in range(batch_size):
            pred_i = pred_flat[i]
            target_i = target_flat[i]
            
            intersection = (pred_i * target_i).sum()
            pred_sum = pred_i.sum()
            target_sum = target_i.sum()
            
            # Case 1: ç›®æ ‡ä¸ºç©ºï¼ˆçœŸæ­£çš„ç©ºmaskï¼Œæ— ç—…å˜ï¼‰
            if target_sum <= adaptive_empty_threshold:
                if pred_sum <= smooth:
                    # ã€ä¿®æ”¹ã€‘å…¨é˜´æ€§æƒ…å†µï¼šé¢„æµ‹ä¹Ÿä¸ºç©ºæ—¶ï¼Œç»™äºˆå®Œå…¨æ­£ç¡®çš„é˜´æ€§é¢„æµ‹æ»¡åˆ†å¥–åŠ±
                    # å¦‚æœé¢„æµ‹ä¹Ÿä¸ºç©ºï¼ŒDice = 1.0ï¼ˆå®Œå…¨æ­£ç¡®ï¼‰
                    dice = 1.0
                else:
                    # è¯¯æ£€æƒ©ç½šï¼šç›®æ ‡ä¸ºç©ºä½†é¢„æµ‹æœ‰å‰æ™¯
                    false_positive_ratio = pred_sum.item() / max(1.0, total_pixels)
                    dice = max(0.0, 1.0 - 1.5 * false_positive_ratio)
            # Case 2: é¢„æµ‹ä¸ºç©ºä½†ç›®æ ‡æœ‰å‰æ™¯
            elif pred_sum <= smooth:
                dice = 0.0
            # Case 3: æ­£å¸¸æƒ…å†µï¼ˆæœ‰ç—…å˜æ ·æœ¬ï¼‰
            else:
                dice = (2. * intersection + smooth) / (pred_sum + target_sum + smooth)
            
            dice_scores.append(dice)
        
        return torch.tensor(dice_scores, device=pred.device)

    def dice_loss(self, logits, targets, smooth=1e-7):
        """
        ç”¨äºè®­ç»ƒçš„Dice Lossï¼ˆæ•°å€¼ç¨³å®šç‰ˆæœ¬ï¼‰ã€‚
        logits: æ¨¡å‹åŸå§‹è¾“å‡º (æœªç»è¿‡sigmoid)
        targets: [0,1] æ©è†œ
        
        æ³¨æ„: è®­ç»ƒæ—¶çš„lossè®¡ç®—ä¿æŒæ ‡å‡†å…¬å¼,ä¸å¯¹ç©ºmaskè¿›è¡Œç‰¹æ®Šå®½å®¹å¤„ç†,
        è¿™æ ·æ‰èƒ½è®©æ¨¡å‹å­¦ä¹ åˆ°æ­£ç¡®çš„é¢„æµ‹è¡Œä¸ºã€‚
        """
        probs = torch.sigmoid(logits)
        # ç¡®ä¿ probs å’Œ targets çš„ç©ºé—´å°ºå¯¸åŒ¹é…
        if probs.shape[2:] != targets.shape[2:]:
            # å°† probs è°ƒæ•´åˆ° targets çš„å°ºå¯¸ï¼ˆå› ä¸º targets æ˜¯ ground truthï¼‰
            probs = F.interpolate(probs, size=targets.shape[2:], mode='bilinear', align_corners=False)
        probs = probs.view(probs.size(0), -1)
        targets = targets.view(targets.size(0), -1)
        intersection = (probs * targets).sum(dim=1)
        denominator = probs.sum(dim=1) + targets.sum(dim=1) + smooth
        # æ£€æŸ¥åˆ†æ¯æ˜¯å¦ä¸ºé›¶æˆ–è¿‡å°
        denominator = torch.clamp(denominator, min=smooth)
        dice = (2. * intersection + smooth) / denominator
        dice = torch.clamp(dice, min=0.0, max=1.0)
        loss = 1 - dice.mean()
        # æ£€æŸ¥NaN/Inf
        if not torch.isfinite(loss):
            loss = torch.tensor(0.0, device=logits.device)
        return loss

    def focal_loss(self, logits, targets, alpha=0.25, gamma=2.0):
        """å¸®åŠ©ç¼“è§£æ ·æœ¬ä¸å¹³è¡¡çš„Focal Lossï¼ˆæ•°å€¼ç¨³å®šç‰ˆæœ¬ï¼‰"""
        # ç¡®ä¿ logits å’Œ targets çš„ç©ºé—´å°ºå¯¸åŒ¹é…
        if logits.shape[2:] != targets.shape[2:]:
            logits = F.interpolate(logits, size=targets.shape[2:], mode='bilinear', align_corners=False)
        bce = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')
        # ä½¿ç”¨clampé˜²æ­¢expæº¢å‡º
        bce_clamped = torch.clamp(bce, min=-50.0, max=50.0)
        pt = torch.exp(-bce_clamped)
        # ä½¿ç”¨clampé˜²æ­¢æ•°å€¼ä¸ç¨³å®š
        pt = torch.clamp(pt, min=1e-7, max=1.0-1e-7)
        focal = alpha * (1 - pt) ** gamma * bce
        # æ£€æŸ¥NaN/Inf
        focal = torch.where(torch.isfinite(focal), focal, torch.zeros_like(focal))
        return focal.mean()

    def tversky_loss(self, logits, targets, alpha=0.1, beta=0.9, smooth=1e-7):
        """
        Tversky Losså¯¹å¬å›/ç²¾ç¡®è¿›è¡ŒåŠ æƒï¼Œæå‡Diceè¡¨ç°ï¼ˆæ•°å€¼ç¨³å®šç‰ˆæœ¬ï¼‰
        
        å‚æ•°è¯´æ˜ï¼š
        - alpha: å‡é˜³æ€§(FP)çš„æƒé‡ï¼Œé»˜è®¤0.1
        - beta: å‡é˜´æ€§(FN/æ¼æŠ¥)çš„æƒé‡ï¼Œé»˜è®¤0.9
        - å½“beta=0.9, alpha=0.1æ—¶ï¼Œæ¼æŠ¥ä¸€ä¸ªåƒç´ çš„æƒ©ç½šæ˜¯å¤šæŠ¥ä¸€ä¸ªåƒç´ æƒ©ç½šçš„9å€
        - è¿™æœ‰åŠ©äºå‡å°‘æ¼æ£€ï¼Œæé«˜å¬å›ç‡ï¼Œç‰¹åˆ«é€‚åˆåŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡
        """
        # ç¡®ä¿ logits å’Œ targets çš„ç©ºé—´å°ºå¯¸åŒ¹é…
        if logits.shape[2:] != targets.shape[2:]:
            logits = F.interpolate(logits, size=targets.shape[2:], mode='bilinear', align_corners=False)
        probs = torch.sigmoid(logits)
        probs = probs.view(probs.size(0), -1)
        targets = targets.view(targets.size(0), -1)

        true_pos = (probs * targets).sum(dim=1)
        false_pos = (probs * (1 - targets)).sum(dim=1)
        false_neg = ((1 - probs) * targets).sum(dim=1)

        denominator = true_pos + alpha * false_pos + beta * false_neg + smooth
        # æ£€æŸ¥åˆ†æ¯æ˜¯å¦ä¸ºé›¶æˆ–è¿‡å°
        denominator = torch.clamp(denominator, min=smooth)
        tversky = (true_pos + smooth) / denominator
        tversky = torch.clamp(tversky, min=0.0, max=1.0)
        loss = 1 - tversky.mean()
        # æ£€æŸ¥NaN/Inf
        if not torch.isfinite(loss):
            loss = torch.tensor(0.0, device=logits.device)
        return loss
    
    def tversky_focal_loss(self, logits, targets, alpha=0.1, beta=0.9, gamma=0.75, smooth=1e-7):
        """
        Focal Tversky Loss: åœ¨Tversky LossåŸºç¡€ä¸Šè¿›ä¸€æ­¥å¼ºè°ƒéš¾åˆ†æ ·æœ¬ï¼Œ
        å¯¹äºDiceéš¾ä»¥æå‡çš„åŒºåŸŸæ›´æ•æ„Ÿï¼Œå¯æœ‰æ•ˆæ”¹å–„å°‘é‡æ¼æ£€é€ æˆçš„Diceä¸‹é™ã€‚
        ï¼ˆæ•°å€¼ç¨³å®šç‰ˆæœ¬ï¼‰
        
        å‚æ•°è¯´æ˜ï¼š
        - alpha: å‡é˜³æ€§(FP)çš„æƒé‡ï¼Œé»˜è®¤0.1
        - beta: å‡é˜´æ€§(FN/æ¼æŠ¥)çš„æƒé‡ï¼Œé»˜è®¤0.9
        - å½“beta=0.9, alpha=0.1æ—¶ï¼Œæ¼æŠ¥ä¸€ä¸ªåƒç´ çš„æƒ©ç½šæ˜¯å¤šæŠ¥ä¸€ä¸ªåƒç´ æƒ©ç½šçš„9å€
        """
        tversky_val = 1.0 - self.tversky_loss(logits, targets, alpha=alpha, beta=beta, smooth=smooth)
        # ç¡®ä¿tversky_valåœ¨åˆç†èŒƒå›´å†…ï¼Œé˜²æ­¢powæº¢å‡º
        tversky_val = torch.clamp(tversky_val, min=1e-7, max=1.0-1e-7)
        focal_term = torch.pow((1.0 - tversky_val), gamma)
        # æ£€æŸ¥NaN/Inf
        focal_term = torch.where(torch.isfinite(focal_term), focal_term, torch.zeros_like(focal_term))
        return focal_term.mean()

    def edge_loss(self, logits, targets):
        """å¼ºè°ƒç›®æ ‡è¾¹ç•Œçš„æ‹‰æ™®æ‹‰æ–¯è¾¹ç¼˜æŸå¤±"""
        # ç¡®ä¿ logits å’Œ targets çš„ç©ºé—´å°ºå¯¸åŒ¹é…
        if logits.shape[2:] != targets.shape[2:]:
            logits = F.interpolate(logits, size=targets.shape[2:], mode='bilinear', align_corners=False)
        probs = torch.sigmoid(logits)
        kernel = logits.new_tensor([[1, 1, 1],
                                    [1, -8, 1],
                                    [1, 1, 1]]).unsqueeze(0).unsqueeze(0)
        preds_edge = F.conv2d(probs, kernel, padding=1)
        target_edge = F.conv2d(targets.float(), kernel, padding=1)
        return F.l1_loss(preds_edge, target_edge)
    
    def hausdorff_distance_loss(self, logits, targets, percentile=95, alpha=1.0):
        """
        Hausdorff Distance Loss - ç›´æ¥ä¼˜åŒ–è¾¹ç•Œè·ç¦»
        
        é€šè¿‡è®¡ç®—é¢„æµ‹è¾¹ç•Œå’ŒçœŸå®è¾¹ç•Œä¹‹é—´çš„Hausdorffè·ç¦»æ¥ä¼˜åŒ–åˆ†å‰²ç²¾åº¦ï¼Œ
        ç‰¹åˆ«é€‚ç”¨äºè¾¹ç•Œæ¨¡ç³Šçš„åŒ»å­¦å½±åƒåˆ†å‰²ä»»åŠ¡ã€‚
        
        Args:
            logits: æ¨¡å‹è¾“å‡ºlogits (B, 1, H, W)
            targets: çœŸå®æ©è†œ (B, 1, H, W)
            percentile: ä½¿ç”¨ç™¾åˆ†ä½æ•°è€Œéæœ€å¤§å€¼ï¼Œæ›´ç¨³å®š (é»˜è®¤95)
            alpha: è·ç¦»å˜æ¢çš„ç¼©æ”¾å› å­
        """
        # ç¡®ä¿ logits å’Œ targets çš„ç©ºé—´å°ºå¯¸åŒ¹é…
        if logits.shape[2:] != targets.shape[2:]:
            logits = F.interpolate(logits, size=targets.shape[2:], mode='bilinear', align_corners=False)
        probs = torch.sigmoid(logits)
        B, C, H, W = probs.shape
        
        # äºŒå€¼åŒ–é¢„æµ‹å’ŒçœŸå®æ©è†œ
        pred_binary = (probs > 0.5).float()
        target_binary = targets.float()
        
        total_loss = 0.0
        valid_samples = 0
        
        for b in range(B):
            pred_mask = pred_binary[b, 0].cpu().numpy()
            target_mask = target_binary[b, 0].cpu().numpy()
            
            # è®¡ç®—è·ç¦»å˜æ¢
            # å¯¹äºé¢„æµ‹è¾¹ç•Œåˆ°çœŸå®è¾¹ç•Œçš„è·ç¦»
            if pred_mask.sum() > 0 and target_mask.sum() > 0:
                # è®¡ç®—é¢„æµ‹è¾¹ç•Œåˆ°æœ€è¿‘çœŸå®è¾¹ç•Œçš„è·ç¦»
                pred_boundary = pred_mask - binary_erosion(pred_mask.astype(np.uint8), iterations=1).astype(np.float32)
                if pred_boundary.sum() > 0:
                    dist_pred_to_target = distance_transform_edt(1 - target_mask)
                    dist_pred = dist_pred_to_target[pred_boundary > 0]
                    if len(dist_pred) > 0:
                        hd_pred = np.percentile(dist_pred, percentile)
                    else:
                        hd_pred = 0.0
                else:
                    hd_pred = 0.0
                
                # è®¡ç®—çœŸå®è¾¹ç•Œåˆ°æœ€è¿‘é¢„æµ‹è¾¹ç•Œçš„è·ç¦»
                target_boundary = target_mask - binary_erosion(target_mask.astype(np.uint8), iterations=1).astype(np.float32)
                if target_boundary.sum() > 0:
                    dist_target_to_pred = distance_transform_edt(1 - pred_mask)
                    dist_target = dist_target_to_pred[target_boundary > 0]
                    if len(dist_target) > 0:
                        hd_target = np.percentile(dist_target, percentile)
                    else:
                        hd_target = 0.0
                else:
                    hd_target = 0.0
                
                # Hausdorffè·ç¦»æ˜¯åŒå‘è·ç¦»çš„æœ€å¤§å€¼
                hd = max(hd_pred, hd_target)
                total_loss += hd * alpha
                valid_samples += 1
        
        if valid_samples > 0:
            loss = torch.tensor(total_loss / valid_samples, device=logits.device, dtype=logits.dtype)
        else:
            loss = torch.tensor(0.0, device=logits.device, dtype=logits.dtype)
        
        return loss
    
    def lovasz_hinge_loss(self, logits, targets):
        """
        Lovasz-HingeæŸå¤± - ç›´æ¥ä¼˜åŒ–IoU/Diceï¼ˆæ•°å€¼ç¨³å®šç‰ˆæœ¬ï¼‰
        
        LovaszæŸå¤±æ˜¯IoU lossçš„å‡¸ä»£ç†,æ¯”æ ‡å‡†Dice lossæ›´æœ‰æ•ˆ
        å‚è€ƒ: "The LovÃ¡sz-Softmax loss" (CVPR 2018)
        """
        probs = torch.sigmoid(logits)
        probs_flat = probs.view(-1)
        targets_flat = targets.view(-1)
        
        # è®¡ç®—è¯¯å·®ï¼ˆä½¿ç”¨hinge lossçš„å½¢å¼ï¼‰
        # errors = max(0, 1 - (2*probs - 1) * (2*targets - 1))
        # å¯¹äºäºŒåˆ†ç±»ï¼šå¦‚æœé¢„æµ‹æ­£ç¡®ï¼Œerroræ¥è¿‘0ï¼›å¦‚æœé¢„æµ‹é”™è¯¯ï¼Œerroræ¥è¿‘1
        errors = torch.clamp(1.0 - (2 * probs_flat - 1) * (2 * targets_flat - 1), min=0.0)
        errors_sorted, indices = torch.sort(errors, descending=True)
        targets_sorted = targets_flat[indices]
        
        # Lovasz extension - ä¿®å¤è®¡ç®—ï¼Œç¡®ä¿éè´Ÿ
        n = len(targets_sorted)
        if n == 0:
            return torch.tensor(0.0, device=logits.device)
        
        # è®¡ç®—IoUçš„Lovaszæ‰©å±•
        # å¯¹äºæ¯ä¸ªä½ç½®ï¼Œè®¡ç®—ç´¯ç§¯çš„intersectionå’Œunion
        tp = targets_sorted.sum()  # æ€»çš„æ­£æ ·æœ¬æ•°
        fp = (1 - targets_sorted).sum()  # æ€»çš„è´Ÿæ ·æœ¬æ•°
        
        # è®¡ç®—ç´¯ç§¯çš„intersectionå’Œunion
        tp_cumsum = targets_sorted.cumsum(0)
        fp_cumsum = (1 - targets_sorted).cumsum(0)
        
        # è®¡ç®—IoU (Jaccard) - å¢åŠ æ•°å€¼ç¨³å®šæ€§
        intersection = tp - tp_cumsum
        union = tp + fp - intersection
        # ä½¿ç”¨æ›´å¤§çš„epsilonå¹¶æ£€æŸ¥é™¤é›¶
        union = torch.clamp(union, min=1e-6)
        jaccard = intersection / union
        jaccard = torch.clamp(jaccard, min=0.0, max=1.0)
        
        # æ£€æŸ¥NaN/Inf
        jaccard = torch.where(torch.isfinite(jaccard), jaccard, torch.zeros_like(jaccard))
        
        # è®¡ç®—Lovaszæ‰©å±•çš„æ¢¯åº¦æƒé‡ï¼ˆå·®åˆ†å½¢å¼ï¼‰
        if n > 1:
            jaccard_diff = torch.zeros_like(jaccard)
            jaccard_diff[0] = jaccard[0]
            jaccard_diff[1:] = jaccard[1:] - jaccard[:-1]
            jaccard = jaccard_diff
        
        # è®¡ç®—æŸå¤±ï¼ˆç¡®ä¿éè´Ÿå’ŒéNaNï¼‰
        loss = torch.dot(errors_sorted, jaccard)
        loss = torch.clamp(loss, min=0.0)  # ç¡®ä¿æŸå¤±éè´Ÿ
        # æœ€ç»ˆæ£€æŸ¥NaN/Inf
        if not torch.isfinite(loss):
            loss = torch.tensor(0.0, device=logits.device)
        return loss
    
    def attention_concentration_loss(self, attention_maps, masks, weight=0.01):
        """
        æ³¨æ„åŠ›é›†ä¸­åº¦æŸå¤± - é¼“åŠ±æ³¨æ„åŠ›èšç„¦åœ¨ç—…ç¶åŒºåŸŸ
        
        åŸç†:
        1. è®¡ç®—æ³¨æ„åŠ›å›¾çš„ç†µ(entropy) - ç†µè¶Šä½è¶Šé›†ä¸­
        2. è®¡ç®—æ³¨æ„åŠ›å›¾ä¸maskçš„å¯¹é½åº¦ - é¼“åŠ±æ³¨æ„åŠ›å…³æ³¨ç—…ç¶åŒºåŸŸ
        
        å‚æ•°:
        - attention_maps: dict of attention maps from different layers
        - masks: ground truth masks
        - weight: lossæƒé‡
        """
        if not attention_maps:
            return 0.0
        
        total_loss = 0.0
        num_maps = 0
        
        for key, att_map in attention_maps.items():
            if att_map is None:
                continue
            
            # Resize mask to match attention map size
            B, _, H, W = att_map.shape
            mask_resized = F.interpolate(masks, size=(H, W), mode='bilinear', align_corners=False)
            
            # Loss 1: ç†µæŸå¤± - é¼“åŠ±æ³¨æ„åŠ›åˆ†å¸ƒæ›´å°–é”(ä½ç†µ)
            # åªåœ¨æœ‰ç—…ç¶çš„æ ·æœ¬ä¸Šè®¡ç®—,é¿å…ç©ºmaskå¯¼è‡´é—®é¢˜
            has_foreground = mask_resized.sum(dim=[1, 2, 3]) > 1e-3
            if has_foreground.any():
                att_fg = att_map[has_foreground]
                # æ·»åŠ å°çš„epsiloné¿å…log(0)
                # æ³¨æ„: ä½¿ç”¨.clampé¿å…autocastä¸‹çš„æ•°å€¼é—®é¢˜
                att_clamped = att_fg.clamp(min=1e-7, max=1.0-1e-7)
                entropy = -(att_clamped * torch.log(att_clamped) + 
                           (1 - att_clamped) * torch.log(1 - att_clamped)).mean()
                
                # Loss 2: å¯¹é½æŸå¤± - ä½¿ç”¨MSEæ›¿ä»£BCE (autocastå®‰å…¨)
                # æˆ–è€…ä½¿ç”¨L1 loss,æ•ˆæœç±»ä¼¼ä½†æ›´ç¨³å®š
                mask_fg = mask_resized[has_foreground]
                alignment_loss = F.mse_loss(att_fg, mask_fg, reduction='mean')
                
                total_loss += entropy * 0.1 + alignment_loss
                num_maps += 1
        
        if num_maps == 0:
            return 0.0
        
        return weight * (total_loss / num_maps)

    def compute_seg_loss(self, logits, masks, bce_criterion, use_lovasz=True, weights=None):
        """
        ç»„åˆå¤šç§æŸå¤±å‡½æ•° - ä¼˜åŒ–ç‰ˆ
        
        Args:
            use_lovasz: æ˜¯å¦ä½¿ç”¨LovaszæŸå¤±(æ¨è,å¯æå‡Dice)
        """
        # ç¡®ä¿ logits å’Œ masks çš„ç©ºé—´å°ºå¯¸åŒ¹é…
        if logits.shape[2:] != masks.shape[2:]:
            # å°† logits è°ƒæ•´åˆ° masks çš„å°ºå¯¸ï¼ˆå› ä¸º masks æ˜¯ ground truthï¼‰
            logits = F.interpolate(logits, size=masks.shape[2:], mode='bilinear', align_corners=False)
        
        bce_loss = bce_criterion(logits, masks)
        probs = torch.sigmoid(logits)
        dice_loss_val = self.dice_loss(logits, masks)
        focal_loss_val = self.focal_loss(logits, masks)
        boundary_loss = self.edge_loss(logits, masks)
        # Tversky Loss: æ¼æŠ¥(FN)çš„æƒ©ç½šæ˜¯å¤šæŠ¥(FP)æƒ©ç½šçš„çº¦2.3å€ (alpha=0.3, beta=0.7)
        # åŠ å¤§å¯¹FNçš„æƒ©ç½šï¼Œå¼ºè¿«æ¨¡å‹è¯†åˆ«å¾®å°ç—…ç¶åŒºåŸŸ
        tversky_loss_val = self.tversky_loss(logits, masks, alpha=0.3, beta=0.7)
        # Focal Tversky Loss: è¿›ä¸€æ­¥å¼ºè°ƒéš¾åˆ†æ ·æœ¬ï¼Œä½¿ç”¨ä¸ä¸»Tversky Lossç›¸åŒçš„å‚æ•°
        tversky_focal_loss_val = self.tversky_focal_loss(logits, masks, alpha=0.3, beta=0.7, gamma=0.8)
        # å‡é˜´æ€§æƒ©ç½šï¼šåº”è¯¥æœ‰ç—…å˜ä½†é¢„æµ‹ä¸ºæ— ç—…å˜
        false_negative_penalty = ((1 - probs) * masks).mean()
        # å‡é˜³æ€§æƒ©ç½šï¼šåº”è¯¥æ— ç—…å˜ä½†é¢„æµ‹ä¸ºæœ‰ç—…å˜ï¼ˆä½¿ç”¨clampç¡®ä¿éè´Ÿï¼‰
        false_positive_penalty = (probs.clamp(min=0.0, max=1.0) ** 2.0 * (1 - masks)).mean()
        
        loss_weights = {
            'bce': 0.20,
            'dice': 0.25,
            'tversky': 0.35,  # å¢åŠ Tversky Lossæƒé‡ï¼Œä½œä¸ºä¸»è¦æŸå¤±å‡½æ•°
            'tversky_focal': 0.05,
            'boundary': 0.05,  # æå‡è¾¹ç•Œæƒé‡
            'hausdorff': 0.05,  # é»˜è®¤å¼€å¯å°æƒé‡çš„Hausdorffï¼Œå…³æ³¨è½®å»“
            'focal': 0.03,
            'lovasz': 0.0,
            'fn_penalty': 0.03,
            'fp_penalty': 0.02,
        }
        if use_lovasz:
            loss_weights['lovasz'] = 0.10
            loss_weights['bce'] = 0.15
            loss_weights['dice'] = 0.20
            loss_weights['tversky'] = 0.35  # ä¿æŒTverskyä¸ºä¸»è¦æŸå¤±
        if weights:
            loss_weights.update(weights)
        
        combined_loss = (
            loss_weights['bce'] * bce_loss
            + loss_weights['dice'] * dice_loss_val
            + loss_weights['tversky'] * tversky_loss_val
            + loss_weights['tversky_focal'] * tversky_focal_loss_val
            + loss_weights['boundary'] * boundary_loss
            + loss_weights['focal'] * focal_loss_val
            + loss_weights['fn_penalty'] * false_negative_penalty
            + loss_weights['fp_penalty'] * false_positive_penalty
            + loss_weights.get('hausdorff', 0.0) * torch.tensor(0.0, device=logits.device)  # é¢„ç•™Hausdorffé¡¹
        )
        if use_lovasz and loss_weights.get('lovasz', 0) > 0:
            lovasz_loss_val = self.lovasz_hinge_loss(logits, masks)
            combined_loss += loss_weights['lovasz'] * lovasz_loss_val
        
        # æ£€æŸ¥æ¯ä¸ªæŸå¤±ç»„ä»¶æ˜¯å¦æœ‰NaN/Inf
        loss_components = {
            'bce': bce_loss,
            'dice': dice_loss_val,
            'tversky': tversky_loss_val,
            'tversky_focal': tversky_focal_loss_val,
            'boundary': boundary_loss,
            'focal': focal_loss_val,
            'fn_penalty': false_negative_penalty,
            'fp_penalty': false_positive_penalty,
            'hausdorff': torch.tensor(0.0, device=logits.device),
        }
        if use_lovasz and loss_weights.get('lovasz', 0) > 0:
            loss_components['lovasz'] = lovasz_loss_val
        
        # æ›¿æ¢NaN/Infçš„æŸå¤±ç»„ä»¶ä¸º0
        for key, loss_val in loss_components.items():
            if not torch.isfinite(loss_val):
                print(f"[è­¦å‘Š] {key}æŸå¤±å‡ºç°NaN/Infï¼Œå·²æ›¿æ¢ä¸º0")
                loss_components[key] = torch.tensor(0.0, device=logits.device)
        
        # é‡æ–°è®¡ç®—ç»„åˆæŸå¤±
        # æ·»åŠ Hausdorff Distance Lossï¼ˆå¦‚æœå¯ç”¨ï¼‰
        hausdorff_loss = None
        if loss_weights.get('hausdorff', 0) > 0:
            try:
                hausdorff_loss = self.hausdorff_distance_loss(logits, masks, percentile=95, alpha=1.0)
                if torch.isfinite(hausdorff_loss):
                    loss_components['hausdorff'] = hausdorff_loss
                else:
                    loss_components['hausdorff'] = torch.tensor(0.0, device=logits.device)
            except Exception as e:
                print(f"[è­¦å‘Š] Hausdorff Lossè®¡ç®—å¤±è´¥: {e}ï¼Œè·³è¿‡")
                loss_components['hausdorff'] = torch.tensor(0.0, device=logits.device)
        
        combined_loss = (
            loss_weights['bce'] * loss_components['bce']
            + loss_weights['dice'] * loss_components['dice']
            + loss_weights['tversky'] * loss_components['tversky']
            + loss_weights['tversky_focal'] * loss_components['tversky_focal']
            + loss_weights['boundary'] * loss_components['boundary']
            + loss_weights['focal'] * loss_components['focal']
            + loss_weights['fn_penalty'] * loss_components['fn_penalty']
            + loss_weights['fp_penalty'] * loss_components['fp_penalty']
        )
        if use_lovasz and loss_weights.get('lovasz', 0) > 0:
            combined_loss += loss_weights['lovasz'] * loss_components['lovasz']
        if loss_weights.get('hausdorff', 0) > 0 and 'hausdorff' in loss_components:
            combined_loss += loss_weights['hausdorff'] * loss_components['hausdorff']
        
        # æœ€ç»ˆæ£€æŸ¥ï¼šå¦‚æœç»„åˆæŸå¤±ä»ç„¶æ˜¯NaN/Infï¼Œä½¿ç”¨BCEæŸå¤±ä½œä¸ºåå¤‡
        if not torch.isfinite(combined_loss):
            print(
                "[ä¸¥é‡è­¦å‘Š] ç»„åˆæŸå¤±ä»ä¸ºNaN/Infï¼Œä½¿ç”¨BCEæŸå¤±ä½œä¸ºåå¤‡ -> "
                f"BCE={loss_components['bce'].item():.4f}, Dice={loss_components['dice'].item():.4f}, "
                f"Tversky={loss_components['tversky'].item():.4f}, Boundary={loss_components['boundary'].item():.4f}, "
                f"Focal={loss_components['focal'].item():.4f}, "
                f"Lovasz={(loss_components.get('lovasz', torch.tensor(0.0)).item() if use_lovasz else 0.0):.4f}"
            )
            combined_loss = loss_components['bce']  # ä½¿ç”¨BCEä½œä¸ºåå¤‡
        
        return combined_loss

    def _ensemble_inference(self, *args, **kwargs):
        """æ¨¡å‹é›†æˆåŠŸèƒ½å·²å–æ¶ˆã€‚"""
        raise RuntimeError("æ¨¡å‹é›†æˆåŠŸèƒ½å·²å–æ¶ˆ")

    def _tta_inference(self, model, images):
        """
        ã€å®Œå…¨é‡å†™ã€‘å¤šå°ºåº¦æµ‹è¯•æ—¶å¢å¼º (MSTTA) - ä¿®å¤ç‰ˆ
        
        æ ¸å¿ƒæ”¹è¿›ï¼š
        1. ç»´åº¦è‡ªé€‚åº”ï¼šåŠ¨æ€æ£€æµ‹è¾“å‡ºé€šé“æ•°ï¼Œå½»åº•è§£å†³ IndexError
        2. æ¦‚ç‡ç©ºé—´èåˆï¼šåœ¨æ¦‚ç‡ç©ºé—´è¿›è¡ŒTTAèåˆï¼Œé¿å…æ•°å­¦é”™è¯¯
        3. æ­£ç¡®çš„åå¤„ç†ï¼šå¯¹æ¦‚ç‡å›¾è¿›è¡Œé«˜æ–¯å¹³æ»‘å’Œåå¤„ç†
        4. ç²¾åº¦ä¼˜åŒ–ï¼šé¿å…åå¤çš„ Log/Sigmoid è½¬æ¢ï¼Œå‡å°‘ç²¾åº¦æŸå¤±
        
        å¤šå°ºåº¦æ¨ç†ï¼š3ä¸ªå°ºåº¦ Ã— 8ç§å˜æ¢ = 24å€æ¨ç†
        - å°ºåº¦å› å­: [0.8, 1.0, 1.2]
        - 8ç§å˜æ¢: åŸå§‹ã€æ°´å¹³ç¿»è½¬ã€å‚ç›´ç¿»è½¬ã€æ—‹è½¬90/180/270åº¦ã€ç¿»è½¬+æ—‹è½¬ç»„åˆ
        """
        import torch.nn.functional as F
        from scipy.ndimage import gaussian_filter
        
        B, C_input, H, W = images.shape  # C_input æ˜¯è¾“å…¥å›¾åƒçš„é€šé“æ•°ï¼ˆé€šå¸¸æ˜¯3ï¼‰
        scales = [0.8, 1.0, 1.2]  # å¤šå°ºåº¦å› å­
        all_prob_maps = []  # å­˜å‚¨æ‰€æœ‰æ¦‚ç‡å›¾ï¼ˆè€ŒéLogitsï¼‰
        all_weights = []  # å­˜å‚¨ç½®ä¿¡åº¦æƒé‡
        
        # ã€å¤šå°ºåº¦å¾ªç¯ã€‘
        for scale in scales:
            # Resizeåˆ°ç›®æ ‡å°ºåº¦
            if scale != 1.0:
                target_h, target_w = int(H * scale), int(W * scale)
                scaled_images = F.interpolate(images, size=(target_h, target_w), 
                                             mode='bilinear', align_corners=False)
            else:
                scaled_images = images
                target_h, target_w = H, W
            
            # ã€8ç§å˜æ¢å¾ªç¯ã€‘
            scale_prob_maps = []
            
            # 1. åŸå§‹å›¾åƒ
            pred_logits = model(scaled_images)
            if isinstance(pred_logits, tuple):
                pred_logits = pred_logits[0]
            if not (torch.any(torch.isnan(pred_logits)) or torch.any(torch.isinf(pred_logits))):
                if scale != 1.0:
                    pred_logits = F.interpolate(pred_logits, size=(H, W), mode='bilinear', align_corners=False)
                # ã€å…³é”®ä¿®å¤ã€‘ç«‹å³è½¬æ¢ä¸ºæ¦‚ç‡å›¾ï¼Œåœ¨æ¦‚ç‡ç©ºé—´è¿›è¡Œèåˆ
                pred_prob = torch.sigmoid(pred_logits)
                scale_prob_maps.append(pred_prob)
            
            # 2. æ°´å¹³ç¿»è½¬
            pred_logits = model(torch.flip(scaled_images, dims=[3]))
            if isinstance(pred_logits, tuple):
                pred_logits = pred_logits[0]
            pred_logits = torch.flip(pred_logits, dims=[3])
            if not (torch.any(torch.isnan(pred_logits)) or torch.any(torch.isinf(pred_logits))):
                if scale != 1.0:
                    pred_logits = F.interpolate(pred_logits, size=(H, W), mode='bilinear', align_corners=False)
                pred_prob = torch.sigmoid(pred_logits)
                scale_prob_maps.append(pred_prob)
            
            # 3. å‚ç›´ç¿»è½¬
            pred_logits = model(torch.flip(scaled_images, dims=[2]))
            if isinstance(pred_logits, tuple):
                pred_logits = pred_logits[0]
            pred_logits = torch.flip(pred_logits, dims=[2])
            if not (torch.any(torch.isnan(pred_logits)) or torch.any(torch.isinf(pred_logits))):
                if scale != 1.0:
                    pred_logits = F.interpolate(pred_logits, size=(H, W), mode='bilinear', align_corners=False)
                pred_prob = torch.sigmoid(pred_logits)
                scale_prob_maps.append(pred_prob)
            
            # 4. æ—‹è½¬90åº¦
            pred_logits = model(torch.rot90(scaled_images, k=1, dims=[2, 3]))
            if isinstance(pred_logits, tuple):
                pred_logits = pred_logits[0]
            pred_logits = torch.rot90(pred_logits, k=-1, dims=[2, 3])
            if not (torch.any(torch.isnan(pred_logits)) or torch.any(torch.isinf(pred_logits))):
                if scale != 1.0:
                    pred_logits = F.interpolate(pred_logits, size=(H, W), mode='bilinear', align_corners=False)
                pred_prob = torch.sigmoid(pred_logits)
                scale_prob_maps.append(pred_prob)
            
            # 5. æ—‹è½¬180åº¦
            pred_logits = model(torch.rot90(scaled_images, k=2, dims=[2, 3]))
            if isinstance(pred_logits, tuple):
                pred_logits = pred_logits[0]
            pred_logits = torch.rot90(pred_logits, k=-2, dims=[2, 3])
            if not (torch.any(torch.isnan(pred_logits)) or torch.any(torch.isinf(pred_logits))):
                if scale != 1.0:
                    pred_logits = F.interpolate(pred_logits, size=(H, W), mode='bilinear', align_corners=False)
                pred_prob = torch.sigmoid(pred_logits)
                scale_prob_maps.append(pred_prob)
            
            # 6. æ—‹è½¬270åº¦
            pred_logits = model(torch.rot90(scaled_images, k=3, dims=[2, 3]))
            if isinstance(pred_logits, tuple):
                pred_logits = pred_logits[0]
            pred_logits = torch.rot90(pred_logits, k=-3, dims=[2, 3])
            if not (torch.any(torch.isnan(pred_logits)) or torch.any(torch.isinf(pred_logits))):
                if scale != 1.0:
                    pred_logits = F.interpolate(pred_logits, size=(H, W), mode='bilinear', align_corners=False)
                pred_prob = torch.sigmoid(pred_logits)
                scale_prob_maps.append(pred_prob)
            
            # 7. æ°´å¹³ç¿»è½¬+æ—‹è½¬90åº¦
            img_aug = torch.flip(scaled_images, dims=[3])
            img_aug = torch.rot90(img_aug, k=1, dims=[2, 3])
            pred_logits = model(img_aug)
            if isinstance(pred_logits, tuple):
                pred_logits = pred_logits[0]
            pred_logits = torch.rot90(pred_logits, k=-1, dims=[2, 3])
            pred_logits = torch.flip(pred_logits, dims=[3])
            if not (torch.any(torch.isnan(pred_logits)) or torch.any(torch.isinf(pred_logits))):
                if scale != 1.0:
                    pred_logits = F.interpolate(pred_logits, size=(H, W), mode='bilinear', align_corners=False)
                pred_prob = torch.sigmoid(pred_logits)
                scale_prob_maps.append(pred_prob)
            
            # 8. å‚ç›´ç¿»è½¬+æ—‹è½¬90åº¦
            img_aug = torch.flip(scaled_images, dims=[2])
            img_aug = torch.rot90(img_aug, k=1, dims=[2, 3])
            pred_logits = model(img_aug)
            if isinstance(pred_logits, tuple):
                pred_logits = pred_logits[0]
            pred_logits = torch.rot90(pred_logits, k=-1, dims=[2, 3])
            pred_logits = torch.flip(pred_logits, dims=[2])
            if not (torch.any(torch.isnan(pred_logits)) or torch.any(torch.isinf(pred_logits))):
                if scale != 1.0:
                    pred_logits = F.interpolate(pred_logits, size=(H, W), mode='bilinear', align_corners=False)
                pred_prob = torch.sigmoid(pred_logits)
                scale_prob_maps.append(pred_prob)
            
            # æ”¶é›†å½“å‰å°ºåº¦çš„æ‰€æœ‰æ¦‚ç‡å›¾
            all_prob_maps.extend(scale_prob_maps)
        
        # ã€æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆé¢„æµ‹ã€‘
        if len(all_prob_maps) == 0:
            print(f"[ä¸¥é‡è­¦å‘Š] MSTTA: æ‰€æœ‰å˜æ¢çš„é¢„æµ‹éƒ½åŒ…å«NaN/Infï¼Œè¿”å›é›¶è¾“å‡º")
            fallback_output = model(images)
            if isinstance(fallback_output, tuple):
                fallback_output = fallback_output[0]
            return torch.zeros_like(fallback_output)
        
        # ã€ç»´åº¦è‡ªé€‚åº”ã€‘ä»ç¬¬ä¸€ä¸ªæ¦‚ç‡å›¾ä¸­è·å–æ¨¡å‹è¾“å‡ºçš„å®é™…é€šé“æ•°
        first_prob = all_prob_maps[0]
        if first_prob.dim() == 4:
            _, C_output, _, _ = first_prob.shape  # C_output æ˜¯æ¨¡å‹è¾“å‡ºçš„é€šé“æ•°
        elif first_prob.dim() == 3:
            # å¦‚æœè¾“å‡ºæ˜¯ [B, H, W]ï¼Œè¯´æ˜æ˜¯å•é€šé“ï¼Œéœ€è¦æ·»åŠ é€šé“ç»´åº¦
            C_output = 1
            all_prob_maps = [p.unsqueeze(1) if p.dim() == 3 else p for p in all_prob_maps]
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„é¢„æµ‹å¼ é‡ç»´åº¦: {first_prob.dim()}")
        
        # ã€å…³é”®ä¿®å¤ã€‘ç»Ÿä¸€æ‰€æœ‰æ¦‚ç‡å›¾çš„ç©ºé—´å°ºå¯¸åˆ°ç›®æ ‡å°ºå¯¸ (H, W)
        # ç¡®ä¿æ‰€æœ‰å¼ é‡åœ¨ stack ä¹‹å‰å…·æœ‰ç›¸åŒçš„ç©ºé—´ç»´åº¦
        target_size = (H, W)
        normalized_prob_maps = []
        for prob_map in all_prob_maps:
            if prob_map.dim() == 4:
                _, _, h, w = prob_map.shape
                if h != H or w != W:
                    # æ’å€¼åˆ°ç›®æ ‡å°ºå¯¸
                    prob_map = F.interpolate(prob_map, size=target_size, mode='bilinear', align_corners=False)
            normalized_prob_maps.append(prob_map)
        all_prob_maps = normalized_prob_maps
        
        # ã€åŠ æƒèåˆã€‘è®¡ç®—æ¯ä¸ªé¢„æµ‹çš„ç½®ä¿¡åº¦æƒé‡ï¼ˆåŸºäºæ¦‚ç‡å›¾ï¼‰
        weights = []
        eps = 1e-8
        for prob_map in all_prob_maps:
            # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦ï¼šä½¿ç”¨ç†µçš„è´Ÿå€¼ä½œä¸ºç½®ä¿¡åº¦åº¦é‡
            # ç†µè¶Šä½ï¼Œç½®ä¿¡åº¦è¶Šé«˜
            entropy = -prob_map * torch.log(prob_map + eps) - (1 - prob_map) * torch.log(1 - prob_map + eps)
            confidence = 1.0 - entropy.mean()  # è½¬æ¢ä¸ºç½®ä¿¡åº¦ï¼ˆ1 - ç†µï¼‰
            weights.append(float(confidence))
        
        # å½’ä¸€åŒ–æƒé‡
        weights = torch.tensor(weights, device=images.device, dtype=torch.float32)
        weights = weights / (weights.sum() + eps)
        
        # ã€æ¦‚ç‡ç©ºé—´åŠ æƒå¹³å‡ã€‘åœ¨æ¦‚ç‡ç©ºé—´è¿›è¡Œèåˆï¼Œè€ŒéLogitsç©ºé—´
        stacked_probs = torch.stack(all_prob_maps, dim=0)  # [N, B, C_output, H, W]
        weights_expanded = weights.view(-1, 1, 1, 1, 1)  # [N, 1, 1, 1, 1]
        fused_prob = (stacked_probs * weights_expanded).sum(dim=0)  # [B, C_output, H, W]
        
        # ã€æ­£ç¡®çš„åå¤„ç†ã€‘å¯¹æ¦‚ç‡å›¾è¿›è¡Œé«˜æ–¯å¹³æ»‘ï¼ˆè€Œéå¯¹Logitsï¼‰
        fused_prob_np = fused_prob.detach().cpu().numpy()
        smoothed_prob_np = np.zeros_like(fused_prob_np)
        for b in range(B):
            for c in range(C_output):  # ã€å…³é”®ä¿®å¤ã€‘ä½¿ç”¨ C_outputï¼Œå½»åº•è§£å†³ IndexError
                smoothed_prob_np[b, c] = gaussian_filter(fused_prob_np[b, c], sigma=0.5)
        
        # ã€æè‡´åå¤„ç†ã€‘åœ¨æ¦‚ç‡å›¾ä¸Šåº”ç”¨LCCå’Œremove_small_holes
        processed_prob_np = np.zeros_like(smoothed_prob_np)
        for b in range(B):
            for c in range(C_output):  # ã€å…³é”®ä¿®å¤ã€‘ä½¿ç”¨ C_outputï¼Œå½»åº•è§£å†³ IndexError
                prob_map = smoothed_prob_np[b, c]
                # åº”ç”¨æè‡´åå¤„ç†æµæ°´çº¿
                processed_mask = ensemble_post_process_global(
                    prob_map,
                    use_lcc=True,  # ä¿ç•™æœ€å¤§è¿é€šåŸŸ
                    use_remove_holes=True,  # å¡«è¡¥å°å­”æ´
                    min_hole_size=100,
                    use_edge_smoothing=True  # è¾¹ç¼˜å¹³æ»‘
                )
                processed_prob_np[b, c] = processed_mask
        
        # ã€å…¼å®¹æ€§è¿”å›ã€‘å°†å¤„ç†å¥½çš„æ¦‚ç‡å›¾æ˜ å°„å›ä¼ªLogitsæ ¼å¼
        # é¿å…ä½¿ç”¨ä¸ç¨³å®šçš„ np.log å…¬å¼ï¼Œç›´æ¥ä½¿ç”¨çº¿æ€§æ˜ å°„
        # 0 -> -10, 1 -> 10ï¼Œä¿æŒæ•°å€¼ç¨³å®šæ€§
        processed_prob_tensor = torch.from_numpy(processed_prob_np).to(images.device).float()
        # çº¿æ€§æ˜ å°„ï¼šprob [0, 1] -> logits [-10, 10]
        final_logits = (processed_prob_tensor - 0.5) * 20.0  # å°† [0, 1] æ˜ å°„åˆ° [-10, 10]
        
        return final_logits
    
    @staticmethod
    def smart_post_processing(
        pred_mask,
        pred_probs,
        tiny_size_thresh: int = 2,
        small_min_size: int = 3,
        small_max_size: int = 19,
        prob_threshold: float = 0.65,
    ):
        """
        æ™ºèƒ½åå¤„ç†å‡½æ•°ï¼ˆSmart Post-Processingï¼‰
        
        ä»…åŸºäºè¿é€šåŸŸé¢ç§¯ + æ¦‚ç‡è‡ªé€‚åº”åœ°è¿‡æ»¤å°ç—…ç¶/å™ªç‚¹ï¼Œé¿å…è¯¯åˆ çœŸå®å¾®å°ç—…ç¶ã€‚
        
        åˆ†çº§ç­–ç•¥ï¼š
        - Level 1: ç»å¯¹å™ªéŸ³ (area <= tiny_size_threshï¼Œé»˜è®¤ <=2 åƒç´ ) -> ç›´æ¥åˆ é™¤
        - Level 2: å®‰å…¨åŒºåŸŸ (area >= 20 åƒç´ ) -> æ— æ¡ä»¶ä¿ç•™
        - Level 3: æ¨¡ç³Šåœ°å¸¦ (3~19 åƒç´ ) -> ä»…åœ¨å¹³å‡æ¦‚ç‡ > prob_threshold æ—¶ä¿ç•™
        """
        # å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…åœ¨æœªå®‰è£… skimage æ—¶ç›´æ¥å´©æºƒ
        try:
            from skimage import measure
        except ImportError:
            # å¦‚æœæ²¡æœ‰ skimageï¼Œå›é€€ä¸ºåŸmaskï¼Œä¸åšæ™ºèƒ½è¿‡æ»¤
            return pred_mask
        
        if isinstance(pred_mask, torch.Tensor):
            mask_np = pred_mask.detach().cpu().numpy()
            is_tensor = True
            device = pred_mask.device
        else:
            mask_np = np.asarray(pred_mask)
            is_tensor = False
            device = None
        
        if isinstance(pred_probs, torch.Tensor):
            probs_np = pred_probs.detach().cpu().numpy()
        else:
            probs_np = np.asarray(pred_probs)
        
        # ä¿è¯äºŒç»´
        if mask_np.ndim > 2:
            mask_np = mask_np.squeeze()
        if probs_np.ndim > 2:
            probs_np = probs_np.squeeze()
        
        # å°ºå¯¸ä¸ä¸€è‡´æ—¶ç›´æ¥è¿”å›åŸmaskï¼Œé¿å…å½¢çŠ¶é”™è¯¯
        if mask_np.shape != probs_np.shape:
            return pred_mask
        
        # äºŒå€¼åŒ–ï¼ˆpreds æœ¬èº«å·²ç»æ˜¯0/1ï¼Œè¿™é‡Œå†æ¬¡ä¿è¯ï¼‰
        binary = (mask_np > 0.5).astype(np.uint8)
        
        # æ²¡æœ‰å‰æ™¯å°±ç›´æ¥è¿”å›
        if binary.sum() == 0:
            return pred_mask
        
        # è¿é€šåŸŸæ ‡è®°ï¼Œå¹¶ä½¿ç”¨æ¦‚ç‡å›¾ä½œä¸º intensity_imageï¼Œä»¥ä¾¿è®¡ç®— mean_intensity
        labels = measure.label(binary, connectivity=1)
        regions = measure.regionprops(labels, intensity_image=probs_np.astype(np.float32))
        
        cleaned = np.zeros_like(binary, dtype=np.uint8)
        
        for region in regions:
            area = region.area
            mean_prob = float(region.mean_intensity) if hasattr(region, "mean_intensity") else 0.0
            
            # Level 1: æå°åŒºåŸŸï¼ˆ<= tiny_size_threshï¼‰è§†ä¸ºç»å¯¹å™ªéŸ³ï¼Œç›´æ¥è·³è¿‡
            if area <= tiny_size_thresh:
                continue
            
            # Level 2: å¤§äºç­‰äº 20 åƒç´ çš„åŒºåŸŸï¼Œæ— æ¡ä»¶ä¿ç•™
            if area >= 20:
                cleaned[labels == region.label] = 1
                continue
            
            # Level 3: 3~19 åƒç´ ä¹‹é—´ï¼Œä¾æ®å¹³å‡æ¦‚ç‡åˆ¤æ–­
            if small_min_size <= area <= small_max_size and mean_prob > prob_threshold:
                cleaned[labels == region.label] = 1
                continue
            # å¦åˆ™è§†ä¸ºå™ªå£°ï¼Œä¸å†™å…¥ cleaned
        
        # å¦‚æœå…¨éƒ¨è¢«è¿‡æ»¤æ‰ï¼Œåˆ™ä¿æŒå…¨ç©ºmaskï¼Œè¡¨ç¤ºæ™ºèƒ½è¿‡æ»¤è®¤ä¸ºè¯¥å›¾åƒä¸­æ²¡æœ‰å¯é ç—…ç¶
        # ä¹‹å‰çš„é€»è¾‘ä¼šå›é€€ä¸ºåŸå§‹ noisy maskï¼Œè¿™ä¼šæ‹‰ä½ Dice_Negï¼Œç°æ ¹æ®ç»Ÿè®¡ç­–ç•¥ç§»é™¤å›é€€ã€‚
        
        if is_tensor:
            return torch.from_numpy(cleaned).to(device=device, dtype=torch.float32)
        else:
            return cleaned.astype(np.float32)

    @staticmethod
    def post_process_optimize(mask):
        """
        å¯¹äºŒå€¼æ©ç è¿›è¡Œå¾®å°è†¨èƒ€ï¼Œå¡«è¡¥è¾¹ç¼˜ï¼Œæå‡ Dice
        
        ã€å…³é”®ã€‘é’ˆå¯¹æ¬ åˆ†å‰²é—®é¢˜ï¼Œé€šè¿‡å¾®å°è†¨èƒ€ï¼ˆ1-2åƒç´ ï¼‰æ¥æå‡ Dice åˆ†æ•°
        é€‚ç”¨äº Specificity å¾ˆé«˜ä½†å¯èƒ½å­˜åœ¨è½»å¾®æ¬ åˆ†å‰²çš„æƒ…å†µ
        
        Args:
            mask: äºŒå€¼æ©ç  (numpy array, 0-1 æˆ– 0-255)
        
        Returns:
            dilated_mask: è†¨èƒ€åçš„æ©ç  (numpy array, 0-1)
        """
        # 1. ç¡®ä¿æ˜¯ uint8 æ ¼å¼
        if mask.dtype != np.uint8:
            if mask.max() <= 1.0:
                mask = (mask * 255).astype(np.uint8)
            else:
                mask = mask.astype(np.uint8)
        
        # 2. å®šä¹‰è†¨èƒ€æ ¸ (Kernel)
        # ä½¿ç”¨ 3x3 çš„æ ¸ï¼Œè¿­ä»£ 1 æ¬¡ï¼Œç›¸å½“äºå‘å¤–æ‰© 1 ä¸ªåƒç´ 
        # å¦‚æœæƒ³æ›´æ¿€è¿›ï¼Œå¯ä»¥æŠŠ iterations æ”¹ä¸º 2
        kernel = np.ones((3, 3), np.uint8)
        
        # 3. æ‰§è¡Œè†¨èƒ€ (Dilation)
        dilated_mask = cv2.dilate(mask, kernel, iterations=1)
        
        # 4. è½¬æ¢å› 0-1 èŒƒå›´
        dilated_mask = (dilated_mask > 127).astype(np.float32)
        
        return dilated_mask

    @staticmethod
    def post_process_mask(
        pred_mask,
        min_size=50,
        use_morphology=True,
        keep_largest=True,
        fill_holes=True,
        enable_opening=True,
        opening_kernel_size: int = 3,
        opening_iterations: int = 1,
    ):
        """
        åå¤„ç†ä¼˜åŒ–é¢„æµ‹mask - å¢å¼ºç‰ˆ
        
        Args:
            pred_mask: é¢„æµ‹mask (numpyæˆ–tensor)
            min_size: ç§»é™¤å°äºæ­¤å¤§å°çš„è¿é€šåŸŸ
            use_morphology: æ˜¯å¦ä½¿ç”¨å½¢æ€å­¦æ“ä½œ
            keep_largest: æ˜¯å¦åªä¿ç•™æœ€å¤§è¿é€šåŸŸï¼ˆå•å™¨å®˜åˆ†å‰²æ¨èï¼‰
            fill_holes: æ˜¯å¦å¡«å……å†…éƒ¨å­”æ´ï¼ˆå»é™¤å‡é˜´æ€§ç©ºæ´ï¼‰
        
        Returns:
            å¤„ç†åçš„mask
        """
        import cv2
        from scipy import ndimage
        
        if isinstance(pred_mask, torch.Tensor):
            pred_np = pred_mask.detach().cpu().numpy()
            is_tensor = True
            device = pred_mask.device
        else:
            pred_np = pred_mask.copy()
            is_tensor = False
        
        if pred_np.sum() < 10:  # å‡ ä¹ä¸ºç©º,ç›´æ¥è¿”å›
            return pred_mask
        
        pred_binary = (pred_np > 0.5).astype(np.uint8)
        
        # 1. å¡«å……å­”æ´ï¼ˆFill Holesï¼‰- å»é™¤å™¨å®˜å†…éƒ¨çš„å‡é˜´æ€§ç©ºæ´
        if fill_holes:
            # ä½¿ç”¨ scipy.ndimage.binary_fill_holes å¡«å……å†…éƒ¨å­”æ´
            pred_binary = ndimage.binary_fill_holes(pred_binary).astype(np.uint8)
        
        # 2. å½¢æ€å­¦é—­æ“ä½œ - è¿›ä¸€æ­¥å¡«å……å°å­”æ´å’Œç¼éš™
        if use_morphology:
            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
            pred_binary = cv2.morphologyEx(pred_binary, cv2.MORPH_CLOSE, kernel)
            # å½¢æ€å­¦å¼€æ“ä½œï¼ˆå¯é€‰ï¼‰- å»é™¤å°å™ªç‚¹/æ¯›åˆº
            if enable_opening:
                k = int(max(1, opening_kernel_size))
                # kernel size éœ€ä¸ºå¥‡æ•°
                if k % 2 == 0:
                    k += 1
                kernel_small = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (k, k))
                iters = int(max(1, opening_iterations))
                pred_binary = cv2.morphologyEx(pred_binary, cv2.MORPH_OPEN, kernel_small, iterations=iters)
        
        # 3. ä¿ç•™æœ€å¤§è¿é€šåŸŸï¼ˆKeep Largest Connected Componentï¼‰- å»é™¤å­¤ç«‹çš„å™ªç‚¹
        if keep_largest:
            labeled, num_features = ndimage.label(pred_binary)
            if num_features > 0:
                # è®¡ç®—æ¯ä¸ªè¿é€šåŸŸçš„å¤§å°
                sizes = ndimage.sum(pred_binary, labeled, range(1, num_features + 1))
                # æ‰¾åˆ°æœ€å¤§çš„è¿é€šåŸŸ
                largest_label = np.argmax(sizes) + 1
                # åªä¿ç•™æœ€å¤§è¿é€šåŸŸ
                pred_binary = (labeled == largest_label).astype(np.uint8)
        else:
            # 4. è¿é€šåŸŸåˆ†æ - ç§»é™¤å°åŒºåŸŸï¼ˆå¦‚æœä¸ä½¿ç”¨keep_largestï¼‰
            if min_size > 0:
                labeled, num_features = ndimage.label(pred_binary)
                if num_features > 0:
                    sizes = ndimage.sum(pred_binary, labeled, range(1, num_features + 1))
                    mask_sizes = sizes >= min_size
                    # åªä¿ç•™å¤§åŒºåŸŸ
                    keep_labels = np.where(mask_sizes)[0] + 1
                    pred_binary = np.isin(labeled, keep_labels).astype(np.uint8)
        
        # è¿”å›åŸå§‹ç±»å‹
        if is_tensor:
            return torch.from_numpy(pred_binary).to(device).float()
        else:
            return pred_binary.astype(np.float32)
    
    @staticmethod
    def post_process_refine_for_hd95(pred_probs, threshold=0.5, min_area_threshold=100, 
                                     use_gaussian_blur=True, use_morphology=True,
                                     dynamic_area_threshold=True):
        """
        ä¼˜åŒ–çš„åå¤„ç†æµæ°´çº¿ï¼šä¸“é—¨ç”¨äºé™ä½HD95ï¼ŒåŒæ—¶ä¿æŒDice > 0.88
        
        ç­–ç•¥ï¼š
        1. é«˜æ–¯æ¨¡ç³Šå¹³æ»‘è¾¹ç¼˜ï¼ˆå¯é€‰ï¼‰
        2. äºŒå€¼åŒ–
        3. å½¢æ€å­¦é—­è¿ç®—ï¼šå¡«å……å†…éƒ¨ç©ºæ´å¹¶å¹³æ»‘è¾¹ç¼˜
        4. ä¸¥æ ¼è¿é€šåŸŸè¿‡æ»¤ï¼šä»…ä¿ç•™é¢ç§¯æœ€å¤§çš„ä¸¤ä¸ªè¿é€šåŸŸï¼Œåˆ é™¤å°åŒºåŸŸ
        5. åŠ¨æ€é¢ç§¯é˜ˆå€¼ï¼šæ ¹æ®è¾“å…¥æ¦‚ç‡åŠ¨æ€è°ƒæ•´é¢ç§¯é˜ˆå€¼ï¼ˆä½æ¦‚ç‡æ ·æœ¬æ›´ä¸¥æ ¼ï¼‰
        
        Args:
            pred_probs: æ¦‚ç‡å›¾ (numpy array æˆ– torch.Tensor, shape: H x W)
            threshold: äºŒå€¼åŒ–é˜ˆå€¼
            min_area_threshold: åŸºç¡€æœ€å°è¿é€šåŸŸé¢ç§¯é˜ˆå€¼ï¼ˆåƒç´ ï¼‰ï¼Œå°äºæ­¤å€¼çš„åŒºåŸŸå°†è¢«åˆ é™¤
            use_gaussian_blur: æ˜¯å¦ä½¿ç”¨é«˜æ–¯æ¨¡ç³Šå¹³æ»‘è¾¹ç¼˜
            use_morphology: æ˜¯å¦ä½¿ç”¨å½¢æ€å­¦é—­è¿ç®—
            dynamic_area_threshold: æ˜¯å¦æ ¹æ®æ¦‚ç‡åŠ¨æ€è°ƒæ•´é¢ç§¯é˜ˆå€¼
        
        Returns:
            å¤„ç†åçš„äºŒå€¼æ©ç  (numpy array æˆ– torch.Tensor, 0-1)
        """
        import cv2
        from scipy import ndimage
        
        # è½¬æ¢ä¸º numpy
        if isinstance(pred_probs, torch.Tensor):
            probs_np = pred_probs.detach().cpu().numpy()
            is_tensor = True
            device = pred_probs.device
        else:
            probs_np = np.asarray(pred_probs)
            is_tensor = False
            device = None
        
        # ç¡®ä¿äºŒç»´
        if probs_np.ndim > 2:
            probs_np = probs_np.squeeze()
        
        # ã€åŠ¨æ€é¢ç§¯é˜ˆå€¼ã€‘æ ¹æ®è¾“å…¥æ¦‚ç‡çš„å¹³å‡å€¼åŠ¨æ€è°ƒæ•´é¢ç§¯é˜ˆå€¼
        # ä½æ¦‚ç‡æ ·æœ¬ï¼ˆå¹³å‡æ¦‚ç‡ < 0.3ï¼‰ä½¿ç”¨æ›´ä¸¥æ ¼çš„è¿‡æ»¤ï¼ˆ1.5å€åŸºç¡€é˜ˆå€¼ï¼‰
        # ä¸­ç­‰æ¦‚ç‡æ ·æœ¬ï¼ˆ0.3 <= å¹³å‡æ¦‚ç‡ < 0.6ï¼‰ä½¿ç”¨æ ‡å‡†é˜ˆå€¼
        # é«˜æ¦‚ç‡æ ·æœ¬ï¼ˆå¹³å‡æ¦‚ç‡ >= 0.6ï¼‰ä½¿ç”¨è¾ƒå®½æ¾çš„è¿‡æ»¤ï¼ˆ0.8å€åŸºç¡€é˜ˆå€¼ï¼‰
        if dynamic_area_threshold:
            mean_prob = float(np.mean(probs_np))
            if mean_prob < 0.3:
                # ä½æ¦‚ç‡æ ·æœ¬ï¼šæ›´ä¸¥æ ¼çš„è¿‡æ»¤ï¼Œå‡å°‘å‡é˜³æ€§
                area_threshold = int(min_area_threshold * 1.5)
            elif mean_prob >= 0.6:
                # é«˜æ¦‚ç‡æ ·æœ¬ï¼šè¾ƒå®½æ¾çš„è¿‡æ»¤ï¼Œé¿å…åˆ é™¤çœŸå®ç—…ç¶
                area_threshold = int(min_area_threshold * 0.8)
            else:
                # ä¸­ç­‰æ¦‚ç‡æ ·æœ¬ï¼šä½¿ç”¨æ ‡å‡†é˜ˆå€¼
                area_threshold = min_area_threshold
        else:
            area_threshold = min_area_threshold
        
        # 1. é«˜æ–¯æ¨¡ç³Šå¹³æ»‘è¾¹ç¼˜ï¼ˆé™ä½HD95çš„å…³é”®æ­¥éª¤ï¼‰
        if use_gaussian_blur:
            probs_blurred = cv2.GaussianBlur(probs_np.astype(np.float32), ksize=(3, 3), sigmaX=0.5)
        else:
            probs_blurred = probs_np.astype(np.float32)
        
        # 2. äºŒå€¼åŒ–
        binary = (probs_blurred > threshold).astype(np.uint8)
        
        # å¦‚æœæ²¡æœ‰å‰æ™¯ï¼Œç›´æ¥è¿”å›
        if binary.sum() == 0:
            if is_tensor:
                return torch.from_numpy(binary.astype(np.float32)).to(device)
            return binary.astype(np.float32)
        
        # 3. å½¢æ€å­¦é—­è¿ç®—ï¼šå¡«å……å†…éƒ¨ç©ºæ´å¹¶å¹³æ»‘è¾¹ç¼˜
        if use_morphology:
            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
            binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel, iterations=1)
        
        # 4. ä¸¥æ ¼è¿é€šåŸŸè¿‡æ»¤ï¼šä»…ä¿ç•™é¢ç§¯æœ€å¤§çš„ä¸¤ä¸ªè¿é€šåŸŸï¼Œåˆ é™¤å°åŒºåŸŸ
        try:
            from skimage import measure
            labels = measure.label(binary, connectivity=1)
            regions = measure.regionprops(labels)
            
            if len(regions) == 0:
                cleaned = np.zeros_like(binary, dtype=np.uint8)
            else:
                # æŒ‰é¢ç§¯é™åºæ’åº
                sorted_regions = sorted(regions, key=lambda r: r.area, reverse=True)
                
                cleaned = np.zeros_like(binary, dtype=np.uint8)
                # ä»…ä¿ç•™é¢ç§¯æœ€å¤§çš„ä¸¤ä¸ªè¿é€šåŸŸï¼ˆå·¦å³è‚ºï¼‰ï¼Œä¸”é¢ç§¯å¿…é¡» >= area_thresholdï¼ˆåŠ¨æ€è°ƒæ•´ï¼‰
                kept_count = 0
                for region in sorted_regions:
                    if region.area >= area_threshold and kept_count < 2:
                        cleaned[labels == region.label] = 1
                        kept_count += 1
        except ImportError:
            # å¦‚æœæ²¡æœ‰ skimageï¼Œä½¿ç”¨ scipy å®ç°
            labeled, num_features = ndimage.label(binary)
            if num_features > 0:
                sizes = ndimage.sum(binary, labeled, range(1, num_features + 1))
                # æ‰¾åˆ°é¢ç§¯æœ€å¤§çš„ä¸¤ä¸ªè¿é€šåŸŸ
                sorted_indices = np.argsort(sizes)[::-1]
                kept_labels = []
                for idx in sorted_indices:
                    if sizes[idx] >= area_threshold and len(kept_labels) < 2:
                        kept_labels.append(idx + 1)
                if kept_labels:
                    cleaned = np.isin(labeled, kept_labels).astype(np.uint8)
                else:
                    cleaned = np.zeros_like(binary, dtype=np.uint8)
            else:
                cleaned = np.zeros_like(binary, dtype=np.uint8)
        
        if is_tensor:
            return torch.from_numpy(cleaned.astype(np.float32)).to(device)
        return cleaned.astype(np.float32)


# ==================== ã€æ ¸å¿ƒä¿®å¤1ã€‘ç‹¬ç«‹å‡½æ•°ï¼šè§£å†³Pickleé”™è¯¯ ====================
# å°†HD95å’ŒDiceè®¡ç®—é€»è¾‘å‰¥ç¦»ä¸ºç‹¬ç«‹å‡½æ•°ï¼Œä¸ä¾èµ–ç±»å®ä¾‹ï¼Œå¯ç”¨äºå¤šè¿›ç¨‹å¹¶è¡Œå¤„ç†

def _compute_hd95_standalone(pred_mask, target_mask):
    """
    ç‹¬ç«‹çš„HD95è®¡ç®—å‡½æ•°ï¼Œä¸ä¾èµ–ç±»å®ä¾‹ï¼Œå¯ç”¨äºå¤šè¿›ç¨‹å¹¶è¡Œå¤„ç†
    
    Args:
        pred_mask: é¢„æµ‹æ©ç  (numpy array)
        target_mask: çœŸå®æ©ç  (numpy array)
    
    Returns:
        HD95å€¼ (float)
    """
    from scipy.ndimage import binary_erosion, distance_transform_edt
    
    pred = pred_mask.astype(bool)
    target = target_mask.astype(bool)
    
    if not pred.any() and not target.any():
        return 0.0
    if not pred.any() or not target.any():
        return np.nan
    
    structure = np.ones((3, 3), dtype=bool)
    pred_border = np.logical_xor(pred, binary_erosion(pred, structure=structure, border_value=0))
    target_border = np.logical_xor(target, binary_erosion(target, structure=structure, border_value=0))
    
    if not pred_border.any():
        pred_border = pred
    if not target_border.any():
        target_border = target
    
    target_distance = distance_transform_edt(~target_border)
    pred_distance = distance_transform_edt(~pred_border)
    
    distances_pred_to_target = target_distance[pred_border]
    distances_target_to_pred = pred_distance[target_border]
    
    all_distances = np.concatenate([distances_pred_to_target, distances_target_to_pred])
    if all_distances.size == 0:
        return 0.0
    return float(np.percentile(all_distances, 95))


def _compute_dice_standalone(pred_mask, target_mask, smooth=1e-7):
    """
    ç‹¬ç«‹çš„Diceè®¡ç®—å‡½æ•°ï¼Œä¸ä¾èµ–ç±»å®ä¾‹ï¼Œå¯ç”¨äºå¤šè¿›ç¨‹å¹¶è¡Œå¤„ç†
    
    Args:
        pred_mask: é¢„æµ‹æ©ç  (numpy array)
        target_mask: çœŸå®æ©ç  (numpy array)
        smooth: å¹³æ»‘ç³»æ•°
    
    Returns:
        Diceå€¼ (float)
    """
    pred = pred_mask.astype(bool)
    target = target_mask.astype(bool)
    intersection = (pred & target).sum()
    union = pred.sum() + target.sum()
    if union == 0:
        return 1.0
    return (2.0 * intersection + smooth) / (union + smooth)


def _ensemble_masks_standalone(mask_list, weights):
    """
    ç‹¬ç«‹çš„é›†æˆå‡½æ•°ï¼Œä¸ä¾èµ–ç±»å®ä¾‹ï¼Œå¯ç”¨äºå¤šè¿›ç¨‹å¹¶è¡Œå¤„ç†
    
    ã€ä»»åŠ¡2ã€‘åƒç´ çº§èåˆå‡½æ•°ä¿®æ­£ï¼šç¡®ä¿ä½¿ç”¨ w1 * mask1 + w2 * mask2
    
    Args:
        mask_list: æ©ç åˆ—è¡¨ï¼ˆçº¯numpyæ•°ç»„åˆ—è¡¨ï¼‰ï¼Œæ”¯æŒ2ä¸ªæ¨¡å‹
        weights: æƒé‡åˆ—è¡¨ï¼ˆçº¯Pythonåˆ—è¡¨ï¼‰ï¼Œé•¿åº¦ä¸º2ï¼Œw1å’Œw2
    
    Returns:
        é›†æˆåçš„æ©ç  (numpy array)
    """
    assert len(mask_list) == len(weights), \
        f"æ©ç æ•°é‡ ({len(mask_list)}) ä¸æƒé‡æ•°é‡ ({len(weights)}) ä¸åŒ¹é…"
    
    # ã€ä»»åŠ¡2ã€‘åŒæ¨¡å‹ä¼˜åŒ–ï¼šç¡®ä¿æƒé‡ä¹‹å’Œä¸º1.0
    if len(weights) == 2:
        w1, w2 = weights[0], weights[1]
        # ç¡®ä¿ w1 + w2 = 1.0
        weight_sum = w1 + w2
        if abs(weight_sum - 1.0) > 1e-6:
            w1 = w1 / weight_sum
            w2 = w2 / weight_sum
        weights = [w1, w2]
    else:
        weight_sum = sum(weights)
        if abs(weight_sum - 1.0) > 1e-6:
            weights = [w / weight_sum for w in weights]
    
    mask_arrays = []
    target_shape = (512, 512)
    import cv2
    
    # ã€ä»»åŠ¡2ã€‘å¼ºåˆ¶ç±»å‹è½¬æ¢ï¼šä¿®å¤ndimæŠ¥é”™
    for i, mask in enumerate(mask_list):
        # å¼ºåˆ¶è½¬æ¢ä¸ºnumpyæ•°ç»„
        if isinstance(mask, list):
            mask = np.array(mask)
        elif isinstance(mask, torch.Tensor):
            mask = mask.detach().cpu().numpy()
        elif not isinstance(mask, np.ndarray) or not hasattr(mask, 'ndim'):
            mask = np.asarray(mask)
        
        # å¤„ç†ç»´åº¦
        if hasattr(mask, 'ndim'):
            if mask.ndim == 3:
                mask = mask[0]
            elif mask.ndim != 2:
                raise ValueError(f"æ©ç  {i} çš„ç»´åº¦ ({mask.ndim}) ä¸æ”¯æŒ")
        else:
            mask = np.asarray(mask)
            if mask.ndim == 3:
                mask = mask[0]
        
        # è°ƒæ•´å°ºå¯¸
        if mask.shape != target_shape:
            mask = cv2.resize(mask.astype(np.float32), 
                            (target_shape[1], target_shape[0]), 
                            interpolation=cv2.INTER_LINEAR)
        
        # å½’ä¸€åŒ–åˆ°[0, 1]
        if mask.max() > 1.0:
            mask = mask / 255.0
        mask = np.clip(mask, 0.0, 1.0)
        mask_arrays.append(mask)
    
    # ã€ä»»åŠ¡2ã€‘åƒç´ çº§èåˆï¼šw1 * mask1 + w2 * mask2
    if len(mask_arrays) == 2:
        ensemble_mask = weights[0] * mask_arrays[0] + weights[1] * mask_arrays[1]
    else:
        ensemble_mask = np.zeros_like(mask_arrays[0], dtype=np.float32)
        for weight, mask in zip(weights, mask_arrays):
            ensemble_mask += weight * mask
    
    return np.clip(ensemble_mask, 0.0, 1.0)


def _ensemble_post_process_standalone(ensemble_mask, use_lcc=True, use_remove_holes=True, min_hole_size=100):
    """
    ç‹¬ç«‹çš„åå¤„ç†å‡½æ•°ï¼Œä¸ä¾èµ–ç±»å®ä¾‹ï¼Œå¯ç”¨äºå¤šè¿›ç¨‹å¹¶è¡Œå¤„ç†
    
    Args:
        ensemble_mask: é›†æˆåçš„æ¦‚ç‡å›¾
        use_lcc: æ˜¯å¦ä½¿ç”¨æœ€å¤§è¿é€šåŸŸï¼ˆå¿…é¡»å¯ç”¨ä»¥ç¡®ä¿HD95ä¼˜åŠ¿ï¼‰
        use_remove_holes: æ˜¯å¦ç§»é™¤å°å­”æ´
        min_hole_size: æœ€å°å­”æ´å¤§å°
    
    Returns:
        å¤„ç†åçš„äºŒå€¼æ©ç 
    """
    from scipy import ndimage
    try:
        from skimage import morphology
        SKIMAGE_AVAILABLE = True
    except ImportError:
        SKIMAGE_AVAILABLE = False
    
    if isinstance(ensemble_mask, torch.Tensor):
        mask_np = ensemble_mask.detach().cpu().numpy()
    else:
        mask_np = np.asarray(ensemble_mask)
    
    if mask_np.ndim > 2:
        mask_np = mask_np.squeeze()
    
    binary_mask = (mask_np > 0.5).astype(np.uint8)
    
    if binary_mask.sum() == 0:
        return binary_mask.astype(np.float32)
    
    # ã€æ ¸å¿ƒä¿®å¤5ã€‘å¼ºåˆ¶æ‰§è¡ŒLCCè¿‡æ»¤ï¼Œç¡®ä¿HD95ä¼˜åŠ¿
    if use_lcc:
        labeled, num_features = ndimage.label(binary_mask)
        if num_features > 0:
            sizes = ndimage.sum(binary_mask, labeled, range(1, num_features + 1))
            largest_label = np.argmax(sizes) + 1
            binary_mask = (labeled == largest_label).astype(np.uint8)
    
    if use_remove_holes and binary_mask.sum() > 0:
        if SKIMAGE_AVAILABLE:
            binary_mask = morphology.remove_small_holes(
                binary_mask.astype(bool), 
                area_threshold=min_hole_size
            ).astype(np.uint8)
        else:
            inverted = (~binary_mask.astype(bool)).astype(np.uint8)
            labeled_holes, num_holes = ndimage.label(inverted)
            if num_holes > 0:
                hole_sizes = ndimage.sum(inverted, labeled_holes, range(1, num_holes + 1))
                small_holes = [i + 1 for i, size in enumerate(hole_sizes) if size < min_hole_size]
                if small_holes:
                    for hole_label in small_holes:
                        binary_mask[labeled_holes == hole_label] = 1
    
    return binary_mask.astype(np.float32)

# ==================== ç‹¬ç«‹å‡½æ•°å®šä¹‰ç»“æŸ ====================

# ==================== ã€ç´§æ€¥ä¿®å¤ã€‘å…¨å±€ç‹¬ç«‹å‡½æ•°ï¼šè§£å†³Pickleé”™è¯¯å’Œå¤šè¿›ç¨‹å†²çª ====================
# å°†é›†æˆç›¸å…³å‡½æ•°ç§»å‡ºTrainThreadç±»ï¼Œå®šä¹‰ä¸ºå…¨å±€ç‹¬ç«‹å‡½æ•°ï¼Œé¿å…PyQt5ä¿¡å·åºåˆ—åŒ–é—®é¢˜

def compute_metrics_worker(mask_tuple, weights, gt_mask):
    """
    å…¨å±€ç‹¬ç«‹çš„å·¥ä½œå‡½æ•°ï¼Œç”¨äºå¤šè¿›ç¨‹å¹¶è¡Œè®¡ç®—æŒ‡æ ‡
    
    Args:
        mask_tuple: (sample_idx, sample_masks) å…ƒç»„ï¼Œå…¶ä¸­sample_masksæ˜¯å¤šä¸ªæ¨¡å‹çš„æ©ç åˆ—è¡¨
        weights: æƒé‡åˆ—è¡¨
        gt_mask: çœŸå®æ©ç 
    
    Returns:
        (dice, hd95): Diceå’ŒHD95å€¼
    """
    sample_idx, sample_masks = mask_tuple
    
    # ã€ä»»åŠ¡4ã€‘å¼ºåˆ¶æ•°æ®ç±»å‹è½¬æ¢
    sample_masks = [np.array(m) if not hasattr(m, 'ndim') else m for m in sample_masks]
    gt_mask = np.array(gt_mask) if not hasattr(gt_mask, 'ndim') else gt_mask
    
    # ä½¿ç”¨å…¨å±€ç‹¬ç«‹å‡½æ•°è¿›è¡Œé›†æˆ
    ensemble_mask = ensemble_masks_global(sample_masks, weights)
    
    # ã€æè‡´åå¤„ç†æµæ°´çº¿ã€‘å¿…é¡»æ‰§è¡Œä¸‰æ­¥åå¤„ç†
    ensemble_mask = ensemble_post_process_global(
        ensemble_mask,
        use_lcc=True,  # ã€ç¬¬ä¸€æ­¥ã€‘ä¿ç•™æœ€å¤§è¿é€šåŸŸï¼Œå½»åº•åˆ‡é™¤ç¦»ç¾¤å™ªç‚¹
        use_remove_holes=True,  # ã€ç¬¬äºŒæ­¥ã€‘å¡«è¡¥å°å­”æ´ï¼Œæå‡Diceçº¦0.5%
        min_hole_size=100,
        use_edge_smoothing=True  # ã€ç¬¬ä¸‰æ­¥ã€‘è¾¹ç¼˜å¹³æ»‘ï¼Œä¿®æ­£é”¯é½¿è¾¹ç¼˜
    )
    
    # è®¡ç®—æŒ‡æ ‡
    dice = _compute_dice_standalone(ensemble_mask, gt_mask)
    hd95 = _compute_hd95_standalone(ensemble_mask, gt_mask)
    
    return dice, hd95


def ensemble_masks_global(mask_list, weights):
    """
    å¤šå°ºåº¦æ¦‚ç‡å›¾é›†æˆï¼šåƒç´ çº§åŠ æƒèåˆï¼ˆæ”¯æŒä»»æ„æ•°é‡Nä¸ªæ¨¡å‹ï¼‰
    
    å°†å¤šä¸ªä¸åŒåˆ†è¾¨ç‡çš„æ¦‚ç‡å›¾ï¼ˆæˆ–äºŒå€¼æ©ç ï¼‰è¿›è¡ŒåŠ æƒèåˆï¼Œåˆ©ç”¨512æ¨¡å‹çš„ç²¾ç»†åº¦ä¿®æ­£224æ¨¡å‹çš„ç²—ç³™è¾¹ç¼˜ã€‚
    
    Args:
        mask_list: æ©ç åˆ—è¡¨ï¼ˆList[numpy.ndarray | torch.Tensor]ï¼‰ï¼Œæ¯ä¸ªå…ƒç´ å¯ä»¥æ˜¯ï¼š
                  - numpy array (H, W) æˆ– (C, H, W) - æ¦‚ç‡å›¾æˆ–äºŒå€¼æ©ç 
                  - torch.Tensor (H, W) æˆ– (C, H, W) - æ¦‚ç‡å›¾æˆ–äºŒå€¼æ©ç 
        weights: æƒé‡åˆ—è¡¨ï¼ˆList[float]ï¼‰ï¼Œé•¿åº¦å¿…é¡»ä¸ mask_list ç›¸åŒï¼Œä¸”æƒé‡ä¹‹å’Œåº”ä¸º1.0
    
    Returns:
        ensemble_mask: èåˆåçš„æ¦‚ç‡å›¾ (numpy array, H x W)
    
    Raises:
        ValueError: å¦‚æœæ©ç æ•°é‡ä¸æƒé‡æ•°é‡ä¸åŒ¹é…
    """
    # ã€æ ¸å¿ƒä¿®å¤ã€‘åŠ¨æ€æ£€æŸ¥ï¼šç¡®ä¿æ•°é‡ä¸¥æ ¼å¯¹é½
    assert len(mask_list) == len(weights), \
        f"æ©ç æ•°é‡ ({len(mask_list)}) ä¸æƒé‡æ•°é‡ ({len(weights)}) ä¸åŒ¹é…"
    
    # ã€ä»»åŠ¡4ã€‘å¼ºåˆ¶æ•°æ®ç±»å‹è½¬æ¢ï¼šè§£å†³ndimé”™è¯¯
    mask_list = [np.array(m) if not hasattr(m, 'ndim') else m for m in mask_list]
    
    # æƒé‡å½’ä¸€åŒ–ï¼ˆå¦‚æœæƒé‡ä¹‹å’Œä¸ä¸º1.0ï¼‰
    weight_sum = sum(weights)
    if abs(weight_sum - 1.0) > 1e-6:
        print(f"âš ï¸  è­¦å‘Š: æƒé‡ä¹‹å’Œ ({weight_sum:.6f}) ä¸ç­‰äº 1.0ï¼Œå°†è‡ªåŠ¨å½’ä¸€åŒ–")
        weights = [w / weight_sum for w in weights]
    
    # ã€æ ¸å¿ƒä¿®å¤ã€‘å¼ºåˆ¶ç±»å‹è½¬æ¢ï¼šç¡®ä¿æ‰€æœ‰æ©ç éƒ½æ˜¯numpyæ•°ç»„
    mask_arrays = []
    target_shape = (512, 512)  # å¼ºåˆ¶ä½¿ç”¨512x512ä½œä¸ºç›®æ ‡å°ºå¯¸
    
    import cv2
    
    for i, mask in enumerate(mask_list):
        # å¼ºåˆ¶è½¬æ¢ä¸ºnumpyæ•°ç»„
        if isinstance(mask, list):
            mask = np.array(mask)
        elif isinstance(mask, torch.Tensor):
            mask = mask.detach().cpu().numpy()
        elif not isinstance(mask, np.ndarray) or not hasattr(mask, 'ndim'):
            mask = np.asarray(mask)
        
        # å¤„ç†ç»´åº¦ï¼šå¦‚æœæ˜¯ (C, H, W)ï¼Œå–ç¬¬ä¸€ä¸ªé€šé“
        if hasattr(mask, 'ndim'):
            if mask.ndim == 3:
                mask = mask[0]  # å–ç¬¬ä¸€ä¸ªé€šé“
            elif mask.ndim != 2:
                raise ValueError(f"æ©ç  {i} çš„ç»´åº¦ ({mask.ndim}) ä¸æ”¯æŒï¼Œåº”ä¸º 2D (H, W) æˆ– 3D (C, H, W)")
        
        # ã€å…³é”®ä¿®å¤ã€‘å¼ºåˆ¶æ‰€æœ‰æ¦‚ç‡å›¾å¯¹é½åˆ°512x512ï¼Œä½¿ç”¨bilinearæ’å€¼
        if mask.shape != target_shape:
            mask = cv2.resize(
                mask.astype(np.float32), 
                (target_shape[1], target_shape[0]),  # (width, height)
                interpolation=cv2.INTER_LINEAR  # ä½¿ç”¨bilinearæ’å€¼
            )
        
        # ç¡®ä¿å€¼åœ¨ [0, 1] èŒƒå›´å†…
        if mask.max() > 1.0:
            mask = mask / 255.0
        mask = np.clip(mask, 0.0, 1.0)
        
        mask_arrays.append(mask)
    
    # ã€ä»»åŠ¡2ã€‘åƒç´ çº§èåˆï¼šw1 * mask1 + w2 * mask2ï¼ˆåŒæ¨¡å‹ä¼˜åŒ–ï¼‰
    if len(mask_arrays) == 2:
        ensemble_mask = weights[0] * mask_arrays[0] + weights[1] * mask_arrays[1]
    else:
        # ã€æ ¸å¿ƒä¿®å¤ã€‘åƒç´ çº§åŠ æƒèåˆï¼šä½¿ç”¨åŠ¨æ€å¾ªç¯ï¼Œæ”¯æŒä»»æ„æ•°é‡æ¨¡å‹
        ensemble_mask = np.zeros_like(mask_arrays[0], dtype=np.float32)
        for weight, mask in zip(weights, mask_arrays):
            ensemble_mask += weight * mask
    
    # ç¡®ä¿å€¼åœ¨ [0, 1] èŒƒå›´å†…
    ensemble_mask = np.clip(ensemble_mask, 0.0, 1.0)
    
    return ensemble_mask


def ensemble_post_process_global(ensemble_mask, use_lcc=True, use_remove_holes=True, 
                                 min_hole_size=100, use_edge_smoothing=True):
    """
    ã€æè‡´åå¤„ç†æµæ°´çº¿ã€‘é›†æˆåå¤„ç†ï¼šå¯¹èåˆåçš„æ¦‚ç‡å›¾è¿›è¡Œåå¤„ç†
    
    ä¸‰æ­¥æµæ°´çº¿ï¼š
    1. Largest Connected Component (LCC): ä¿ç•™æœ€å¤§è¿é€šåŸŸï¼Œå½»åº•åˆ‡é™¤ç¦»ç¾¤å™ªç‚¹
    2. remove_small_holes: å¡«è¡¥å°å­”æ´ï¼Œæå‡Diceçº¦0.5%
    3. è¾¹ç¼˜å¹³æ»‘: å¾®å°è…èš€+è†¨èƒ€ï¼Œä¿®æ­£é”¯é½¿è¾¹ç¼˜
    
    Args:
        ensemble_mask: èåˆåçš„æ¦‚ç‡å›¾ (numpy array, H x W)
        use_lcc: æ˜¯å¦ä½¿ç”¨æœ€å¤§è¿é€šåŸŸ
        use_remove_holes: æ˜¯å¦ç§»é™¤å°å­”æ´
        min_hole_size: æœ€å°å­”æ´å¤§å°ï¼ˆåƒç´ ï¼‰ï¼Œå°äºæ­¤å€¼çš„å­”æ´å°†è¢«å¡«è¡¥
        use_edge_smoothing: æ˜¯å¦ä½¿ç”¨è¾¹ç¼˜å¹³æ»‘ï¼ˆè…èš€+è†¨èƒ€ï¼‰
    
    Returns:
        processed_mask: å¤„ç†åçš„äºŒå€¼æ©ç  (numpy array, H x W, 0-1)
    """
    from scipy import ndimage
    from scipy.ndimage import binary_erosion, binary_dilation
    try:
        from skimage import morphology
        SKIMAGE_AVAILABLE = True
    except ImportError:
        SKIMAGE_AVAILABLE = False
    
    # ç¡®ä¿æ˜¯numpyæ•°ç»„
    if isinstance(ensemble_mask, torch.Tensor):
        mask_np = ensemble_mask.detach().cpu().numpy()
    else:
        mask_np = np.asarray(ensemble_mask)
    
    # ç¡®ä¿æ˜¯2D
    if mask_np.ndim > 2:
        mask_np = mask_np.squeeze()
    
    # äºŒå€¼åŒ–ï¼ˆä½¿ç”¨0.5ä½œä¸ºé˜ˆå€¼ï¼‰
    binary_mask = (mask_np > 0.5).astype(np.uint8)
    
    if binary_mask.sum() == 0:
        return binary_mask.astype(np.float32)
    
    # ã€ç¬¬ä¸€æ­¥ï¼šLCC è¿‡æ»¤ã€‘ä¿ç•™æœ€å¤§è¿é€šåŸŸï¼Œå½»åº•åˆ‡é™¤ç¦»ç¾¤å™ªç‚¹
    if use_lcc:
        labeled, num_features = ndimage.label(binary_mask)
        if num_features > 0:
            # è®¡ç®—æ¯ä¸ªè¿é€šåŸŸçš„å¤§å°
            sizes = ndimage.sum(binary_mask, labeled, range(1, num_features + 1))
            # æ‰¾åˆ°æœ€å¤§çš„è¿é€šåŸŸ
            largest_label = np.argmax(sizes) + 1
            # åªä¿ç•™æœ€å¤§è¿é€šåŸŸ
            binary_mask = (labeled == largest_label).astype(np.uint8)
    
    # ã€ç¬¬äºŒæ­¥ï¼šç©ºæ´å¡«å……ã€‘å¡«è¡¥å°å­”æ´ï¼Œæå‡Diceçº¦0.5%
    if use_remove_holes and binary_mask.sum() > 0:
        if SKIMAGE_AVAILABLE:
            # ä½¿ç”¨skimage.morphology.remove_small_holesï¼ˆæ›´ç²¾ç¡®ï¼‰
            binary_mask = morphology.remove_small_holes(
                binary_mask.astype(bool), 
                area_threshold=min_hole_size
            ).astype(np.uint8)
        else:
            # ä½¿ç”¨scipyå®ç°ï¼ˆå›é€€æ–¹æ¡ˆï¼‰
            # åè½¬æ©ç ï¼Œæ‰¾åˆ°å­”æ´ï¼ˆèƒŒæ™¯ä¸­çš„è¿é€šåŸŸï¼‰
            inverted = (~binary_mask.astype(bool)).astype(np.uint8)
            labeled_holes, num_holes = ndimage.label(inverted)
            if num_holes > 0:
                # è®¡ç®—æ¯ä¸ªå­”æ´çš„å¤§å°
                hole_sizes = ndimage.sum(inverted, labeled_holes, range(1, num_holes + 1))
                # æ‰¾åˆ°éœ€è¦å¡«è¡¥çš„å°å­”æ´
                small_holes = []
                for i, size in enumerate(hole_sizes):
                    if size < min_hole_size:
                        small_holes.append(i + 1)
                # å¡«è¡¥å°å­”æ´
                if small_holes:
                    for hole_label in small_holes:
                        binary_mask[labeled_holes == hole_label] = 1
    
    # ã€ç¬¬ä¸‰æ­¥ï¼šè¾¹ç¼˜å¹³æ»‘ã€‘å¾®å°è…èš€+è†¨èƒ€ï¼Œä¿®æ­£é”¯é½¿è¾¹ç¼˜
    if use_edge_smoothing and binary_mask.sum() > 0:
        # ä½¿ç”¨3x3ç»“æ„å…ƒç´ è¿›è¡Œå¾®å°è…èš€ï¼ˆå»é™¤ç»†å°çªèµ·ï¼‰
        structure = np.ones((3, 3), dtype=bool)
        binary_mask = binary_erosion(binary_mask.astype(bool), structure=structure, iterations=1).astype(np.uint8)
        # ä½¿ç”¨3x3ç»“æ„å…ƒç´ è¿›è¡Œè†¨èƒ€ï¼ˆæ¢å¤å¤§è‡´å½¢çŠ¶ï¼Œä½†è¾¹ç¼˜æ›´å¹³æ»‘ï¼‰
        binary_mask = binary_dilation(binary_mask.astype(bool), structure=structure, iterations=1).astype(np.uint8)
    
    return binary_mask.astype(np.float32)


def calculate_official_total_score_global(dice, iou, hd95, sensitivity, specificity):
    """
    è®¡ç®—æ¯”èµ›å®˜æ–¹æ€»åˆ†å…¬å¼ï¼š
    Total = 0.6*Dice + 0.1*IoU + 0.1/(1+HD95) + 0.1*Sens + 0.1*Spec
    
    Args:
        dice: Diceç³»æ•°
        iou: IoUç³»æ•°
        hd95: HD95å€¼ï¼ˆå¦‚æœä¸ºNaNæˆ–Infï¼Œåˆ™ä½¿ç”¨ä¸€ä¸ªå¾ˆå¤§çš„å€¼ï¼‰
        sensitivity: æ•æ„Ÿåº¦ï¼ˆå¬å›ç‡ï¼‰
        specificity: ç‰¹å¼‚æ€§
    
    Returns:
        æ€»åˆ†
    """
    # å¤„ç†HD95çš„NaN/Infæƒ…å†µ
    if np.isnan(hd95) or np.isinf(hd95):
        hd95_term = 0.0  # å¦‚æœHD95ä¸å¯è®¡ç®—ï¼Œè¯¥é¡¹ä¸º0
    else:
        hd95_term = 0.1 / (1.0 + hd95)
    
    total_score = (
        0.6 * dice +
        0.1 * iou +
        hd95_term +
        0.1 * sensitivity +
        0.1 * specificity
    )
    return total_score


def find_optimal_ensemble_weights_global(mask_list, gt_masks, weight_range=(0.0, 1.0, 0.1),
                                         hd95_threshold=3.0, device=None, search_samples=100, 
                                         use_parallel=True, n_jobs=4):
    """
    å¯»æ‰¾æœ€ä¼˜é›†æˆæƒé‡ï¼Œä½¿å¾—éªŒè¯é›†ä¸Šçš„ Dice æå‡ä¸” HD95 ä¿æŒåœ¨é˜ˆå€¼ä»¥å†…
    
    Args:
        mask_list: æ©ç åˆ—è¡¨ï¼ˆå¤šä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœï¼‰
        gt_masks: çœŸå®æ©ç åˆ—è¡¨ï¼ˆground truthï¼‰
        weight_range: æƒé‡æœç´¢èŒƒå›´ (min, max, step)
        hd95_threshold: HD95 é˜ˆå€¼ï¼Œé»˜è®¤ 3.0
        device: è®¡ç®—è®¾å¤‡ï¼ˆç”¨äºè®¡ç®—HD95ï¼‰
        search_samples: éšæœºé‡‡æ ·æ•°é‡ï¼Œé»˜è®¤100ï¼ˆç”¨äºåŠ é€Ÿæœç´¢ï¼‰
        use_parallel: æ˜¯å¦ä½¿ç”¨å¹¶è¡Œå¤„ç†ï¼Œé»˜è®¤True
        n_jobs: å¹¶è¡Œä»»åŠ¡æ•°ï¼Œ-1è¡¨ç¤ºä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
    
    Returns:
        best_weights: æœ€ä¼˜æƒé‡åˆ—è¡¨
        best_metrics: æœ€ä¼˜æŒ‡æ ‡å­—å…¸ {'dice': float, 'hd95': float, 'total_score': float}
    """
    import gc
    import random
    from scipy.ndimage import binary_erosion, distance_transform_edt
    
    # å°è¯•å¯¼å…¥joblibç”¨äºå¹¶è¡Œå¤„ç†
    try:
        from joblib import Parallel, delayed
        JOBLIB_AVAILABLE = True
    except ImportError:
        JOBLIB_AVAILABLE = False
        if use_parallel:
            print("âš ï¸  è­¦å‘Š: joblibæœªå®‰è£…ï¼Œå°†ä½¿ç”¨å•è¿›ç¨‹æ¨¡å¼ã€‚å»ºè®®å®‰è£…: pip install joblib")
    
    # ã€ä»»åŠ¡4ã€‘å¼ºåˆ¶æ•°æ®ç±»å‹è½¬æ¢ï¼šè§£å†³ndimé”™è¯¯
    # ã€æ ¸å¿ƒä¿®å¤2ã€‘å½»åº•è§£å†³æ•°æ®ç±»å‹å¼‚å¸¸ï¼šå¼ºåˆ¶ç±»å‹è½¬æ¢
    # æ³¨æ„ï¼šmask_listå¯èƒ½æ˜¯åµŒå¥—åˆ—è¡¨ï¼Œéœ€è¦é€’å½’å¤„ç†
    converted_mask_list = []
    for model_idx, model_masks in enumerate(mask_list):
        if isinstance(model_masks, list):
            converted_model_masks = []
            for mask_idx, mask in enumerate(model_masks):
                # å¼ºåˆ¶ç±»å‹è½¬æ¢ï¼šç¡®ä¿æ˜¯numpyæ•°ç»„
                if isinstance(mask, list):
                    mask = np.array(mask)
                elif isinstance(mask, torch.Tensor):
                    mask = mask.detach().cpu().numpy()
                elif not isinstance(mask, np.ndarray) or not hasattr(mask, 'ndim'):
                    mask = np.asarray(mask)
                converted_model_masks.append(mask)
            converted_mask_list.append(converted_model_masks)
        else:
            # å¦‚æœå·²ç»æ˜¯æ•°ç»„ï¼Œä¹Ÿè¦æ£€æŸ¥
            if not isinstance(model_masks, np.ndarray) or not hasattr(model_masks, 'ndim'):
                converted_mask_list.append(np.asarray(model_masks))
            else:
                converted_mask_list.append(model_masks)
    
    mask_list = converted_mask_list
    
    # ã€æ ¸å¿ƒä¿®å¤2ç»­ã€‘ç¡®ä¿mask_listä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½æœ‰ndimå±æ€§
    for model_idx, model_masks in enumerate(mask_list):
        if isinstance(model_masks, list):
            for mask_idx, mask in enumerate(model_masks):
                if not hasattr(mask, 'ndim'):
                    mask_list[model_idx][mask_idx] = np.asarray(mask)
    
    # åŒæ ·å¤„ç†gt_masks
    if isinstance(gt_masks, list):
        converted_gt_masks = []
        for mask in gt_masks:
            if isinstance(mask, list):
                mask = np.array(mask)
            elif isinstance(mask, torch.Tensor):
                mask = mask.detach().cpu().numpy()
            elif not isinstance(mask, np.ndarray):
                mask = np.asarray(mask)
            converted_gt_masks.append(mask)
        gt_masks = converted_gt_masks
        
        num_models = len(mask_list)
        if num_models < 1:
            raise ValueError("è‡³å°‘éœ€è¦1ä¸ªæ¨¡å‹è¿›è¡Œé›†æˆ")
        
        # ã€å†›ä»¤çŠ¶ï¼šæè‡´æé€Ÿã€‘å¼ºåˆ¶é‡‡æ ·ç­–ç•¥ï¼šæœç´¢é˜¶æ®µåªä½¿ç”¨100å¼ å›¾ç‰‡
        total_samples = len(gt_masks)
        search_samples_fixed = 100  # å¼ºåˆ¶å›ºå®šä¸º100å¼ ï¼Œç¡®ä¿æœç´¢é€Ÿåº¦ï¼ˆä»112ç§’/ité™è‡³3ç§’/itï¼‰
        
        # ä¿å­˜åŸå§‹æ•°æ®ç”¨äºç»ˆæ•ˆè¯„ä¼°
        original_mask_list = mask_list
        original_gt_masks = gt_masks
        
        if search_samples_fixed < total_samples:
            # ã€å†›ä»¤çŠ¶ã€‘å‡åŒ€é‡‡æ ·100å¼ å›¾ç‰‡ï¼ˆç¡®ä¿ç»Ÿè®¡åˆ†å¸ƒä»£è¡¨æ€§ï¼‰
            if total_samples <= search_samples_fixed:
                sample_indices = list(range(total_samples))
            else:
                # å‡åŒ€é‡‡æ ·ï¼šæ¯éš” total_samples/search_samples_fixed å–ä¸€å¼ 
                step = total_samples / search_samples_fixed
                sample_indices = [int(i * step) for i in range(search_samples_fixed)]
                # ç¡®ä¿æœ€åä¸€ä¸ªç´¢å¼•ä¸è¶…è¿‡èŒƒå›´
                sample_indices = [min(idx, total_samples - 1) for idx in sample_indices]
                # å»é‡å¹¶æ’åº
                sample_indices = sorted(list(set(sample_indices)))
            
            print(f"ğŸš€ ã€æè‡´æé€Ÿã€‘é‡‡æ ·ç­–ç•¥: ä» {total_samples} å¼ å›¾ç‰‡ä¸­å‡åŒ€æŠ½å– {len(sample_indices)} å¼ è¿›è¡Œæƒé‡æœç´¢")
            print(f"   é¢„æœŸæé€Ÿ: ä» ~112ç§’/it é™è‡³ ~3ç§’/it (æé€Ÿçº¦ {100*(1-100/total_samples):.1f}%)")
            
            sampled_mask_list = []
            for model_masks in mask_list:
                if isinstance(model_masks, list):
                    sampled_mask_list.append([model_masks[i] for i in sample_indices])
                else:
                    sampled_mask_list.append(model_masks[sample_indices] if hasattr(model_masks, '__getitem__') else model_masks)
            sampled_gt_masks = [gt_masks[i] for i in sample_indices]
            mask_list = sampled_mask_list
            gt_masks = sampled_gt_masks
            print(f"âœ… é‡‡æ ·å®Œæˆï¼Œå®é™…ä½¿ç”¨ {len(gt_masks)} å¼ å›¾ç‰‡è¿›è¡Œæœç´¢")
        else:
            print(f"ğŸ“Š ä½¿ç”¨å…¨é‡ {total_samples} å¼ å›¾ç‰‡è¿›è¡Œæƒé‡æœç´¢ï¼ˆæ•°æ®é‡è¾ƒå°ï¼‰")
        
        # ã€ä»»åŠ¡2ã€‘åŠ¨æ€æƒé‡ç”Ÿæˆï¼šæ£€æµ‹Nä¸ªæ¨¡å‹ï¼Œè‡ªåŠ¨é€‚é…æœç´¢ç­–ç•¥
        min_w, max_w, step_w = weight_range
        
        # ç”Ÿæˆæ‰€æœ‰æƒé‡ç»„åˆ
        if num_models == 1:
            weight_combinations = [[1.0]]
        elif num_models == 2:
            # ã€ä»»åŠ¡2ã€‘N=2æ—¶ï¼Œè‡ªåŠ¨åˆ‡æ¢ä¸ºä¸€ç»´æœç´¢ï¼šw1ä»0åˆ°1ï¼Œw2 = 1.0 - w1
            weight_combinations = []
            for w1 in np.arange(0.0, 1.0 + step_w, step_w):
                w1 = round(w1, 2)
                w2 = round(1.0 - w1, 2)
                weight_combinations.append([w1, w2])
            print(f"âœ… åŒæ¨¡å‹ä¸€ç»´æœç´¢ï¼šç”Ÿæˆ {len(weight_combinations)} ç§æƒé‡ç»„åˆï¼ˆw1: 0.0-1.0, æ­¥é•¿: {step_w}ï¼‰")
        else:
            # ã€ä»»åŠ¡2ã€‘N>2æ—¶ï¼Œä½¿ç”¨itertools.productç”Ÿæˆæ­¥é•¿ä¸º0.1çš„æƒé‡ç»„åˆ
            import itertools
            # ä½¿ç”¨0.1æ­¥é•¿ç”Ÿæˆæƒé‡ç»„åˆï¼ˆè€Œä¸æ˜¯ä½¿ç”¨step_wï¼Œé¿å…ç»„åˆæ•°è¿‡å¤šï¼‰
            weight_steps = np.arange(min_w, max_w + 0.1, 0.1)
            weight_steps = [round(w, 1) for w in weight_steps]
            
            all_combinations = list(itertools.product(weight_steps, repeat=num_models))
            
            weight_combinations = []
            for combo in all_combinations:
                combo_sum = sum(combo)
                if combo_sum > 0:
                    # ã€ä»»åŠ¡2ã€‘ç¡®ä¿sum(weights)å½’ä¸€åŒ–ä¸º1.0
                    normalized = [round(w / combo_sum, 2) for w in combo]
                    if all(min_w <= w <= max_w for w in normalized):
                        weight_combinations.append(normalized)
            
            if len(weight_combinations) > 10000:
                print(f"âš ï¸  è­¦å‘Š: æƒé‡ç»„åˆæ•°é‡è¿‡å¤š ({len(weight_combinations)})ï¼Œä½¿ç”¨é‡‡æ ·ç­–ç•¥ï¼ˆæ¯10ä¸ªå–1ä¸ªï¼‰")
                weight_combinations = weight_combinations[::10]
            
            # å»é‡
            unique_combinations = []
            seen = set()
            for combo in weight_combinations:
                combo_tuple = tuple(combo)
                if combo_tuple not in seen:
                    seen.add(combo_tuple)
                    unique_combinations.append(combo)
            weight_combinations = unique_combinations
        
        print(f"ğŸ” å¼€å§‹æœç´¢æœ€ä¼˜é›†æˆæƒé‡...")
        print(f"   æ¨¡å‹æ•°é‡: {num_models}")
        print(f"   æƒé‡æœç´¢èŒƒå›´: [{min_w}, {max_w}], æ­¥é•¿: {step_w}")
        print(f"   æ€»ç»„åˆæ•°: {len(weight_combinations)}")
        print(f"   HD95 é˜ˆå€¼: {hd95_threshold}")
        
        best_score = -1.0
        best_weights = None
        best_metrics = None
        
        # ã€å†›ä»¤çŠ¶ã€‘å½»åº•ç‰©ç†éš”ç¦»ï¼šå°†æ‰€æœ‰æ•°æ®è½¬æ¢ä¸ºnumpyæ•°ç»„ï¼Œå‡†å¤‡ä¼ å…¥Parallel
        # ç¡®ä¿mask_listå’Œgt_maskséƒ½æ˜¯çº¯numpyæ•°ç»„ï¼Œæ²¡æœ‰ä»»ä½•ç±»å¼•ç”¨
        final_mask_list = []
        for model_masks in mask_list:
            if isinstance(model_masks, list):
                # è½¬æ¢ä¸ºnumpyæ•°ç»„
                model_array = np.array([np.array(m) if not isinstance(m, np.ndarray) else m for m in model_masks])
            elif isinstance(model_masks, np.ndarray):
                model_array = model_masks
            else:
                model_array = np.array(model_masks)
            final_mask_list.append(model_array)
        
        final_gt_masks = []
        for gt in gt_masks:
            if isinstance(gt, np.ndarray):
                final_gt_masks.append(gt)
            else:
                final_gt_masks.append(np.array(gt))
        
        # ã€å†›ä»¤çŠ¶ã€‘å½»åº•ç‰©ç†éš”ç¦»ï¼šä½¿ç”¨Parallelå’Œdelayedè¿›è¡ŒçœŸæ­£çš„å¹¶è¡Œè®¡ç®—
        total_combinations = len(weight_combinations)
        
        # ç¡®å®šæ˜¯å¦ä½¿ç”¨å¹¶è¡Œå¤„ç†
        actual_n_jobs = 1
        if use_parallel and JOBLIB_AVAILABLE and len(final_gt_masks) > 10:
            actual_n_jobs = min(n_jobs if n_jobs > 0 else 4, 4)
            print(f"ğŸš€ å¯ç”¨å¹¶è¡Œå¤„ç†: {actual_n_jobs} ä¸ªè¿›ç¨‹")
        else:
            print(f"ğŸ“ ä½¿ç”¨ä¸²è¡Œå¤„ç†")
        
        # ã€å†›ä»¤çŠ¶ï¼šæè‡´æé€Ÿã€‘ä½¿ç”¨tqdmå®ç°å®æ—¶è¿›åº¦å¯è§†åŒ–
        from tqdm import tqdm
        
        # åˆ›å»ºä¸»è¿›åº¦æ¡ï¼ˆæ˜¾ç¤ºæ•´ä½“è¿›åº¦å’Œæœ€ä½³ç»“æœï¼‰
        main_pbar = tqdm(
            total=total_combinations,
            desc="ğŸ” æƒé‡æœç´¢",
            unit="ç»„åˆ",
            bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}] | æœ€ä½³: {postfix}'
        )
        
        # åˆå§‹åŒ–æœ€ä½³ç»“æœæ˜¾ç¤º
        best_display = "ç­‰å¾…ä¸­..."
        main_pbar.set_postfix_str(best_display)
        
        # ã€å†›ä»¤çŠ¶ã€‘åˆ†æ‰¹å¹¶è¡Œå¤„ç†ï¼šæ¯æ¬¡å¤„ç†ä¸€æ‰¹æƒé‡ç»„åˆï¼Œé¿å…å†…å­˜æº¢å‡º
        batch_size = 50  # æ¯æ‰¹å¤„ç†50ä¸ªæƒé‡ç»„åˆ
        processed_count = 0
        
        for batch_start in range(0, total_combinations, batch_size):
            batch_end = min(batch_start + batch_size, total_combinations)
            batch_weights = weight_combinations[batch_start:batch_end]
            
            # å¹¶è¡Œå¤„ç†å½“å‰æ‰¹æ¬¡
            if actual_n_jobs > 1:
                try:
                    # ã€æ ¸å¿ƒä¿®å¤ã€‘ä½¿ç”¨Parallelå’Œdelayedè¿›è¡ŒçœŸæ­£çš„å¹¶è¡Œè®¡ç®—
                    # ã€12ç‚¹å†›ä»¤çŠ¶ä»»åŠ¡ã€‘ä½¿ç”¨æ–°çš„ calculate_metrics_for_weights å‡½æ•°ï¼ˆåŒ…å«LCCåå¤„ç†ï¼‰
                    batch_results = Parallel(n_jobs=actual_n_jobs)(
                        delayed(calculate_metrics_for_weights)(w, final_mask_list, final_gt_masks) 
                        for w in batch_weights
                    )
                except Exception as e:
                    print(f"\nâš ï¸  å¹¶è¡Œè®¡ç®—é”™è¯¯: {e}ï¼Œå›é€€åˆ°ä¸²è¡Œæ¨¡å¼")
                    batch_results = [
                        calculate_metrics_for_weights(w, final_mask_list, final_gt_masks) 
                        for w in batch_weights
                    ]
            else:
                # ä¸²è¡Œå¤„ç†ï¼ˆå›é€€æ–¹æ¡ˆï¼‰
                batch_results = [
                    calculate_metrics_for_weights(w, final_mask_list, final_gt_masks) 
                    for w in batch_weights
                ]
            
            # å¤„ç†å½“å‰æ‰¹æ¬¡çš„ç»“æœ
            for weight_idx_in_batch, (weights, result) in enumerate(zip(batch_weights, batch_results)):
                weight_idx = batch_start + weight_idx_in_batch
                total_score, avg_dice, avg_hd95, normalized_weights = result
                
                # æ£€æŸ¥HD95çº¦æŸ
                if not np.isnan(avg_hd95) and avg_hd95 > hd95_threshold:
                    processed_count += 1
                    main_pbar.update(1)
                    continue
                
                # æ›´æ–°æœ€ä½³ç»“æœ
                if total_score > best_score:
                    best_score = total_score
                    best_weights = weights
                    best_metrics = {
                        'dice': avg_dice,
                        'hd95': avg_hd95,
                        'total_score': total_score
                    }
                    # ã€å®æ—¶å¯è§†åŒ–ã€‘æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤ºçš„æœ€ä½³ç»“æœ
                    best_display = f"Dice={best_metrics['dice']:.4f}, HD95={best_metrics['hd95']:.4f}, Score={best_metrics['total_score']:.4f}, W={best_weights}"
                    main_pbar.set_postfix_str(best_display)
                    # ã€å®æ—¶æ‰“å°ã€‘æ§åˆ¶å°è¾“å‡ºå½“å‰æœ€ä½³ç»“æœ
                    print(f"\nğŸ¯ å½“å‰æœ€ä½³æƒé‡: {best_weights}, å½“å‰æœ€é«˜åˆ†: {best_metrics['total_score']:.4f} (Dice={best_metrics['dice']:.4f}, HD95={best_metrics['hd95']:.4f})")
                
                processed_count += 1
                main_pbar.update(1)
            
            # ã€æ€§èƒ½ä¼˜åŒ–5ã€‘å†…å­˜é‡Šæ”¾ï¼šæ¯å¤„ç†ä¸€æ‰¹åé‡Šæ”¾å†…å­˜
            del batch_results
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        
        # å…³é—­ä¸»è¿›åº¦æ¡
        main_pbar.close()
        
        if best_weights is None:
            print("âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°æ»¡è¶³HD95çº¦æŸçš„æƒé‡ç»„åˆ")
            # è¿”å›å¹³å‡æƒé‡ä½œä¸ºé»˜è®¤å€¼
            best_weights = [1.0 / num_models] * num_models
            best_metrics = {'dice': 0.0, 'hd95': np.nan, 'total_score': 0.0}
        else:
            print(f"\nâœ… ã€é‡‡æ ·æœç´¢å®Œæˆã€‘æ‰¾åˆ°æœ€ä¼˜æƒé‡ç»„åˆ:")
            print(f"   æƒé‡: {best_weights}")
            print(f"   Dice: {best_metrics['dice']:.4f}")
            print(f"   HD95: {best_metrics['hd95']:.4f}")
            print(f"   Total Score: {best_metrics['total_score']:.4f}")
            print(f"   Scoreå…¬å¼: 0.6 * Dice + 0.1 / (1 + HD95)")
        
        # ã€å†›ä»¤çŠ¶ï¼šç»ˆæ•ˆè¯„ä¼°ã€‘ç”¨æœ€ä¼˜æƒé‡è·‘å…¨é‡æ•°æ®
        if best_weights is not None and len(original_gt_masks) > len(final_gt_masks):
            print(f"\nğŸ¯ ã€ç»ˆæ•ˆè¯„ä¼°ã€‘ä½¿ç”¨æœ€ä¼˜æƒé‡å¯¹å…¨é‡ {len(original_gt_masks)} å¼ å›¾ç‰‡è¿›è¡Œæœ€ç»ˆè¯„ä¼°...")
            
            # å‡†å¤‡å…¨é‡æ•°æ®
            final_full_mask_list = []
            for model_masks in original_mask_list:
                if isinstance(model_masks, list):
                    final_full_mask_list.append(np.array([np.array(m) if not isinstance(m, np.ndarray) else m for m in model_masks]))
                elif isinstance(model_masks, np.ndarray):
                    final_full_mask_list.append(model_masks)
                else:
                    final_full_mask_list.append(np.array(model_masks))
            
            final_full_gt_masks = []
            for gt in original_gt_masks:
                if isinstance(gt, np.ndarray):
                    final_full_gt_masks.append(gt)
                else:
                    final_full_gt_masks.append(np.array(gt))
            
            # ä½¿ç”¨æœ€ä¼˜æƒé‡è®¡ç®—å…¨é‡æŒ‡æ ‡ï¼ˆåŒ…å«æè‡´åå¤„ç†æµæ°´çº¿ï¼‰
            print("   æ­£åœ¨è®¡ç®—å…¨é‡æŒ‡æ ‡ï¼ˆåŒ…å«æè‡´åå¤„ç†ï¼šLCC + ç©ºæ´å¡«å…… + è¾¹ç¼˜å¹³æ»‘ï¼‰...")
            full_total_score, full_avg_dice, full_avg_hd95, _ = calculate_metrics_for_weights(
                best_weights, final_full_mask_list, final_full_gt_masks
            )
            
            print(f"\nğŸ“Š ã€ç»ˆæ•ˆè¯„ä¼°ç»“æœã€‘å…¨é‡ {len(original_gt_masks)} å¼ å›¾ç‰‡:")
            print(f"   Dice: {full_avg_dice:.4f}")
            print(f"   HD95: {full_avg_hd95:.4f} (ç›®æ ‡: â‰¤ 5.0)")
            print(f"   Total Score: {full_total_score:.4f}")
            print(f"   Scoreå…¬å¼: 0.6 * Dice + 0.1 / (1 + HD95)")
            
            # æ›´æ–°æœ€ä½³æŒ‡æ ‡ä¸ºå…¨é‡ç»“æœ
            best_metrics = {
                'dice': full_avg_dice,
                'hd95': full_avg_hd95,
                'total_score': full_total_score
            }
            
            # ã€æœ€ç»ˆæ£€æŸ¥ã€‘å¦‚æœDice > 0.91 ä¸” HD95 < 5.0ï¼Œç«‹å³åœæ­¢å¹¶ä¿å­˜ç»“æœ
            hd95_target = 5.0  # ç›®æ ‡HD95é˜ˆå€¼
            dice_target = 0.91  # ç›®æ ‡Diceé˜ˆå€¼
            
            if full_avg_hd95 <= hd95_target:
                print(f"   âœ… HD95æ»¡è¶³ç›®æ ‡æ¡ä»¶ (â‰¤ {hd95_target})")
            else:
                print(f"   âš ï¸  HD95è¶…å‡ºç›®æ ‡æ¡ä»¶ (>{hd95_target})")
            
            if full_avg_dice > dice_target and full_avg_hd95 < hd95_target:
                print(f"\nğŸ‰ ã€å®Œç¾è¾¾æˆã€‘æŒ‡æ ‡æ»¡è¶³æ‰€æœ‰è¦æ±‚:")
                print(f"   âœ… Dice = {full_avg_dice:.4f} > {dice_target} (ç›®æ ‡è¾¾æˆ)")
                print(f"   âœ… HD95 = {full_avg_hd95:.4f} < {hd95_target} (ç›®æ ‡è¾¾æˆ)")
                print(f"   ğŸ’¾ å»ºè®®ç«‹å³ä¿å­˜ç»“æœï¼")
            elif full_avg_dice > dice_target:
                print(f"\nâœ… Diceç›®æ ‡è¾¾æˆ ({full_avg_dice:.4f} > {dice_target})ï¼Œä½†HD95ä»éœ€ä¼˜åŒ–")
            elif full_avg_hd95 < hd95_target:
                print(f"\nâœ… HD95ç›®æ ‡è¾¾æˆ ({full_avg_hd95:.4f} < {hd95_target})ï¼Œä½†Diceä»éœ€ä¼˜åŒ–")
            else:
                print(f"\nâš ï¸  æŒ‡æ ‡ä»éœ€ä¼˜åŒ–: Dice={full_avg_dice:.4f} (ç›®æ ‡>{dice_target}), HD95={full_avg_hd95:.4f} (ç›®æ ‡<{hd95_target})")
        
        return best_weights, best_metrics
    
    def _compute_hd95_for_ensemble(self, pred_mask, target_mask):
        """
        è®¡ç®—HD95çš„è¾…åŠ©æ–¹æ³•ï¼ˆç”¨äºé›†æˆè¯„ä¼°ï¼‰
        
        Args:
            pred_mask: é¢„æµ‹æ©ç 
            target_mask: çœŸå®æ©ç 
        
        Returns:
            HD95å€¼
        """
        from scipy.ndimage import binary_erosion, distance_transform_edt
        
        pred = pred_mask.astype(bool)
        target = target_mask.astype(bool)
        
        if not pred.any() and not target.any():
            return 0.0
        if not pred.any() or not target.any():
            return np.nan
        
        structure = np.ones((3, 3), dtype=bool)
        pred_border = np.logical_xor(pred, binary_erosion(pred, structure=structure, border_value=0))
        target_border = np.logical_xor(target, binary_erosion(target, structure=structure, border_value=0))
        
        if not pred_border.any():
            pred_border = pred
        if not target_border.any():
            target_border = target
        
        target_distance = distance_transform_edt(~target_border)
        pred_distance = distance_transform_edt(~pred_border)
        
        distances_pred_to_target = target_distance[pred_border]
        distances_target_to_pred = pred_distance[target_border]
        
        all_distances = np.concatenate([distances_pred_to_target, distances_target_to_pred])
        if all_distances.size == 0:
            return 0.0
        return float(np.percentile(all_distances, 95))
        
    def _compute_dice_for_ensemble(self, pred_mask, target_mask, smooth=1e-7):
        """
        è®¡ç®—Diceçš„è¾…åŠ©æ–¹æ³•ï¼ˆç”¨äºé›†æˆè¯„ä¼°ï¼‰
        
        Args:
            pred_mask: é¢„æµ‹æ©ç 
            target_mask: çœŸå®æ©ç 
            smooth: å¹³æ»‘é¡¹
        
        Returns:
            Diceç³»æ•°
        """
        pred = pred_mask.astype(bool)
        target = target_mask.astype(bool)
        intersection = (pred & target).sum()
        union = pred.sum() + target.sum()
        if union == 0:
            return 1.0
        return (2.0 * intersection + smooth) / (union + smooth)
        
    def _compute_iou_for_ensemble(self, pred_mask, target_mask, smooth=1e-7):
        """
        è®¡ç®—IoUçš„è¾…åŠ©æ–¹æ³•ï¼ˆç”¨äºé›†æˆè¯„ä¼°ï¼‰
        
        Args:
            pred_mask: é¢„æµ‹æ©ç 
            target_mask: çœŸå®æ©ç 
            smooth: å¹³æ»‘é¡¹
        
        Returns:
            IoUç³»æ•°
        """
        pred = pred_mask.astype(bool)
        target = target_mask.astype(bool)
        intersection = (pred & target).sum()
        union = (pred | target).sum()
        if union == 0:
            return 1.0
        return (intersection + smooth) / (union + smooth)
        
    def _compute_sens_spec_for_ensemble(self, pred_mask, target_mask):
        """
        è®¡ç®—Sensitivityå’ŒSpecificityçš„è¾…åŠ©æ–¹æ³•ï¼ˆç”¨äºé›†æˆè¯„ä¼°ï¼‰
        
        Args:
            pred_mask: é¢„æµ‹æ©ç 
            target_mask: çœŸå®æ©ç 
        
        Returns:
            (sensitivity, specificity) å…ƒç»„
        """
        pred = pred_mask.astype(bool)
        target = target_mask.astype(bool)
        tp = (pred & target).sum()
        fn = (~pred & target).sum()
        fp = (pred & ~target).sum()
        tn = (~pred & ~target).sum()
        
        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0
        
        return sensitivity, specificity
    
    def evaluate_ensemble_performance(self, mask_list, weights, gt_masks, baseline_score=0.8273):
        """
        è¯„ä¼°é›†æˆåçš„æ€§èƒ½ï¼Œå¯¹æ¯”å•æ¨¡å‹baseline
        
        Args:
            mask_list: æ©ç åˆ—è¡¨ï¼ˆå¤šä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœï¼‰
            weights: é›†æˆæƒé‡
            gt_masks: çœŸå®æ©ç åˆ—è¡¨
            baseline_score: å•æ¨¡å‹baselineæ€»åˆ†ï¼Œé»˜è®¤0.8273
        
        Returns:
            metrics: æŒ‡æ ‡å­—å…¸ï¼ŒåŒ…å«Dice, IoU, HD95, Sensitivity, Specificity, Total Score
            improvement: ç›¸å¯¹äºbaselineçš„æå‡
        """
        # ã€å…³é”®ä¿®å¤ã€‘å¼ºåˆ¶ç±»å‹è½¬æ¢ï¼šç¡®ä¿mask_listä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½æ˜¯numpyæ•°ç»„
        converted_mask_list = []
        for model_idx, model_masks in enumerate(mask_list):
            if isinstance(model_masks, list):
                converted_model_masks = []
                for mask_idx, mask in enumerate(model_masks):
                    if isinstance(mask, list):
                        mask = np.array(mask)
                    elif isinstance(mask, torch.Tensor):
                        mask = mask.detach().cpu().numpy()
                    elif not isinstance(mask, np.ndarray):
                        mask = np.asarray(mask)
                    converted_model_masks.append(mask)
                converted_mask_list.append(converted_model_masks)
            else:
                converted_mask_list.append(model_masks)
        
        mask_list = converted_mask_list
        
        # åŒæ ·å¤„ç†gt_masks
        if isinstance(gt_masks, list):
            converted_gt_masks = []
            for mask in gt_masks:
                if isinstance(mask, list):
                    mask = np.array(mask)
                elif isinstance(mask, torch.Tensor):
                    mask = mask.detach().cpu().numpy()
                elif not isinstance(mask, np.ndarray):
                    mask = np.asarray(mask)
                converted_gt_masks.append(mask)
            gt_masks = converted_gt_masks
        
        # å¯¹æ¯ä¸ªæ ·æœ¬è¿›è¡Œé›†æˆ
        ensemble_preds = []
        for sample_idx in range(len(gt_masks)):
            # ã€å…³é”®ä¿®å¤ã€‘ç¡®ä¿sample_masksä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½æ˜¯numpyæ•°ç»„
            sample_masks = []
            for model_masks in mask_list:
                if isinstance(model_masks, list):
                    if sample_idx < len(model_masks):
                        mask = model_masks[sample_idx]
                        # å¼ºåˆ¶è½¬æ¢ä¸ºnumpyæ•°ç»„
                        if isinstance(mask, list):
                            mask = np.array(mask)
                        elif isinstance(mask, torch.Tensor):
                            mask = mask.detach().cpu().numpy()
                        elif not isinstance(mask, np.ndarray):
                            mask = np.asarray(mask)
                        # å¤„ç†ç»´åº¦ï¼šå¦‚æœæ˜¯ (C, H, W)ï¼Œå–ç¬¬ä¸€ä¸ªé€šé“
                        if mask.ndim == 3:
                            mask = mask[0]
                        sample_masks.append(mask)
                    else:
                        # å¦‚æœç´¢å¼•è¶…å‡ºèŒƒå›´ï¼Œåˆ›å»ºä¸€ä¸ªé›¶æ•°ç»„
                        sample_masks.append(np.zeros_like(gt_masks[0] if len(gt_masks) > 0 else np.zeros((512, 512))))
                else:
                    # å¦‚æœmodel_masksæ˜¯æ•°ç»„ï¼Œç›´æ¥ä½¿ç”¨
                    if model_masks.ndim > 2:
                        mask = model_masks[sample_idx]
                    else:
                        mask = model_masks
                    if not isinstance(mask, np.ndarray):
                        mask = np.asarray(mask)
                    sample_masks.append(mask)
            
            # é›†æˆæ¦‚ç‡å›¾
            ensemble_mask = ensemble_masks_global(sample_masks, weights)
            
            # ã€æè‡´åå¤„ç†æµæ°´çº¿ã€‘åº”ç”¨ä¸‰æ­¥åå¤„ç†ï¼ˆLCC + ç©ºæ´å¡«å…… + è¾¹ç¼˜å¹³æ»‘ï¼‰
            ensemble_mask = ensemble_post_process_global(
                ensemble_mask,
                use_lcc=True,  # ã€ç¬¬ä¸€æ­¥ã€‘ä¿ç•™æœ€å¤§è¿é€šåŸŸï¼Œå½»åº•åˆ‡é™¤ç¦»ç¾¤å™ªç‚¹
                use_remove_holes=True,  # ã€ç¬¬äºŒæ­¥ã€‘å¡«è¡¥å°å­”æ´ï¼Œæå‡Diceçº¦0.5%
                min_hole_size=100,
                use_edge_smoothing=True  # ã€ç¬¬ä¸‰æ­¥ã€‘è¾¹ç¼˜å¹³æ»‘ï¼Œä¿®æ­£é”¯é½¿è¾¹ç¼˜
            )
            ensemble_preds.append(ensemble_mask)
        
        # è®¡ç®—æ•´ä½“æŒ‡æ ‡
        dice_scores = []
        iou_scores = []
        hd95_scores = []
        sensitivity_scores = []
        specificity_scores = []
        
        for pred, gt in zip(ensemble_preds, gt_masks):
            dice = self._compute_dice_for_ensemble(pred, gt)
            iou = self._compute_iou_for_ensemble(pred, gt)
            hd95 = self._compute_hd95_for_ensemble(pred, gt)
            sensitivity, specificity = self._compute_sens_spec_for_ensemble(pred, gt)
            
            dice_scores.append(dice)
            iou_scores.append(iou)
            if not np.isnan(hd95):
                hd95_scores.append(hd95)
            sensitivity_scores.append(sensitivity)
            specificity_scores.append(specificity)
        
        avg_dice = np.mean(dice_scores)
        avg_iou = np.mean(iou_scores)
        avg_hd95 = np.mean(hd95_scores) if hd95_scores else np.nan
        avg_sensitivity = np.mean(sensitivity_scores)
        avg_specificity = np.mean(specificity_scores)
        
        # è®¡ç®—å®˜æ–¹æ€»åˆ†
        total_score = calculate_official_total_score_global(
            avg_dice, avg_iou, avg_hd95, avg_sensitivity, avg_specificity
        )
        
        # è®¡ç®—æå‡
        improvement = total_score - baseline_score
        
        metrics = {
            'dice': avg_dice,
            'iou': avg_iou,
            'hd95': avg_hd95,
            'sensitivity': avg_sensitivity,
            'specificity': avg_specificity,
            'total_score': total_score
        }
        
        print(f"\nğŸ“Š é›†æˆæ€§èƒ½è¯„ä¼°:")
        print(f"   Dice: {avg_dice:.4f}")
        print(f"   IoU: {avg_iou:.4f}")
        print(f"   HD95: {avg_hd95:.4f}")
        print(f"   Sensitivity: {avg_sensitivity:.4f}")
        print(f"   Specificity: {avg_specificity:.4f}")
        print(f"   Total Score: {total_score:.4f}")
        print(f"   Baseline Score: {baseline_score:.4f}")
        print(f"   æå‡: {improvement:+.4f} ({'âœ… æå‡' if improvement > 0 else 'âŒ ä¸‹é™'})")
        
        return metrics, improvement
    
    @staticmethod
    def calculate_official_total_score(dice, iou, hd95, sensitivity, specificity):
        """
        è®¡ç®—æ¯”èµ›å®˜æ–¹æ€»åˆ†å…¬å¼ï¼š
        Total = 0.6*Dice + 0.1*IoU + 0.1/(1+HD95) + 0.1*Sens + 0.1*Spec
        
        Args:
            dice: Diceç³»æ•°
            iou: IoUç³»æ•°
            hd95: HD95å€¼ï¼ˆå¦‚æœä¸ºNaNæˆ–Infï¼Œåˆ™ä½¿ç”¨ä¸€ä¸ªå¾ˆå¤§çš„å€¼ï¼‰
            sensitivity: æ•æ„Ÿåº¦ï¼ˆå¬å›ç‡ï¼‰
            specificity: ç‰¹å¼‚æ€§
        
        Returns:
            æ€»åˆ†
        """
        # å¤„ç†HD95çš„NaN/Infæƒ…å†µ
        if np.isnan(hd95) or np.isinf(hd95):
            hd95_term = 0.0  # å¦‚æœHD95ä¸å¯è®¡ç®—ï¼Œè¯¥é¡¹ä¸º0
        else:
            hd95_term = 0.1 / (1.0 + hd95)
        
        total_score = (
            0.6 * dice +
            0.1 * iou +
            hd95_term +
            0.1 * sensitivity +
            0.1 * specificity
        )
        return total_score
    
    def calculate_hd95(self, pred_mask: np.ndarray, target_mask: np.ndarray) -> float:
        """
        è®¡ç®—Hausdorff Distance 95 (HD95)ï¼Œè¡¡é‡åˆ†å‰²è¾¹ç•Œè·ç¦»ã€‚
        è‹¥ä»»ä¸€æ©è†œä¸ºç©ºï¼Œåˆ™è¿”å›nanï¼Œè¡¨ç¤ºè¯¥æŒ‡æ ‡ä¸å¯è®¡ç®—ã€‚
        
        ã€å…³é”®ã€‘ä½¿ç”¨åŸå§‹åƒç´ åæ ‡ç³»ï¼Œä¸è¿›è¡Œå½’ä¸€åŒ–ã€‚
        distance_transform_edt é»˜è®¤ä½¿ç”¨åƒç´ è·ç¦»ï¼ˆæ¯ä¸ªåƒç´ =1å•ä½ï¼‰ï¼Œ
        å› æ­¤è¿”å›çš„HD95å€¼ç›´æ¥è¡¨ç¤ºåƒç´ è·ç¦»ï¼Œæ— éœ€ä¹˜ä»¥åƒç´ é—´è·ã€‚
        """
        if self.matlab_metrics_bridge:
            try:
                return self.matlab_metrics_bridge.compute_hd95(pred_mask, target_mask)
            except Exception as exc:
                print(f"[MATLAB HD95] å›é€€åˆ°CPUå®ç°: {exc}")

        pred = pred_mask.astype(bool)
        target = target_mask.astype(bool)

        if not pred.any() and not target.any():
            return 0.0
        if not pred.any() or not target.any():
            return np.nan

        structure = np.ones((3, 3), dtype=bool)
        pred_border = np.logical_xor(pred, binary_erosion(pred, structure=structure, border_value=0))
        target_border = np.logical_xor(target, binary_erosion(target, structure=structure, border_value=0))

        if not pred_border.any():
            pred_border = pred
        if not target_border.any():
            target_border = target

        # ã€å…³é”®ã€‘distance_transform_edt ä½¿ç”¨åŸå§‹åƒç´ åæ ‡ç³»
        # è¿”å›çš„è·ç¦»å€¼ç›´æ¥è¡¨ç¤ºåƒç´ æ•°ï¼Œæ— éœ€å½’ä¸€åŒ–æˆ–ä¹˜ä»¥åƒç´ é—´è·
        target_distance = distance_transform_edt(~target_border)
        pred_distance = distance_transform_edt(~pred_border)

        distances_pred_to_target = target_distance[pred_border]
        distances_target_to_pred = pred_distance[target_border]

        all_distances = np.concatenate([distances_pred_to_target, distances_target_to_pred])
        if all_distances.size == 0:
            return 0.0
        # è¿”å›95ç™¾åˆ†ä½è·ç¦»ï¼ˆåƒç´ å•ä½ï¼‰
        return float(np.percentile(all_distances, 95))

    def calculate_custom_score(
        self,
        dice: float,
        iou: float,
        precision: float,
        recall: float,
        specificity: float,
        hd95: float,
    ) -> float:
        """
        è‡ªå®šä¹‰ç»¼åˆè¯„åˆ†å‡½æ•°:
        Score = (Dice * 50) + (IoU * 10) + (Precision * 10) + (Recall * 10) + (Specificity * 10) + Score_HD95
        å…¶ä¸­ Score_HD95 = 10 / (HD95 + 1)ï¼Œè‹¥HD95ä¸å¯ç”¨åˆ™è¯¥é¡¹è®°ä¸º0ã€‚
        """
        dice = float(dice)
        iou = float(iou)
        precision = float(precision)
        recall = float(recall)
        specificity = float(specificity)

        # HD95 é¡¹ï¼šHD95 è¶Šå°è¶Šå¥½ï¼Œä½¿ç”¨åæ¯”å˜æ¢ï¼›è‹¥æ— æ•ˆåˆ™è®°ä¸º 0
        if hd95 is None or not np.isfinite(hd95) or hd95 < 0:
            score_hd95 = 0.0
        else:
            score_hd95 = 10.0 / (float(hd95) + 1.0)

        total_score = (
            dice * 50.0
            + iou * 10.0
            + precision * 10.0
            + recall * 10.0
            + specificity * 10.0
            + score_hd95
        )
        return float(total_score)

    def scan_best_threshold(self, prob_maps: np.ndarray, gt_masks: np.ndarray):
        """
        åœ¨ç»™å®šçš„æ¦‚ç‡å›¾å’ŒçœŸå®æ©è†œä¸Šæ‰«æé˜ˆå€¼ï¼Œå¯»æ‰¾ç»¼åˆè¯„åˆ†æœ€é«˜çš„é˜ˆå€¼ã€‚

        Args:
            prob_maps: æ¦‚ç‡å›¾ï¼Œå½¢çŠ¶ [N, H, W] æˆ– [N, 1, H, W]ï¼Œæ•°å€¼èŒƒå›´ [0,1]
            gt_masks:  çœŸå®æ©è†œï¼Œå½¢çŠ¶ä¸ prob_maps å¯¹åº”ï¼Œå–å€¼ {0,1}

        Returns:
            best_thresh: ç»¼åˆè¯„åˆ†æœ€é«˜çš„é˜ˆå€¼
            best_metrics: å¯¹åº”é˜ˆå€¼ä¸‹çš„æŒ‡æ ‡å­—å…¸ï¼ˆdice, iou, precision, recall, specificity, hd95, scoreï¼‰
        """
        prob_maps = np.asarray(prob_maps, dtype=np.float32)
        gt_masks = np.asarray(gt_masks, dtype=np.float32)

        # ç»Ÿä¸€ä¸º [N, H, W]
        if prob_maps.ndim == 4:
            prob_maps = prob_maps[:, 0]
        if gt_masks.ndim == 4:
            gt_masks = gt_masks[:, 0]

        # äºŒå€¼åŒ–çœŸå€¼
        gt_bool = gt_masks > 0.5

        thresholds = np.arange(0.3, 0.91, 0.05, dtype=np.float32)
        best_thresh = 0.5
        best_score = -float("inf")
        best_metrics = {}

        for thr in thresholds:
            pred_bool = prob_maps >= float(thr)

            # å…¨å±€æ··æ·†çŸ©é˜µï¼ˆæ‰€æœ‰åƒç´ ä¸€èµ·ç»Ÿè®¡ï¼‰
            tp = np.logical_and(pred_bool, gt_bool).sum(dtype=np.float64)
            fp = np.logical_and(pred_bool, ~gt_bool).sum(dtype=np.float64)
            fn = np.logical_and(~pred_bool, gt_bool).sum(dtype=np.float64)
            tn = np.logical_and(~pred_bool, ~gt_bool).sum(dtype=np.float64)

            pred_sum = tp + fp
            mask_sum = tp + fn

            dice_den = 2.0 * tp + fp + fn
            if dice_den < 1e-7:
                dice = 1.0 if (mask_sum < 1e-7 and pred_sum < 1e-7) else 0.0
            else:
                dice = (2.0 * tp) / (dice_den + 1e-8)

            union = tp + fp + fn
            iou = 1.0 if union < 1e-7 else tp / (union + 1e-8)

            if pred_sum < 1e-7:
                precision = 1.0 if mask_sum < 1e-7 else 0.0
            else:
                precision = tp / (pred_sum + 1e-8)

            if (tp + fn) < 1e-7:
                recall = 1.0 if pred_sum < 1e-7 else 0.0
            else:
                recall = tp / (tp + fn + 1e-8)

            if (tn + fp) < 1e-7:
                specificity = 1.0
            else:
                specificity = tn / (tn + fp + 1e-8)

            # è®¡ç®—è¯¥é˜ˆå€¼ä¸‹çš„å¹³å‡ HD95ï¼ˆå¯¹æ¯ä¸ªæ ·æœ¬å•ç‹¬è®¡ç®—ï¼‰
            hd95_list = []
            for i in range(pred_bool.shape[0]):
                try:
                    hd = calculate_hd95(
                        pred_bool[i].astype(np.uint8),
                        gt_bool[i].astype(np.uint8),
                    )
                except Exception:
                    hd = float("nan")
                if np.isfinite(hd):
                    hd95_list.append(float(hd))

            if hd95_list:
                hd95_mean = float(np.nanmean(hd95_list))
            else:
                # è‹¥æ‰€æœ‰æ ·æœ¬éƒ½æ— æ³•è®¡ç®— HD95ï¼Œåˆ™è®°ä¸ºæ— ç©·å¤§ï¼Œä»¥ä¾¿åœ¨è¯„åˆ†ä¸­è®©è¯¥é¡¹ä¸º 0
                hd95_mean = float("inf")

            total_score = calculate_custom_score(
                dice=dice,
                iou=iou,
                precision=precision,
                recall=recall,
                specificity=specificity,
                hd95=hd95_mean,
            )

            if total_score > best_score:
                best_score = float(total_score)
                best_thresh = float(thr)
                best_metrics = {
                    "dice": float(dice),
                    "iou": float(iou),
                    "precision": float(precision),
                    "recall": float(recall),
                    "specificity": float(specificity),
                    "hd95": float(hd95_mean) if np.isfinite(hd95_mean) else float("nan"),
                    "score": float(total_score),
                }

        return best_thresh, best_metrics
    



# é¢„æµ‹å·¥ä½œçº¿ç¨‹

class PredictThread(QThread):
    update_progress = pyqtSignal(int, str)
    prediction_finished = pyqtSignal(list, list, list)  # æ·»åŠ åŸå§‹å›¾åƒè·¯å¾„å‚æ•°
    
    def __init__(self, image_paths, model_path, threshold=0.5, save_results=True, output_dir=None):
        super().__init__()
        self.image_paths = image_paths
        self.model_path = model_path
        self.threshold = threshold
        self.save_results = save_results
        self.output_dir = output_dir
        if self.save_results and self.output_dir:
            os.makedirs(self.output_dir, exist_ok=True)
        self.model_config = read_checkpoint_config(model_path) if model_path else None
        self.model_type = (self.model_config or {}).get("model_type", "improved_unet")
        self.swin_params = (self.model_config or {}).get("swin_params")
        self.dstrans_params = (self.model_config or {}).get("dstrans_params")
        self.model_threshold = (self.model_config or {}).get("best_threshold")
        if self.model_threshold is not None:
            self.threshold = float(self.model_threshold)
        self.use_tta = True
        context_cfg = (self.model_config or {}).get("context") or {}
        self.context_slices = int(context_cfg.get("slices", os.environ.get("SEG_CONTEXT_SLICES", "0")))
        self.context_gap = int(context_cfg.get("gap", os.environ.get("SEG_CONTEXT_GAP", "1")))
        self.required_modalities = (self.model_config or {}).get("extra_modalities") or []
        self.extra_modalities_dirs = parse_extra_modalities_spec(os.environ.get("SEG_EXTRA_MODALITIES"))
        if self.required_modalities:
            missing = [m for m in self.required_modalities if m not in self.extra_modalities_dirs]
            if missing:
                print(f"[æç¤º] æ¨¡å‹æœŸæœ›é¢å¤–æ¨¡æ€: {missing}ï¼Œå½“å‰æœªåœ¨ SEG_EXTRA_MODALITIES ä¸­é…ç½®ï¼Œå°†å°è¯•ä»…ä½¿ç”¨å¯ç”¨æ¨¡æ€ã€‚")
        skull_cfg = (self.model_config or {}).get("skull_stripping") or {}
        self.use_skull_stripper = skull_cfg.get("enabled", False)
        self.skull_stripper_path = skull_cfg.get("model_path")
        self.skull_stripper_threshold = skull_cfg.get("threshold", 0.5)
        if self.use_skull_stripper and not self.skull_stripper_path:
            self.use_skull_stripper = False
        # nnFormer é…ç½®
        self.use_nnformer = False
    
    def _predict_with_tta(self, model, image, use_tta=True):
        import torch.nn.functional as F
        if not use_tta:
            return torch.sigmoid(model(image))
        preds = []
        preds.append(torch.sigmoid(model(image)))
        preds.append(torch.flip(torch.sigmoid(model(torch.flip(image, dims=[3]))), dims=[3]))
        preds.append(torch.flip(torch.sigmoid(model(torch.flip(image, dims=[2]))), dims=[2]))
        preds.append(torch.rot90(torch.sigmoid(model(torch.rot90(image, k=1, dims=[2, 3]))), k=-1, dims=[2, 3]))
        
        # ã€å…³é”®ä¿®å¤ã€‘ç»Ÿä¸€æ‰€æœ‰é¢„æµ‹çš„ç©ºé—´å°ºå¯¸
        if len(preds) > 0 and preds[0].dim() == 4:
            _, _, H, W = preds[0].shape
            target_size = (H, W)
            normalized_preds = []
            for pred in preds:
                if pred.dim() == 4:
                    _, _, h, w = pred.shape
                    if h != H or w != W:
                        # æ’å€¼åˆ°ç›®æ ‡å°ºå¯¸
                        pred = F.interpolate(pred, size=target_size, mode='bilinear', align_corners=False)
                normalized_preds.append(pred)
            preds = normalized_preds
        
        return torch.stack(preds, dim=0).mean(dim=0)
    
    def _post_process(self, prob_tensor):
        processed = TrainThread.post_process_mask(
            prob_tensor.squeeze(0), 
            min_size=30, 
            use_morphology=True,
            keep_largest=False,  # å…è®¸å¤šå‘ç—…ç¶åŒæ—¶å­˜åœ¨
            fill_holes=True     # å¡«å……å­”æ´ï¼Œå»é™¤å‡é˜´æ€§ç©ºæ´
        )
        if isinstance(processed, torch.Tensor):
            return processed.unsqueeze(0).unsqueeze(0)
        processed = torch.from_numpy(processed).float()
        return processed.unsqueeze(0).unsqueeze(0)
    

    def run(self):
        try:
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            self.update_progress.emit(0, f"ä½¿ç”¨è®¾å¤‡: {device}")

            
            # æ•°æ®è½¬æ¢
            transform = A.Compose([
                A.Resize(256, 256),
                A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
                ToTensorV2()
            ])
            
            # åˆ›å»ºæ•°æ®é›†
            extra_modalities = build_extra_modalities_lists(self.image_paths, self.extra_modalities_dirs)
            dataset = MedicalImageDataset(
                self.image_paths,
                transform=transform,
                training=False,
                extra_modalities=extra_modalities,
                context_slices=self.context_slices,
                context_gap=self.context_gap
            )
            dataloader = DataLoader(dataset, batch_size=1, shuffle=False)
            if self.model_threshold is not None:
                self.update_progress.emit(8, f"ä½¿ç”¨æ¨¡å‹è‡ªé€‚åº”é˜ˆå€¼: {self.threshold:.3f}")
            
            # åŠ è½½åˆ†å‰²æ¨¡å‹ - ä½¿ç”¨å…¼å®¹åŠ è½½
            model = instantiate_model(self.model_type, device, self.swin_params, self.dstrans_params, None)
            success, msg = load_model_compatible(model, self.model_path, device, verbose=True)
            if not success:
                raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥: {msg}")
            model.eval()
            skull_stripper = None
            if self.use_skull_stripper:
                skull_stripper = SkullStripper(self.skull_stripper_path, device, self.skull_stripper_threshold)
                if not skull_stripper.is_available():
                    skull_stripper = None
                    self.update_progress.emit(6, "SkullStripperä¸å¯ç”¨ï¼Œå›é€€ä¸ºå•é˜¶æ®µæ¨ç†")
            
            self.update_progress.emit(10, "æ¨¡å‹åŠ è½½å®Œæˆï¼Œå¼€å§‹é¢„æµ‹...")
            
            input_images = []
            output_masks = []
            input_numpy_images = []  # å­˜å‚¨åŸå§‹å›¾åƒæ•°æ®
            
            with torch.no_grad():
                for i, batch_data in enumerate(dataloader):
                    # å¤„ç†æ•°æ®
                    if isinstance(batch_data, tuple):
                        if len(batch_data) == 2:
                            image, mask = batch_data
                        else:
                            image = batch_data[0]
                    else:
                        image = batch_data
                    # ç¡®ä¿imageæ˜¯tensor
                    if not isinstance(image, torch.Tensor):
                        if isinstance(image, (list, tuple)) and len(image) > 0:
                            image = image[0]
                    image = image.to(device)
                    brain_mask = None
                    if skull_stripper and skull_stripper.is_available():
                        image, brain_mask = skull_stripper.strip(image)
                    
                    # åˆ†å‰²é¢„æµ‹
                    prob = self._predict_with_tta(model, image, use_tta=self.use_tta)
                    if brain_mask is not None:
                        prob = prob * brain_mask
                    pred = (prob > self.threshold).float()
                    pred = self._post_process(pred)
                    
                    # è½¬æ¢å›å›¾åƒæ ¼å¼
                    image_np = image[0].cpu().numpy().transpose(1, 2, 0)
                    image_np = image_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])
                    image_np = np.clip(image_np * 255, 0, 255).astype(np.uint8)
                    prob_np = prob[0, 0].cpu().numpy()
                    pred_np = pred[0, 0].cpu().numpy()
                    pred_np = (pred_np * 255).astype(np.uint8)
                    
                    # å­˜å‚¨åŸå§‹å›¾åƒæ•°æ®
                    input_numpy_images.append((image_np, pred_np, prob_np, ""))
                    
                    # å¦‚æœéœ€è¦ä¿å­˜ç»“æœ
                    if self.save_results and self.output_dir:
                        # å®‰å…¨è·å–æ–‡ä»¶å
                        if i < len(self.image_paths):
                            base_name = os.path.splitext(os.path.basename(self.image_paths[i]))[0]
                        else:
                            base_name = f"image_{i}"
                        input_path = os.path.join(self.output_dir, f"{base_name}_input.png")
                        output_path = os.path.join(self.output_dir, f"{base_name}_mask.png")
                        cv2.imwrite(input_path, cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR))
                        cv2.imwrite(output_path, pred_np)
                        
                        input_images.append(input_path)
                        output_masks.append(output_path)
                    else:
                        # å¦‚æœä¸ä¿å­˜ï¼Œä½¿ç”¨ä¸´æ—¶æ–‡ä»¶å
                        input_images.append(f"image_{i}_input")
                        output_masks.append(f"image_{i}_mask")
                    
                    progress_msg = f"å¤„ç†å›¾åƒ {i+1}/{len(dataloader)}"
                    progress = 10 + int(90 * (i + 1) / len(dataloader))
                    self.update_progress.emit(progress, progress_msg)
            
            self.prediction_finished.emit(input_images, output_masks, input_numpy_images)
        
        except Exception as e:
            self.update_progress.emit(0, f"é¢„æµ‹é”™è¯¯: {str(e)}")


